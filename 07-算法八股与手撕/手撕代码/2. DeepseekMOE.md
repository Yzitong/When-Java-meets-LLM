```python
# ---------------------- DeepseekMoE 核心类 ----------------------
class DeepseekMoE(nn.Module):
    """DeepSeek MoE核心层：含共享专家 + 动态路由专家，支持训练/推理优化
    核心设计：
    1. 分层专家：独立专家（处理细分任务） + 共享专家（处理通用语义）
    2. 动态门控：MoEGate选Top-K专家，训练/推理分支优化
    3. 负载均衡：通过辅助损失（aux_loss）平衡专家利用率
    """
    def __init__(self, config):
        super().__init__()
        self.config = config  # 配置项：hidden_size, num_experts, num_experts_per_tok等
        self.num_experts_per_tok = config.num_experts_per_tok  # 每个token激活的专家数（如8）
        
        # ---------- 1. 初始化独立专家 ----------
        # 每个专家是定制MLP，处理中间层维度为 config.moe_intermediate_size
        self.experts = nn.ModuleList([
            DeepseekMLP(config, intermediate_size=config.moe_intermediate_size) 
            for _ in range(config.num_experts)  # 独立专家总数（如256）
        ])
        
        # ---------- 2. 门控网络：选专家 + 算辅助损失 ----------
        self.gate = MoEGate(config)
        
        # ---------- 3. 共享专家（可选） ----------
        # 处理通用语义，避免专家冗余，config.n_shared_experts控制数量（如1）
        if config.n_shared_experts is not None:
            # 共享专家的中间层维度：独立专家中间层 × 共享专家数（增强表达）
            shared_intermediate_size = config.moe_intermediate_size * config.n_shared_experts
            self.shared_experts = DeepseekMLP(
                config=config, 
                intermediate_size=shared_intermediate_size
            )

    def forward(self, hidden_states):
        """前向传播：分训练和推理分支，优化计算效率
        Args:
            hidden_states: (batch_size, seq_len, hidden_size) - 输入隐藏态
        Returns:
            y: 专家输出融合结果 + 共享专家输出（若有）
        """
        # 保存原始形状，用于后续恢复
        identity = hidden_states  # 共享专家的输入（保留原始输入，避免梯度消失）
        orig_shape = hidden_states.shape  # (B, S, H)
        
        # ---------- Step 1：门控网络选Top-K专家 ----------
        # topk_idx: (B, S, top_k) - 选中的专家索引
        # topk_weight: (B, S, top_k) - 专家权重（已归一化）
        # aux_loss: 辅助损失（训练时用于负载均衡）
        topk_idx, topk_weight, aux_loss = self.gate(hidden_states)
        
        # 展平维度：(B*S, H)，方便批量处理
        hidden_states = hidden_states.view(-1, hidden_states.shape[-1])  # (N, H), N=B*S
        flat_topk_idx = topk_idx.view(-1)  # (N*top_k,) → 展平为1D索引
        
        # ---------- Step 2：分训练/推理分支处理 ----------
        if self.training:
            # 训练时：重复输入，让每个专家处理对应token（简单但耗显存）
            # 示例：N个token，每个激活top_k专家 → 复制N×top_k次输入
            hidden_states = hidden_states.repeat_interleave(
                self.num_experts_per_tok, 
                dim=0  # 沿batch维度复制
            )
            # 初始化输出张量
            y = torch.empty_like(hidden_states)  # (N×top_k, H)
            
            # 逐个专家处理：将对应token的输入喂给专家
            for i, expert in enumerate(self.experts):
                # 筛选出该专家负责的token（flat_topk_idx == i）
                mask = (flat_topk_idx == i)
                if mask.any():
                    y[mask] = expert(hidden_states[mask])
            
            # 加权融合：将top_k个专家的输出按权重累加
            # 形状恢复：(N, top_k, H) → 加权和为 (N, H)
            y = y.view(*topk_weight.shape, -1) * topk_weight.unsqueeze(-1)
            y = y.sum(dim=1)  # 按token聚合Top-K专家的输出
            
            # 注入辅助损失（训练时影响反向传播）
            y = AddAuxiliaryLoss.apply(y, aux_loss)
        
        else:
            # 推理时：更高效的实现（如moe_infer，避免重复复制，此处简化示意）
            # 实际会用索引批量处理，减少循环开销
            y = self.moe_infer(hidden_states, flat_topk_idx, topk_weight.view(-1, 1))
        
        # 恢复原始形状：(B, S, H)
        y = y.view(*orig_shape)
        
        # ---------- Step 3：添加共享专家输出（可选） ----------
        if hasattr(self, 'shared_experts'):
            # 共享专家处理原始输入（identity），避免被门控的动态性干扰
            shared_output = self.shared_experts(identity)
            y = y + shared_output  # 残差连接，融合通用与专精知识
        
        return y
```

