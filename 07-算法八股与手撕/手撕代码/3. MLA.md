```python
import torch
import torch.nn as nn
import math
import torch.nn.functional as F
from rms_norm import RMSNorm  # 假设已实现RMSNorm（稳定训练的归一化）

def rotate_half(x):
    """辅助函数：将最后一维拆分为两半，实现旋转（RoPE的核心操作）"""
    x1, x2 = x.chunk(2, dim=-1)
    return torch.cat((-x2, x1), dim=-1)

def apply_rotary_pos_emb(q_rope, k_rope, cos, sin):
    """对q_rope和k_rope应用旋转位置编码（RoPE）"""
    # 核心逻辑：模拟复数旋转的实数运算
    q_rot = (q_rope * cos) + (rotate_half(q_rope) * sin)
    k_rot = (k_rope * cos) + (rotate_half(k_rope) * sin)
    return q_rot, k_rot

class MultiHeadLatentAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        # ========== 1. 基础配置 ==========
        self.hidden_size = config.hidden_size      # 模型隐藏层维度
        self.num_heads = config.num_attention_heads# 注意力头数
        
        # ========== 2. LoRA秩配置（效率优化） ==========
        self.q_lora_rank = config.q_lora_rank      # Query的LoRA秩（可选）
        self.kv_lora_rank = config.kv_lora_rank    # KV联合压缩的LoRA秩（核心优化）
        
        # ========== 3. 头维度拆分（解耦位置编码） ==========
        # q/k分为：nope（不应用RoPE） + rope（应用RoPE），v单独
        self.qk_nope_head_dim = config.qk_nope_head_dim  # q/k的非位置编码部分维度
        self.qk_rope_head_dim = config.qk_rope_head_dim  # q/k的位置编码部分维度
        self.qk_head_dim = self.qk_nope_head_dim + self.qk_rope_head_dim  # q/k总维度
        self.v_head_dim = config.v_head_dim        # Value维度
        
        # ========== 4. 三组投影定义 ==========
        # ---------- 4.1 Query投影（可选LoRA） ----------
        if self.q_lora_rank is None:
            self.q_proj = nn.Linear(
                self.hidden_size, 
                self.num_heads * self.qk_head_dim,  # 输出：头数×(nope+rope维度)
                bias=False
            )
        else:
            # LoRA降维（减少参数量，方便微调）
            self.q_down_proj = nn.Linear(
                self.hidden_size, 
                config.q_lora_rank, 
                bias=config.attention_bias
            )
            self.q_layernorm = RMSNorm(config.q_lora_rank)  # 归一化稳定训练
            # LoRA升维（恢复到原始头维度）
            self.q_up_proj = nn.Linear(
                config.q_lora_rank, 
                self.num_heads * self.qk_head_dim, 
                bias=False
            )
        
        # ---------- 4.2 KV联合低秩压缩（显存优化核心） ----------
        self.kv_down_proj = nn.Linear(
            self.hidden_size, 
            config.kv_lora_rank, 
            bias=config.attention_bias
        )
        self.kv_layernorm = RMSNorm(config.kv_lora_rank)
        self.kv_up_proj = nn.Linear(
            config.kv_lora_rank, 
            # 输出：头数×(k_nope维度 + v维度)
            self.num_heads * (self.qk_nope_head_dim + self.v_head_dim),  
            bias=False
        )
        
        # ---------- 4.3 K的RoPE专属投影（位置编码解耦） ----------
        self.k_rope_proj = nn.Linear(
            self.hidden_size, 
            self.qk_rope_head_dim,  # 仅输出k的位置编码部分
            bias=config.attention_bias
        )
        
        # ---------- 4.4 输出投影 ----------
        self.o_proj = nn.Linear(
            self.num_heads * self.v_head_dim, 
            self.hidden_size, 
            bias=config.attention_bias
        )
    
    def forward(self, hidden_states, cos, sin):
        """
        hidden_states: [batch_size, seq_len, hidden_size] → 输入序列
        cos, sin:      RoPE的余弦和正弦矩阵 → 用于旋转位置编码
        return:        [batch_size, seq_len, hidden_size] → 注意力输出
        """
        B, T, D = hidden_states.size()  # B=batch, T=seq_len, D=hidden_size
        
        # ========== Step 1: 计算Query（Q），并拆分为q_nope和q_rope ==========
        if self.q_lora_rank is None:
            q = self.q_proj(hidden_states)  # 直接线性投影
        else:
            # LoRA计算：降维→归一化→升维
            q = self.q_down_proj(hidden_states)
            q = self.q_layernorm(q)
            q = self.q_up_proj(q)
        
        # 维度重塑：[B, T, nh*qk_dim] → [B, nh, T, qk_dim]（nh=num_heads）
        q = q.view(B, T, self.num_heads, self.qk_head_dim).transpose(1, 2)  
        
        # 拆分：前半部分是q_nope（不应用RoPE），后半是q_rope（应用RoPE）
        q_nope, q_rope = torch.split(
            q, 
            [self.qk_nope_head_dim, self.qk_rope_head_dim], 
            dim=-1
        )
        
        # ========== Step 2: 计算KV（k_nope和v共享低秩压缩；k_rope单独生成） ==========
        # ---------- 2.1 KV联合低秩压缩 ----------
        kv = self.kv_down_proj(hidden_states)
        kv = self.kv_layernorm(kv)
        kv = self.kv_up_proj(kv)
        # 维度重塑：[B, T, nh*(nope_dim + v_dim)] → [B, nh, T, nope_dim + v_dim]
        kv = kv.view(B, T, self.num_heads, self.qk_nope_head_dim + self.v_head_dim).transpose(1, 2)  
        
        # 拆分k_nope和v：前半是k_nope（K的非位置编码部分），后半是v
        k_nope, v = torch.split(
            kv, 
            [self.qk_nope_head_dim, self.v_head_dim], 
            dim=-1
        )
        
        # ---------- 2.2 单独计算k_rope（所有头共享，通过repeat复制） ----------
        k_rope = self.k_rope_proj(hidden_states)  # [B, T, rope_dim]
        # 扩展维度：[B, T, rope_dim] → [B, 1, T, rope_dim] → 复制到所有头 → [B, nh, T, rope_dim]
        k_rope = k_rope.unsqueeze(1).repeat(1, self.num_heads, 1, 1)
        
        # ========== Step 3: 应用旋转位置编码（RoPE） ==========
        if cos is not None and sin is not None:
            q_rope, k_rope = apply_rotary_pos_emb(q_rope, k_rope, cos, sin)
        
        # ========== Step 4: 组装完整的Q和K（nope + rope部分） ==========
        # 初始化空张量，保持设备和数据类型一致
        q = k_rope.new_empty(B, self.num_heads, T, self.qk_head_dim)
        q[..., :self.qk_nope_head_dim] = q_nope  # 拼接非位置编码部分
        q[..., self.qk_nope_head_dim:] = q_rope  # 拼接位置编码部分
        
        k = k_rope.new_empty(B, self.num_heads, T, self.qk_head_dim)
        k[..., :self.qk_nope_head_dim] = k_nope  # 拼接非位置编码部分
        k[..., self.qk_nope_head_dim:] = k_rope  # 拼接位置编码部分
        
        # ========== Step 5: 计算多头自注意力（Causal Self-Attention） ==========
        # 1. 计算注意力得分：q @ k^T，除以根号维度（方差归一化）
        attn_scores = q @ k.transpose(-1, -2) / math.sqrt(self.qk_head_dim)
        
        # 2. 应用Causal Mask（下三角掩码，避免看到未来token）
        mask = torch.tril(torch.ones(T, T, device=attn_scores.device)).view(1, 1, T, T)
        attn_scores = attn_scores.masked_fill(mask[..., :T, :T] == 0, float('-inf'))
        
        # 3. 归一化（Softmax）
        attn_probs = torch.softmax(attn_scores, dim=-1)
        
        # 4. 注意力加权和（attn @ v）
        attn_output = attn_probs @ v  # [B, nh, T, v_head_dim]
        
        # 5. 重塑维度 + 输出投影
        attn_output = attn_output.transpose(1, 2).contiguous()  # [B, T, nh, v_head_dim]
        attn_output = attn_output.view(B, T, self.num_heads * self.v_head_dim)  # 合并头维度
        attn_output = self.o_proj(attn_output)  # 映射回原始隐藏维度
        
        return attn_output
```

