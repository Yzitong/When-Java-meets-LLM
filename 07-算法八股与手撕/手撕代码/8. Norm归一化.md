### 1. BatchNorm

```python
import torch
from torch import nn

class MyBatchNorm(nn.Module):
    """
    自定义批归一化（Batch Normalization）层。
    批归一化通过标准化每一层的输入，减少内部协变量偏移，提升训练稳定性。
    """
    def __init__(self, num_features, eps=1e-5, momentum=0.1):
        """
        初始化批归一化层。
        
        参数:
            num_features: 输入特征的数量
            eps: 防止除零的小常数，默认为 1e-5
            momentum: 移动平均的动量，用于更新运行均值和方差，默认为 0.1
        """
        super(MyBatchNorm, self).__init__()
        self.eps = eps
        self.momentum = momentum
        
        # 可学习的参数：gamma（缩放）和 beta（偏移）
        self.gamma = nn.Parameter(torch.ones(num_features))  # 初始化为 1
        self.beta = nn.Parameter(torch.zeros(num_features))  # 初始化为 0
        
        # 运行时的均值和方差（非训练时使用）
        self.running_mean = torch.zeros(num_features)
        self.running_var = torch.ones(num_features)

    def forward(self, x):
        """
        前向传播实现批归一化。
        
        参数:
            x: 输入张量，形状为 (batch_size, num_features, ...)
        
        返回:
            y: 归一化后的输出张量
        """
        # 检查当前是否为训练模式
        if self.training:
            # 计算批量内的均值
            mu = x.mean(dim=0)
            
            # 计算批量内的方差
            var = x.var(dim=0)
            
            # 更新运行均值和方差，使用动量法
            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mu
            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var
            
            # 归一化公式：(x - mu) / sqrt(var + eps)
            y = self.gamma * (x - mu) / torch.sqrt(var + self.eps) + self.beta
        else:
            # 测试模式下使用运行均值和方差
            y = self.gamma * (x - self.running_mean) / torch.sqrt(self.running_var + self.eps) + self.beta
            
        return y

# 示例用法
if __name__ == "__main__":
    # 设置特征数量
    num_features = 512
    
    # 创建批归一化层实例
    mybatchnorm = MyBatchNorm(num_features)
    
    # 生成随机输入张量 (batch_size=10, num_features=512)
    input_tensor = torch.randn(10, num_features)
    
    # 前向传播
    output_tensor = mybatchnorm(input_tensor)
    
    print("输入张量形状:", input_tensor.shape)
    print("输出张量形状:", output_tensor.shape)
```

### 2. LayerNorm

```python
import torch
import torch.nn as nn

class LayerNorm(nn.Module):
    """
    Layer Normalization (LayerNorm) 层实现。
    LayerNorm 在每个样本的特征维度上进行归一化，与 BatchNorm 不同，它对批量中的样本独立操作。
    适用于 Transformer 模型，稳定训练过程。
    """
    def __init__(self, normalized_shape, eps=1e-5):
        """
        初始化 LayerNorm。
        
        参数:
            normalized_shape: 要归一化的特征维度，通常是最后一个维度的大小
            eps: 防止除零的小常数，默认为 1e-5
        """
        super(LayerNorm, self).__init__()
        self.weight = nn.Parameter(torch.ones(normalized_shape))  # 可学习缩放参数，初始化为 1
        self.bias = nn.Parameter(torch.zeros(normalized_shape))  # 可学习偏移参数，初始化为 0
        self.eps = eps  # 数值稳定性常数

    def forward(self, x):
        """
        前向传播实现 LayerNorm。
        
        参数:
            x: 输入张量，形状为 (batch_size, seq_len, features) 或其他形状
        
        返回:
            y: 归一化后的输出张量
        原理:
            1. 对每个样本的特征维度计算均值和方差。
            2. 标准化：(x - mean) / sqrt(var + eps)。
            3. 应用可学习参数 weight 和 bias。
        """
        # 计算每个样本的均值，沿着特征维度（通常是最后一个维度）
        mean = x.mean(dim=-1, keepdim=True)
        
        # 计算每个样本的方差
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        
        # 标准化
        x_normalized = (x - mean) / torch.sqrt(var + self.eps)
        
        # 应用可学习权重和偏置
        y = self.weight * x_normalized + self.bias
        
        return y
```

### 3. RMSNorm

```python
class RMSNorm(nn.Module):
    """
    Root Mean Square Layer Normalization (RMSNorm) 层实现。
    RMSNorm 仅基于均方根 (RMS) 进行归一化，不计算均值偏移，计算更简单。
    常用于高效 Transformer 模型。
    """
    def __init__(self, normalized_shape, eps=1e-5):
        """
        初始化 RMSNorm。
        
        参数:
            normalized_shape: 要归一化的特征维度
            eps: 防止除零的小常数，默认为 1e-5
        """
        super(RMSNorm, self).__init__()
        self.weight = nn.Parameter(torch.ones(normalized_shape))  # 可学习缩放参数，初始化为 1
        self.eps = eps  # 数值稳定性常数

    def forward(self, x):
        """
        前向传播实现 RMSNorm。
        
        参数:
            x: 输入张量，形状为 (batch_size, seq_len, features) 或其他形状
        
        返回:
            y: 归一化后的输出张量
        原理:
            1. 计算每个样本的均方根 (RMS)，即 sqrt(mean(x^2) + eps)。
            2. 标准化：x / RMS。
            3. 应用可学习权重。
            RMSNorm 不移除均值偏移，仅基于方差尺度调整。
        """
        # 计算每个样本的均方根，沿着特征维度
        rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)
        
        # 标准化
        x_normalized = x / rms
        
        # 应用可学习权重
        y = self.weight * x_normalized
        
        return y
```

