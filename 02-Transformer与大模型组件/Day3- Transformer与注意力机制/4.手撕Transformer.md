# 4. 手撕Transformer

b站指路：[带你手撕Transformer实战_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1MGrTYVEXq/?spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=aa6afb9d0536d09ecdcb5d2c1fcf4c79)

基础知识学习：[第一章：自然语言处理 · Transformers快速入门](https://transformers.run/c1/nlp/#nnlm-模型)

```python
from torch import nn
import torch
"""
    input shape: [batch,seq_len,d_model]
"""
# 位置编码
class PositionEncoding(nn.Module):
    def __init__(self,d_model,max_seq_len=512):
        super().__init__()
        # shape: [max_seq_len,1] max_seq_len为最大序列长度，后面根据实际的输入序列长度进行切片
        # 位置编码的公式为：
        # PE(pos,2i) = sin(pos/(10000^(2i/d_model)))
        # PE(pos,2i+1) = cos(pos/(10000^(2i/d_model)))
        # pos为位置，i为维度索引，d_model为模型的维度
        # 这里的d_model是指模型的维度，max_seq_len是指最大序列长度
        postion = torch.arange(0,max_seq_len).unsqueeze(1)# unsqueeze表示并在第i维上增加一个维度

        item = 1/10000**(torch.arange(0,d_model,2)/d_model)

        tmp_pos = postion*item

        pe = torch.zeros(max_seq_len,d_model)

        pe[:,0::2] = torch.sin(tmp_pos) #: 表示选取所有行（第 0 维）。0::2 表示从第 1 维的索引 0 开始，每隔 2 个元素选取一个，直到该维度结束
        pe[:,1::2] = torch.cos(tmp_pos)
        pe = pe.unsqueeze(0)
        self.register_buffer("pe",pe,False)

    def forward(self,x):
        batch,seq_len,_ = x.shape #x.shape 表示张量 x 在每个维度上的大小，是一个元组
        pe = self.pe
        return x+pe[:,:seq_len,:]

# 计算注意力分数
def attention(query,key,value,mask=None):
    d_model = key.shape[-1]
    # query,key,value shape:[batch,seq_len,d_model]
    #attetion注意力计算公式：Attention(Q, K, V) = Softmax(QK^T / sqrt(d_k)) * V
    att_ = torch.matmul(query,key.transpose(-2,-1))/d_model**0.5

    if mask is not None:
        att_ = att_.masked_fill_(mask,-1e9)
    
    att_score = torch.softmax(att_,-1)
    return torch.matmul(att_score,value)

# 多头注意力机制的实现
class MultiHeadAttention(nn.Module):
    def __init__(self,heads,d_model,dropout=0.1):
        super().__init__()
        assert d_model % heads == 0
        # q k v通过线性变换，将输入的特征映射到不同的子空间，使得每个注意力头可以专注于不同的特征。
        self.q_linear = nn.Linear(d_model,d_model,bias=False)
        self.k_linear = nn.Linear(d_model,d_model,bias=False)
        self.v_linear = nn.Linear(d_model,d_model,bias=False)
        self.linear = nn.Linear(d_model,d_model,bias=False)
        # dropout用于防止过拟合
        self.dropout = nn.Dropout(dropout)
        self.heads = heads
        self.d_k = d_model // heads
        self.d_model = d_model

    def forward(self,q,k,v,mask=None):
        # 最终目标：[n,seq_len,d_model] -> [n,heads,seq_len,d_k]
        # reshape的目的是将d_model维度分成多个头，每个头的维度为d_k [n,seq_len,d_model] -> [n,seq_len,heads,d_k]
        # self.q_linear 是一个线性层（nn.Linear），它的作用就是实现 Q = Wq * x。
        q = self.q_linear(q).reshape(q.shape[0],-1,self.heads,self.d_k).transpose(1,2)
        k = self.k_linear(k).reshape(k.shape[0],-1,self.heads,self.d_k).transpose(1,2)
        v = self.v_linear(v).reshape(v.shape[0],-1,self.heads,self.d_k).transpose(1,2)
        out = attention(q,k,v,mask)
        out = out.transpose(1,2).reshape(out.shape[0],-1,self.d_model)
        out = self.linear(out)
        out = self.dropout(out)
        return out

# FNN层实现
class FeedForward(nn.Module):
    # 参数：d_ff是前馈网络的隐藏层维度，d_model是输入和输出的维度，dropout是dropout的概率
    def __init__(self,d_model,d_ff,dropout=0.1): 
        super().__init__()
        self.ffn = nn.Sequential(
            # 通过线性层将输入的特征映射到更大的隐藏层维度d_ff，然后通过ReLU激活函数进行非线性变换，使模型学到更多的知识，最后通过线性层将特征映射回d_model维度
            nn.Linear(d_model,d_ff,bias=False),
            nn.ReLU(),
            nn.Linear(d_ff,d_model,bias=False),
            nn.Dropout(dropout)
        )
    
    def forward(self,x):
        return self.ffn(x)

 # 编码层全结构的实现   
class EncoderLayer(nn.Module):
    def __init__(self,heads,d_model,d_ff,dropout=0.1):
        super().__init__()
        self.self_multi_head_att = MultiHeadAttention(heads,d_model,dropout)
        self.ffn = FeedForward(d_model,d_ff,dropout)
        # 创建两个 LayerNorm 层
        self.norms = nn.ModuleList([nn.LayerNorm(d_model) for i in range(2)])
        self.dropout = nn.Dropout(dropout)
    
    def forward(self,x,mask=None):
        multi_head_att_out = self.self_multi_head_att(x,x,x,mask)
        multi_head_att_out = self.norms[0](x+multi_head_att_out)
        ffn_out = self.ffn(multi_head_att_out)
        ffn_out = self.norms[1](multi_head_att_out+ffn_out)
        out = self.dropout(ffn_out)
        return out

# 编码器的实现,将上面已实现的全部组合起来
class Encoder(nn.Module):
    def __init__(self,vocab_size,pad_idx,d_model,num_layes,heads,d_ff,dropout=0.1,max_seq_len=512):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size,d_model,pad_idx)
        self.position_encode = PositionEncoding(d_model,max_seq_len)
        self.encode_layers = nn.ModuleList([EncoderLayer(heads,d_model,d_ff,dropout) for i in range(num_layes)])
    
    def forward(self,x,src_mask=None):
        embed_x = self.embedding(x)
        pos_encode_x = self.position_encode(embed_x)
        for layer in self.encode_layers:
            pos_encode_x = layer(pos_encode_x,src_mask)
        return pos_encode_x   


# 解码器层的堆叠
# 解码器层包含了自注意力机制、编码器-解码器注意力机制和前馈网络
class DecoderLayer(nn.Module):
    def __init__(self,heads,d_model,d_ff,dropout=0.1):
        super().__init__()
        self.masked_att = MultiHeadAttention(heads,d_model,dropout)
        self.att = MultiHeadAttention(heads,d_model,dropout)
        self.norms = nn.ModuleList([nn.LayerNorm(d_model) for i in range(3)])
        self.ffn = FeedForward(d_model,d_ff,dropout)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self,x,encode_kv,dst_mask=None,src_dst_mask=None):
        mask_att_out = self.masked_att(x,x,x,dst_mask)
        mask_att_out = self.norms[0](x+mask_att_out)
        att_out = self.att(mask_att_out,encode_kv,encode_kv,src_dst_mask)
        att_out = self.norms[1](mask_att_out+att_out)
        ffn_out = self.ffn(att_out)
        ffn_out = self.norms[2](att_out+ffn_out)
        out = self.dropout(ffn_out)
        return out

# 解码器的实现
class Decoder(nn.Module):
    def __init__(self,vocab_size,pad_idx,d_model,num_layes,heads,d_ff,dropout=0.1,max_seq_len=512):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size,d_model,pad_idx)
        self.position_encode = PositionEncoding(d_model,max_seq_len)
        self.decoder_layes = nn.ModuleList([DecoderLayer(heads,d_model,d_ff,dropout) for i in range(num_layes)])
    
    def forward(self,x,encoder_kv,dst_mask=None,src_dst_mask=None):
        embed_x = self.embedding(x)
        pos_encode_x = self.position_encode(embed_x)
        for layer in self.decoder_layes:
            pos_encode_x = layer(pos_encode_x,encoder_kv,dst_mask,src_dst_mask)
        return pos_encode_x


# Transformer模型的实现
class Transformer(nn.Module):
    def __init__(self,enc_vocab_size,dec_vocab_size,pad_idx,d_model,num_layes,heads,d_ff,dropout=0.1,max_seq_len=512):
        super().__init__()
        self.encoder = Encoder(enc_vocab_size,pad_idx,d_model,num_layes,heads,d_ff,dropout,max_seq_len)
        self.decoder = Decoder(dec_vocab_size,pad_idx,d_model,num_layes,heads,d_ff,dropout,max_seq_len)
        self.linear = nn.Linear(d_model,dec_vocab_size)
        self.pad_idx = pad_idx

    
    def generate_mask(self,query,key,is_triu_mask=False):
        '''
            生成上三角掩码矩阵mask,后续在注意力机制时，在需要mask的地方填充-1e9
            query,key shape：[batch,seq_len]
            query 和 key 是从 x 中提取的索引值，所以形状为 [batch, seq_len] 注意与q k v 的区别
        '''
        device = query.device # 获取张量所在的设备，确保后续计算中所有张量在同一个设备上。
        batch,seq_q = query.shape
        _,seq_k = key.shape
        # pad_idx = 0 用于表示词向量的填充标识
        # 首先将 key 中的 pad_idx 的位置设置为 True，其他位置设置为 False
        # 这样可以确保在计算注意力分数时，填充的部分不会影响结果。填充的部分就是句子不够长补齐的位置。
        # 将掩码扩展到 [batch, 1, seq_q, seq_k] 的形状。
        mask = (key==self.pad_idx).unsqueeze(1).unsqueeze(2)
        mask = mask.expand(batch,1,seq_q,seq_k).to(device)
        # 如果 is_triu_mask 为 True，则生成上三角掩码矩阵
        # 上三角掩码矩阵的作用是确保在解码时，当前时刻只能看到之前的时刻，不能看到未来的时刻
        # 这里的 diagonal=1 表示上三角矩阵的对角线以上的部分
        if is_triu_mask:
            dst_triu_mask = torch.triu(torch.ones(seq_q,seq_k,dtype=torch.bool),diagonal=1)
            dst_triu_mask = dst_triu_mask.unsqueeze(0).unsqueeze(1).expand(batch,1,seq_q,seq_k).to(device)
            # 按位或 合并掩码，有一个为true，则对应位置为true
            return mask|dst_triu_mask
        return mask


    def forward(self,src,dst):
        # src shape: [batch,seq_len] dst shape: [batch,seq_len]
        # src 是源序列，通常是输入序列
        # dst 是目标序列，通常是输出序列
        src_mask = self.generate_mask(src,src)
        encoder_out = self.encoder(src,src_mask)
        dst_mask = self.generate_mask(dst,dst,True)
        src_dst_mask = self.generate_mask(dst,src)
        decoder_out = self.decoder(dst,encoder_out,dst_mask,src_dst_mask)
        out = self.linear(decoder_out)
        return out


if __name__=="__main__":
    # PositionEncoding(512,100)
    att = Transformer(100,200,0,512,6,8,1024,0.1)
    x = torch.randint(0,100,(4,64))
    y = torch.randint(0,200,(4,64))
    out = att(x,y)
    print(out.shape)
```

