# Norm归一化方法

## 一、归一化的核心意义：解决「内部协变量偏移」

深度网络训练中，每层的输入分布会因前层参数更新而**动态变化**（即 **内部协变量偏移**），导致：

- **激活函数饱和**：如 Sigmoid/Tanh 的输入落入饱和区，梯度消失，学习停滞。
- **优化器震荡**：损失曲面崎岖，优化器难收敛。

归一化的目标是 **固定每层输入的分布**（如均值 0、方差 1），稳定训练过程。

## 第一部分：BatchNorm、LayerNorm、RMSNorm 深度对比

### 1. BatchNorm（BN）：跨样本同特征归一化

#### 原理与公式

对**同一 Batch 内的相同特征维度**计算统计量（均值/方差），实现归一化。
假设 Batch 数据为 $$ X \in \mathbb{R}^{B \times D} $$（$$ B $$：批次大小，$$ D $$：特征维度），对第 $$ k $$ 个特征：

1. **批次均值**：$$ \mu_k = \frac{1}{B} \sum_{i=1}^B X_{i,k} $$
2. **批次方差**：$$ \sigma_k^2 = \frac{1}{B} \sum_{i=1}^B (X_{i,k} - \mu_k)^2 $$
3. **归一化**：$$ \hat{X}*{i,k} = \frac{X*{i,k} - \mu_k}{\sqrt{\sigma_k^2 + \epsilon}} $$（$$ \epsilon $$ 防除零）
4. **缩放平移**：$$ Y_{i,k} = \gamma_k \hat{X}_{i,k} + \beta_k $$（$$ \gamma, \beta $$ 可学习，恢复表达能力）

#### 优缺点与适用场景

| **优点**                     | **缺点**                                                     | **适用场景**                                             |
| ---------------------------- | ------------------------------------------------------------ | -------------------------------------------------------- |
| 稳定梯度，加速收敛           | 强依赖 **Batch Size**：  - 小 Batch 时，统计量不稳定；  - 大 Batch 占内存 | 计算机视觉（CNN）：  图像尺寸固定，Batch 大（如 64/128） |
| 允许更大学习率，提升泛化能力 | 不适用于序列模型（如 NLP）：  序列长度可变，Batch 内样本长度不同，无法统一计算跨样本统计量 | 固定尺寸任务（如图像分类）                               |

#### 案例

- **ResNet**：用 BN 稳定 CNN 训练，使上百层网络可训。
- **局限性**：NLP 中，若 Batch 内序列长度不同（如句子长短不一），BN 无法计算统一的跨样本统计量，因此被 LN 替代。

### 2. LayerNorm（LN）：同一样本跨特征归一化

#### 原理与公式

对**单个样本的所有特征维度**计算统计量（均值/方差），适配序列模型（如 Transformer）。
假设输入向量为 $$ \boldsymbol{x} \in \mathbb{R}^d $$（$$ d $$：特征维度）：

1. **样本均值**：$$ \mu = \frac{1}{d} \sum_{i=1}^d x_i $$
2. **样本方差**：$$ \sigma^2 = \frac{1}{d} \sum_{i=1}^d (x_i - \mu)^2 $$
3. **归  一化**：$$ \hat{x}_i = \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} $$  ,  **ϵ（epsilon）** 是一个**极小的正数**，它的核心作用是**防止方差过小、分母为零或数值计算不稳定**。
4. **缩放平移**：$$ y_i = \gamma \hat{x}_i + \beta $$（$$ \gamma, \beta $$ 可学习）

#### 优缺点与适用场景

| **优点**                                                  | **缺点**                                               | **适用场景**                                                 |
| --------------------------------------------------------- | ------------------------------------------------------ | ------------------------------------------------------------ |
| 不依赖 Batch Size：  仅需单个样本的特征，适配序列长度变化 | 计算比 RMSNorm 复杂：  需同时算均值和方差              | NLP 序列模型（如 Transformer、RNN）：  每个 token 的向量独立归一化 |
| 适配序列模型：  每个时间步/token 的向量可独立归一化       | 对特征维度敏感：  若特征维度分布不均，归一化效果受影响 | 需严格均值归一化的场景（如早期 Transformer）                 |

#### 案例

- **BERT/Transformer 原始论文**：用 LN 稳定残差块训练，适配可变长文本。
- **细节**：LN 对每个 token 的词向量（如 768 维）单独归一化，不管 Batch 内其他 token，因此适合 NLP。

### 3. RMSNorm：LN 的简化版（无均值中心化）

#### 原理与公式

省略**均值计算**，仅通过**均方根（RMS）**缩放，效率更高（大语言模型常用）。
假设输入向量为 $$ \boldsymbol{x} \in \mathbb{R}^d $$：

1. **均方根（无均值）**：
   $$ \text{RMS}(\boldsymbol{x}) = \sqrt{\frac{1}{d} \sum_{i=1}^d x_i^2} $$
2. **归一化**：
   $$ \hat{x}_i = \frac{x_i}{\text{RMS}(\boldsymbol{x}) + \epsilon} $$
3. **缩放**：$$ y_i = \gamma \hat{x}_i $$（仅 $$ \gamma $$ 可学习，无平移参数 $$ \beta $$）

#### **注：为什么最后都需要乘以可学习的参数？**

归一化（如 RMSNorm）的本质是 **约束特征的尺度**（让均方根接近 1），但这会带来两个潜在问题：

1. **丢失固有幅度信息**：不同特征维度的 “天然重要性” 可能被归一化抹除（比如某些维度的数值本就该更大，却被强行缩放到 1 附近）。
2. **限制模型灵活性**：严格的归一化可能让模型无法根据任务需求调整特征的整体幅度（比如某些层需要更大的梯度，或某些场景下特征需要更突出）。

**可学习的缩放参数（`self.weight`）的作用，就是让模型在归一化的基础上，重新赋予特征灵活的幅度调整能力** —— 既保证训练稳定（归一化的作用），又保留拟合数据的灵活性（缩放的作用）。

#### 优缺点与适用场景

| **优点**                                 | **缺点**                                                     | **适用场景**                                                |
| ---------------------------------------- | ------------------------------------------------------------ | ----------------------------------------------------------- |
| 比 LN 更高效：  省均值计算，训练速度更快 | 无均值中心化：  若数据均值偏移大，可能损失信息（ReLU 下影响小） | 大语言模型（如 Llama、GPT-4）：  追求训练效率，控制向量尺度 |
| 不依赖 Batch Size（同 LN）               | 功能略弱于 LN：  无法修正均值偏移                            | 对均值不敏感的场景（如 ReLU 激活后，输出均值接近 0）        |

#### 案例

- **Llama 系列模型**：用 RMSNorm 替代 LN，减少计算量，支撑千亿参数训练。
- **代码细节**：如 RMSNorm 实现，核心是 $$ \text{torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + eps)} $$，仅计算方差的平方根。

```python
import torch
import torch.nn as nn

class RMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))  # 可学习的缩放参数
    
    def _norm(self, x):
        # 计算均方根（RMS）并归一化：x / sqrt( (x²的均值) + eps )
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
    
    def forward(self, x):
        # 处理数据类型（如FP16），保证计算稳定
        output = self._norm(x.float()).type_as(x)
        # 应用可学习的缩放
        return output * self.weight
```

### 4. 三者对比表格（核心维度）

| **维度**       | BatchNorm             | LayerNorm            | RMSNorm                    |
| -------------- | --------------------- | -------------------- | -------------------------- |
| **归一化对象** | 跨样本，同特征        | 同一样本，跨特征     | 同一样本，跨特征（无均值） |
| **计算开销**   | 高（Batch 均值/方差） | 中（样本均值/方差）  | 低（仅样本方差）           |
| **Batch 依赖** | 强（小 Batch 失效）   | 无                   | 无                         |
| **典型应用**   | CV（ResNet）          | NLP 基础模型（BERT） | 大语言模型（Llama）        |

BatchNorm与LayerNorm两者都是在特征的维度上进行归一化，但是batch是在整个批次内对一个特征做归一化，layer是对样本的所以特征值做归一化

![image-20250704144752845](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250704144752845.png)

## 第二部分：

## Pre-Norm vs Post-Norm（Transformer 残差块中的 Norm 位置）

### 1. 定义与公式推导

Transformer 的残差块结构：$$ \text{Block} = \text{Norm}(\boldsymbol{x} + \text{Sublayer}(\boldsymbol{x})) $$（Post-Norm）或 $$ \text{Block} = \boldsymbol{x} + \text{Sublayer}(\text{Norm}(\boldsymbol{x})) $$（Pre-Norm）。

#### Post-Norm（Norm 在残差后）

**公式**：
$$ \boldsymbol{x}_{t+1} = \text{Norm}\left( \boldsymbol{x}_t + F_t(\boldsymbol{x}_t) \right) $$

- **逻辑**：先计算子层（如自注意力），再残差相加，最后 Norm。
- **展开迭代**：
  $$ \boldsymbol{x}*L = \text{Norm}\left( \boldsymbol{x}*{L-1} + F_{L-1}(\boldsymbol{x}*{L-1}) \right) = \text{Norm}\left( \text{Norm}(\dots) + F*{L-1}(\text{Norm}(\dots)) \right) $$
  残差直接累加，**保留层间幅度差异**，更能体现“深度”的优势，但易因梯度爆炸难训。

#### Pre-Norm（Norm 在残差前）

**公式**：
$$ \boldsymbol{x}_{t+1} = \boldsymbol{x}_t + F_t\left( \text{Norm}(\boldsymbol{x}_t) \right) $$

- **逻辑**：先 Norm，再计算子层，最后残差相加。
- **展开迭代**：
  $$ \boldsymbol{x}_L = \boldsymbol{x}*0 + \sum*{t=0}^{L-1} F_t\left( \text{Norm}(\boldsymbol{x}_t) \right) $$
  每层的输入是 **Norm 后的稳定值**，梯度更平缓，但 **层间更新量相近**，深层时更像“加宽”而非“加深”，长依赖捕捉能力弱（最优设置下）。

### 2. 核心区别：训练稳定性 vs 模型表达力

| **维度**     | Pre-Norm                               | Post-Norm                                    |
| ------------ | -------------------------------------- | -------------------------------------------- |
| **梯度特性** | 梯度稳定，不易消失                     | 梯度易爆炸（需 Warmup 和小学习率）           |
| **模型能力** | 深层时更“宽”（层更新量相近）           | 保留深度优势，易学长距离依赖                 |
| **训练难度** | 易（无需复杂调度）                     | 难（需 Warmup+小学习率）                     |
| **微调效果** | 对浅层参数更新强（可能破坏预训练知识） | 对浅层参数更新弱（保护预训练知识，微调更好） |

### 3. 优缺点与模型应用

#### Pre-Norm 的优缺点

- 优点：
  - **训练稳定**：Norm 提前“熨平”输入分布，梯度不会因深层而消失，适合 **超深层模型**（如 GPT-3 的 96 层）。
  - **收敛快**：可直接用较大学习率（如 $$ 1 \times 10^{-4} $$），无需 Warmup。
- 缺点：
  - **表达力上限低**：深层时层间差异被 Norm 削弱，长距离依赖捕捉能力弱于 Post-Norm（需堆叠更多层补偿）。

#### Post-Norm 的优缺点

- 优点：
  - **性能上限高**：残差直接累加，保留层间幅度差异，更易捕捉长距离依赖（如 BERT 的 12 层 Post-Norm 性能优于同参数 Pre-Norm）。
  - **微调友好**：对浅层参数更新弱，预训练模型微调时更稳定（如 BERT 微调任务）。
- 缺点：
  - **训练难**：初始学习率不能太大，需 **Warmup**（学习率先从 $$ 1 \times 10^{-6} $$ 增至 $$ 1 \times 10^{-4} $$，再下降），否则前几层梯度爆炸。

#### 模型应用案例

- Post-Norm：
  - **BERT（12 层）**：用 Post-Norm+Warmup，追求 12 层内的极致性能。
  - **原始 Transformer 论文**：Post-Norm 是默认选择，配合 Warmup 调度。
- Pre-Norm：
  - **GPT-3（96 层）、Llama（32/65 层）**：超深层模型必须用 Pre-Norm 稳定训练，牺牲部分长依赖换稳定性。

### 4. 实践调参建议

- Post-Norm 训练：
  - 必须用 **Warmup**（如前 1000 步学习率从 $$ 1 \times 10^{-6} $$ 增至 $$ 1 \times 10^{-4} $$，再衰减）。
  - 初始学习率要小（如 $$ 1 \times 10^{-5} $$），避免梯度爆炸。
- Pre-Norm 训练：
  - 可直接用较大学习率（如 $$ 1 \times 10^{-4} $$），无需 Warmup。
  - 若任务需长距离依赖，可堆叠更多层（如 100 层 Pre-Norm 替代 60 层 Post-Norm）。





#### 总结：怎么选哪个Norm归一化方法呢？

1. 归一化方法选择：
   - **CV+大 Batch** → BN；
   - **NLP 基础模型+需均值归一化** → LN；
   - **大语言模型+追求效率** → RMSNorm。
2. Norm 位置选择（Pre/Post）：
   - **浅层模型（<30 层）+性能优先** → Post-Norm+Warmup；
   - **深层模型（>30 层）+稳定优先** → Pre-Norm；
   - **微调任务** → 优先选 Post-Norm（保护预训练知识）。