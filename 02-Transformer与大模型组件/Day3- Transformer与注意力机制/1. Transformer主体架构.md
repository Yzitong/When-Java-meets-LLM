# 1. Transformer核心结构

#### 引言

Transformer 模型是一种革命性的深度学习架构，于 2017 年由 Vaswani 等人在论文《Attention Is All You Need》中提出。它最初被设计用于机器翻译任务，但由于其强大的能力和灵活性，迅速成为自然语言处理（NLP）领域的基石，并扩展到计算机视觉、语音识别等其他领域。

与传统的循环神经网络（RNN）和卷积神经网络（CNN）相比，Transformer 的主要优势在于其高效的并行化能力和捕捉长距离依赖的能力。这些优势源于其核心的自注意力机制（self-attention），无需依赖 RNN 的循环结构或 CNN 的局部卷积操作。

《Attention is all your need》：[[1706.03762\] Attention Is All You Need](https://arxiv.org/abs/1706.03762)

B站指路：快速了解→[《Attention is all you need》论文解读及Transformer架构详细介绍_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1xoJwzDESD/?spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=aa6afb9d0536d09ecdcb5d2c1fcf4c79)

​		  论文详解→[Transformer论文逐段精读【论文精读】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=aa6afb9d0536d09ecdcb5d2c1fcf4c79)

​		  代码解读→[Transformer代码(源码Pytorch版本)从零解读(Pytorch版本）_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1dR4y1E7aL/?spm_id_from=333.1387.homepage.video_card.click&vd_source=aa6afb9d0536d09ecdcb5d2c1fcf4c79)

#### Transformer 相较于 RNN 和 CNN 的优势
- **RNN 的局限性**：
  - **串行处理：**RNN 通过循环结构处理序列数据，但由于其逐步处理的特性，无法实现高效的并行化，在长序列上训练速度较慢。
  - **长距离遗忘问题：**RNN 容易出现梯度消失或梯度爆炸问题，导致在处理长距离依赖时表现不佳。例如，在长句子的翻译中，早期信息可能被遗忘。
- **CNN 的局限性**：
  - **上下文语义的缺失：**CNN 擅长处理局部特征，但在处理序列数据时，**无法直接捕捉全局依赖关系**。例如，在句子中，远距离词之间的语义关联可能被忽略。
  - CNN 的卷积核通常假设局部相关性，难以扩展到长序列的全局建模。
- **Transformer 的解决方案**：
  - **自注意力机制**：允许模型直接关注序列中的任意位置，从而捕捉长距离依赖。例如，在翻译任务中，“it”可能与句子的开头词相关，Transformer 可直接建模。
  - **完全并行化**：Transformer 可以同时处理整个序列，显著提高训练和推理速度，尤其适合 GPU 加速。文献如 [理解语言的 Transformer 模型](https://www.tensorflow.org/tutorials/text/transformer?hl=zh-cn) 强调了这一优势。
  - **无需循环或卷积**：通过纯注意力机制，Transformer 避免了 RNN 的循环结构和 CNN 的局部卷积操作，实现了更高的灵活性和效率。

以下表格总结了三者的对比：

| 模型类型    | 并行化能力 | 长距离依赖 | 训练速度 | 适用场景        |
| ----------- | ---------- | ---------- | -------- | --------------- |
| RNN         | 差         | 一般       | 慢       | 短序列任务      |
| CNN         | 一般       | 较弱       | 中等     | 图像、局部特征  |
| Transformer | 优秀       | 优秀       | 快       | NLP、长序列任务 |

#### Transformer 的核心结构
Transformer 包括嵌入（embedding）、编码器（encoder）和解码器（decoder）三部分：  

- 嵌入将词转为向量，并添加位置信息。  
- 编码器处理输入序列，包含多头自注意力机制和前馈网络。  
- 解码器生成输出序列，结合编码器结果和掩码自注意力。

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250630202032430.png" alt="image-20250630202032430" style="zoom:80%;" />

##### 1. 嵌入（Embedding）
- **输入转换**：输入序列（如单词序列）首先被映射到一个固定维度的向量空间中。其中每个词被转换为一个维度为 512的嵌入向量。然后引入位置编码，位置编码也是512维的向量，即为整个嵌入的过程。
  - 输入的embedding可以通过word2vec，bert等方式获取，目的是将文本映射到连续的向量空间。
  - 位置编码是为了捕捉输入中token的顺序信息，常用的有RoPE，绝对位置编码等
  - 假设输入的是“我 爱 水 课”，那么输入矩阵X，就是n*d=4 * 512的大小。n为分词后的序列长度。
- **位置编码（Positional Encoding）**：
  - 由于 Transformer 没有循环结构，无法直接捕捉词序信息，因此需要引入位置编码。
  - 位置编码是一个固定函数，不需要学习，其目的是为每个位置的嵌入向量添加位置信息。
  - **计算公式**：
    对于位置 \(pos\) 和维度 \(i\)：位置编码向量的维度与词向量的维度对齐都是512
  - $$PE(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)$$
  - $$PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)$$
  - 其中 \($$d_{\text{model}} = 512$$\)。
  - **举例说明**：
    - 对于位置 \(pos = 0\) 和维度 \(i = 0\)：
      - 当 ( i = 0 )（偶数，( 2k = 0 )，( k = 0 )）： $$ PE(0, 0) = \sin\left(\frac{0}{10000^{0 / 512}}\right) = \sin(0) = 0 $$
      - 当 ( i = 1 )（奇数，( 2k+1 = 1 )，( k = 0 )）： $$ PE(0, 1) = \cos\left(\frac{0}{10000^{0 / 512}}\right) = \cos(0) = 1 $$
    - 对于位置 \(pos = 1\) 和维度 \(i = 0\)：
      - 当 ( i = 0 )（偶数，( 2k = 0 )，( k = 0 )）： $$ PE(1, 0) = \sin\left(\frac{1}{10000^{0 / 512}}\right) = \sin(1) \approx 0.8415 $$
      - 当 ( i = 1 )（奇数，( 2k+1 = 1 )，( k = 0 )）： $$ PE(1, 1) = \cos\left(\frac{1}{10000^{0 / 512}}\right) = \cos(1) \approx 0.5403 $$
    - 这样，每个位置的嵌入向量都会被添加一个独特的位置编码向量，确保模型能感知序列顺序。

##### 2. 编码器（Encoder）
###### 编码器的详细过程
- **结构**：编码器由 \(N = 6\) 个相同的层组成，每层包含两个子层：
  1. **多头自注意力（Multi-Head Self-Attention）**：捕捉上下文语义信息。
  2. **位置-wise 前馈神经网络（Position-wise Feed-Forward Network）**：引入非线性变换，使大模型学到更多的东西。
- **残差连接（Residual Connection）**：
  - 每个子层都有残差连接，即将输入直接添加到子层的输出中。
  - 作用：帮助梯度在深层网络中更好地传播，缓解梯度消失问题。
- **层归一化（Layer Normalization）**：
  - 每个子层之后都应用层归一化。
  - 作用：稳定化训练过程，确保不同层的输入分布一致。
  - <img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250702133237970.png" alt="image-20250702133237970" style="zoom:67%;" />

###### 为什么需要 FNN？

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250702133456717.png" alt="image-20250702133456717" style="zoom:64%;" />

- **FNN 的作用**：
  - 前馈神经网络（FNN）是一个两层全连接网络（通常是 ReLU 激活函数），应用于每个位置的向量上。
  - 它引入非线性性，使模型能够学习更复杂的特征表示。
- **举例说明**：
  - 假设输入序列为 "I love cats"，经过自注意力层后，每个词的表示已经包含了序列中其他词的信息。
  - FNN 则进一步处理这些表示，提取更高层次的特征，例如情感或语义信息。

###### 为什么要有残差连接？
- 残差连接通过将输入直接添加到子层的输出中，**确保了即使子层学习能力不足，模型也能传递原始信息。**
- 这在深层网络中尤为重要，防止梯度消失。例如，在 6 层编码器中，残差连接确保深层信息不丢失。

###### 为什么要归一化？
- 归一化（Layer Normalization）确保每个层的输入分布稳定，避免数值不稳定性，提高训练效率。
- 例如，在训练过程中，层归一化帮助模型适应不同输入规模，加速收敛。

##### 3. 解码器（Decoder）
###### 解码器的详细过程
- **结构**：解码器也由 \(N = 6\) 个相同的层组成，每层包含三个子层：
  1. **掩码多头自注意力（Masked Multi-Head Self-Attention）**
  2. **编码器-解码器注意力（Encoder-Decoder Attention）**
  3. **位置-wise 前馈神经网络**
  
- **掩码矩阵：**解码的过程中为了让模型只能看到之前的预测结果，使用一个上三角的掩码矩阵。只将之前预测的输出作为当前预测的输入，通过掩码矩阵防止第i个文本智道i+1个文本之后的信息。
  
  ![image-20250702134140287](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250702134140287.png)
  
  ![image-20250702134200410](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250702134200410.png)
  
- **残差连接和层归一化**：与编码器类似，每个子层都有残差连接和层归一化。

- **cross-attention交叉注意力机制：**

  自注意力的K和V矩阵是由encoder的编码信息矩阵计算得到的，Q是由上一个decoder bock计算得到的。

###### 为什么 masked 后的结果作为 Q？
- 在解码器的自注意力中，查询（Q）来自解码器的当前层输出。
- 为了保持自回归性（即生成当前词时只能看到之前的词），需要对自注意力施加掩码，确保当前位置不能看到后续位置。
- 例如，在生成句子时，当前词“is”不能依赖后面的“blue”，掩码确保模型只看之前的词。

###### 为什么 encoder 的结果作为 K、V？
- 在编码器-解码器注意力中，解码器需要关注编码器的输出（即输入序列的表示）。
- 因此，编码器的输出作为键（K）和值（V），而解码器的输出作为查询（Q）。
- 这允许解码器在生成输出时参考整个输入序列。例如，在翻译中，解码器可直接利用编码器的上下文信息。

#### 总结
Transformer 模型通过自注意力机制和前馈神经网络的组合，实现了高效的序列处理能力。它解决了 RNN 在并行化和长距离依赖方面的局限性，并在各种 NLP 任务中取得了突破性进展。嵌入、编码器和解码器的设计，以及多头注意力机制和层归一化的使用，使其成为现代深度学习中的核心架构。

- ##### 关键要点

  - Transformer 模型的核心是编码器和解码器，通过自注意力机制处理序列数据。
  - 相较于 RNN 和 CNN，Transformer 解决了并行计算和长距离依赖问题。
  - 嵌入部分包括词嵌入和位置编码，位置编码用公式计算。
  - 编码器和解码器各有多个层，包含多头自注意力机制和前馈网络。
  - 多头注意力机制允许多个并行关注点，层归一化稳定训练。
  
- Transformer的三大贡献：

  - Muli-head Attention 用于捕捉词与词之间的关系，可以使计算并行。
  - FFN：对于捕捉到的词与词之间的特征进行学习，知识存储的位置，消耗大量的参数。
  - 残差连接：防止loss的消失。
