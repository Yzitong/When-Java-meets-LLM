# 2. 多头注意力机制与归一化方法

B站指路：[强烈推荐！台大李宏毅自注意力机制和Transformer详解！_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1v3411r78R/?spm_id_from=333.1387.favlist.content.click&vd_source=aa6afb9d0536d09ecdcb5d2c1fcf4c79)

### 引言

注意力机制的设计灵感源于人类对信息的处理方式 —— 我们在理解文本或图像时，会自然地关注重要部分，忽略次要信息。在深度学习中，它的核心作用是：

- **动态权重分配**：为输入序列中的每个元素（如句子中的单词、图像中的像素）计算一个 “注意力权重”，权重越高，代表该元素对当前任务的重要性越高。
- **上下文编码**：通过加权求和的方式，将关键信息整合到当前位置的表示中，形成包含上下文语义的编码。

### 1. 传统注意力机制与自注意力机制

- **传统注意力机制**（Attention）：
  - 在Transformer之前就有注意力机制，其含义是在序列到序列模型中，**解码器**通过注意力机制关注**输入序列**的不同部分。
  - 例如，在机器翻译中，解码器生成法语词时，会关注英语输入序列中相关词。

- **自注意力机制**（self-Attention）：
  - 查询（Q）、键（K）和值（V）都来自同一个序列。查询Q可以看做是一个问题，代表当前位置想 “找什么” ；K代表所有位置 “有什么” 信息，V是代表所有位置 “要融合的语义” 。通过注意力机制，来获取句子上下文的语义信息。
  - “self”主要体现在 Q、K、V **均来自输入序列本身**，而不是来自不同的序列。
  - 例如，在句子“猫在睡觉”中，自注意力允许“猫”直接与“睡觉”建立关系。
  - **注意力机制的计算公式**：
    ![image-20250630214113046](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250630214113046.png)

- **举例说明**：
  - 假设序列为 "我爱水课"，每个词的嵌入为 E1, E2, E3，E4。
  - 对于位置 2（"爱"），与其他3个字进行关联度的计算。
    - ![image-20250630220910787](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250630220910787.png)
    - ![image-20250630221002667](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250630221002667.png)
    - ![image-20250630221022139](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250630221022139.png)
  - 计算注意力分数：softmax( (Q2 K^T) / sqrt(d_k) )，然后与 V 相乘得到输出。
  - ![image-20250630221700836](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250630221700836.png)
- **为什么要除以根号dk呢？**

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250630223006226.png" alt="image-20250630223006226" style="zoom: 67%;" />

### 2. 为什么用多头注意力机制？

从上面可以看出，注意力机制就是计算上下文各个词彼此相似度的过程，现在想一个问题，“相似”是不是也可以分类呢？从各个子角度去判断词之间的相似度，会不会让模型对于上下文理解的更加透彻呢？比如两个水果，可以从“颜色”、“味道”、“价格”等等去判断其相似度从而更好的分析两个词之间的关系，最后再经过一个线性层将不同角度下得到信息融合起来，这就是多头注意力比其单头注意力做的事情。

多头注意力机制的核心思想是将输入的特征向量分成多个子空间（即多个头），每个头独立计算注意力分数，然后将结果合并。这种方式可以让模型从不同的角度关注输入序列的特征。把输入序列投影为多组不同的Query，Key，Value，并行分别计算后，再把各组计算的结果合并作为最终的结果。类似CNN中的多个channel，生成多个$W^q W^k W^v$。（V,K,Q）三个矩阵通过h个线性变换，分别得到h组（V,K,Q）矩阵，每一组（V,K,Q）经过Attention计算，得到h个Attention Score并进行拼接（Concat），最后通过一个线性变换得到输出，其维度与输入词向量的维度一致，其中h就是多头注意力机制的“头数”。

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250630223146794.png" alt="image-20250630223146794" style="zoom:67%;" />

- #### **多头注意力机制（Multi-Head Attention）详细计算过程**

###### 1. 输入与参数定义

- 输入序列：$$ X \in \mathbb{R}^{n \times d_{\text{model}}} $$，其中：
  - $$ n $$ 为序列长度；
  - $$ d_{\text{model}} = 512 $$ 为模型维度。
- 权重矩阵：
  - 第 $$ i $$ 个头的查询矩阵：$$ W_q^i \in \mathbb{R}^{d_{\text{model}} \times d_k} $$，其中 $$ i = 1, 2, \dots, h $$，$$ h = 8 $$ 为头数，$$ d_k = 64 $$ 为单头查询/键维度。
  - 第 $$ i $$ 个头的键矩阵：$$ W_k^i \in \mathbb{R}^{d_{\text{model}} \times d_k} $$。
  - 第 $$ i $$ 个头的值矩阵：$$ W_v^i \in \mathbb{R}^{d_{\text{model}} \times d_v} $$，其中 $$ d_v = 64 $$ 为单头值维度。
  - 输出投影矩阵：$$ W_o \in \mathbb{R}^{h \cdot d_v \times d_{\text{model}}} $$，其中 $$ h \cdot d_v = 8 \times 64 = 512 $$，与输入维度匹配。

###### 2. 分步公式

- 步骤1：生成多组 Q、K、V
  - 对每个头 $$ i $$，通过线性变换将输入 $$ X $$ 投影为对应头的查询（Q）、键（K）、值（V）：
    	$$ Q_i = X \cdot W_q^i \in \mathbb{R}^{n \times d_k} $$
    	$$ K_i = X \cdot W_k^i \in \mathbb{R}^{n \times d_k} $$
    	$$ V_i = X \cdot W_v^i \in \mathbb{R}^{n \times d_v} $$

- 步骤2：计算单头注意力
  - 第 $$ i $$ 个头的注意力输出（Scaled Dot-Product Attention）：
    	$$ \text{head}_i = \text{Attention}(Q_i, K_i, V_i) = \text{softmax}\left( \frac{Q_i \cdot K_i^T}{\sqrt{d_k}} \right) \cdot V_i $$
  - 其中：
    - $$ Q_i \cdot K_i^T \in \mathbb{R}^{n \times n} $$ 为注意力分数矩阵；
    - $$ \sqrt{d_k} $$ 为缩放因子，避免点积值过大导致 softmax 梯度消失；
    - $$ \text{softmax} $$ 将分数归一化为概率分布。

- 步骤3：拼接多头结果
  - 将 $$ h = 8 $$ 个头的输出按列拼接（Concat），维度为 $$ n \times (h \cdot d_v) $$：
    - $$ \text{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h) = \begin{bmatrix} \text{head}_1 & \text{head}_2 & \cdots & \text{head}_h \end{bmatrix} \in \mathbb{R}^{n \times (h \cdot d_v)} $$

- 步骤4：线性投影输出
  - 通过输出矩阵 $$ W_o $$ 对拼接结果进行线性变换，得到最终输出（维度与输入一致）：
    - $$ \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}*h) \cdot W_o \in \mathbb{R}^{n \times d*{\text{model}}} $$

###### 3. 完整公式（整合版）

$$ \text{MultiHead}(Q, K, V) = \left[ \text{Attention}(X W_q^1, X W_k^1, X W_v^1); \cdots; \text{Attention}(X W_q^h, X W_k^h, X W_v^h) \right] \cdot W_o $$
（注：分号“;”表示按列拼接）

- **维度兼容性**：输入 $$ X $$（$$ n \times 512 $$）与输出（$$ n \times 512 $$）维度一致，保证模型结构连贯。
- **并行性**：8 个头的计算可并行执行，提升计算效率。
- **多视角捕捉**：每个头通过独立权重矩阵学习不同的关联（如语法、语义、情感等），拼接后融合多维度信息。
