# 1. 位置编码

在自然语言处理（NLP）中，**位置编码**是Transformer模型处理序列顺序信息的核心技术。由于Transformer架构的自注意力机制（Self-Attention）是**排列不变**的，即不感知输入序列中元素的顺序，因此需要通过位置编码为模型提供序列的顺序信息。位置编码分为两大类：**绝对位置编码**和**相对位置编码**，它们在建模位置信息的方式上存在根本差异。

![image-20250707161800485](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250707161800485.png)

如图可知：文字顺序变化，不影响字级别的embedding结果，也就是文字的词意与其在句中的位置无关，这显然不符合认知，所以我们就需要一个位置编码函数f，将位置的信息融入词嵌入中。

## 一、**绝对位置编码：给每个位置 “贴唯一标签”**

### 1. 定义

**绝对位置编码**为序列中的每个位置赋予一个**唯一的固定编码**，并将其嵌入输入的词向量中。例如，位置$$3$$始终对应一个固定的向量，不随上下文变化。分为 **可学习位置编码**（如 BERT）和 **三角位置编码**（如 GPT 早期、Transformer 论文）。

### 2. 实现方式

#### 2.1 可学习位置编码

- **描述**：直接对不同的位置随机初始化一个位置编码，与文本嵌入一起送给大模型进行训练，这种方法引入了大量的参数需要学习，需要大量的数据才能训练。通过训练学习一个位置编码矩阵，每个位置对应一个可更新的向量。BERT等模型广泛采用此方法。

- 公式：

  $$ \text{Input}_i = \text{TokenEmbedding}_i + \text{PosEmbedding}_i + \text{SegEmbedding}_i $$

  - 参数说明：
    - $$ \text{Input}_i $$：第$$i$$个位置的最终输入向量。
    - $$ \text{TokenEmbedding}_i $$：词嵌入向量。
    - $$ \text{PosEmbedding}*i $$：位置嵌入向量，来自矩阵$$ \mathbf{PE} \in \mathbb{R}^{\text{maxLen} \times d*{\text{model}}} $$。
    - $$ \text{SegEmbedding}_i $$：段嵌入向量（如BERT中区分句子对）。

- **优点**：灵活性高，能适应不同任务中的位置模式。

- **缺点**：受限于训练时的最大长度$$ \text{maxLen} $$，推理时超长输入可能导致性能下降。

#### 2.2 固定的绝对位置编码：三角位置编码

- **描述**：利用正弦和余弦函数生成固定的位置编码，最早由Transformer论文提出。

- 公式：

  $$ PE_{(\text{pos}, 2i)} = \sin\left( \dfrac{\text{pos}}{10000^{2i/d}} \right) $$

  $$ PE_{(\text{pos}, 2i+1)} = \cos\left( \dfrac{\text{pos}}{10000^{2i/d}} \right) $$

  - 参数说明：
    - $$ \text{pos} $$：位置索引。
    - $$ d $$：嵌入维度（需为偶数）。
    - $$ i $$：维度索引，$$ 0 \leq i < d/2 $$。

- **优点**：具有周期性，可外推到训练长度之外的序列。pos+k位置的位置编码可以用pos位置的位置编码线性表示，一定程度上体现了其相对位置的关系。可证：pos+k和pos之间的内积随着相对距离的增加而减小，符合文本中token之间一般距离越远关系越弱的原理

- **缺点**：固定的公式可能无法捕捉复杂的语义位置关系。比如：三角位置编码无法区分方向，即pos+k、pos之间的距离表示与pos、pos-k之间使用的距离是一样的。

### 3. 典型应用

- **BERT**：采用可学习位置编码，适用于短文本任务。
- **GPT**：早期使用三角位置编码，后期结合可学习编码。

### 4.总结

绝对位置编码实现简单，但是具有以下缺点：

- **无法直接编码相对位置信息：** 尽管能包含一定的相对位置信息，但是这种信息仅仅保存在位置编码内部，在计算自注意力时，这种位置信息就被破坏了。 一个token的位置编码是什么由其在句子中的绝对位置决定，但是真正重要的往往不是绝对位置，而是它与其他token之间的关系。 

- **长序列泛化性差：**对输入的长度敏感，一旦输入变化则需要重新调整。

## 三、相对位置编码

### 1. 定义

**相对位置编码**关注序列中元素之间的**相对距离或顺序关系**，而非绝对位置。例如，单词“狗”与“猫”的前后顺序通过编码体现。核心是 **直接建模 token 间的相对距离 / 顺序**，通过修改注意力机制实现。

分为 **基于注意力修改的编码**（T5、XLNet、DeBERTa）、**旋转位置编码（RoPE，如 LLama）**，以及轻量方案 **Alibi**。

### 2. 实现方式

#### 2.1 **基于注意力修改的相对编码**：通过改写注意力计算式，将 “相对距离” 直接融入注意力得分。

#### （1）**T5：可训练偏置 + 分桶策略**

- **描述**：T5在注意力权重中引入相对位置偏差，使用分桶策略将相对距离分组。
- 原理：简化注意力展开式，仅保留 **内容 - 内容交互** 和 **可训练的相对位置偏置** `β_ij`（`β_ij`仅依赖相对距离`i-j`）。
  对`i-j`做 **分桶处理**：近距（如 0~7）每个距离一个桶，远距（如 8~11）共享桶，更远桶更粗（如表格中`i-j=8~11`都映射到`f(i-j)=8`），减少参数。
- 公式： $$ A_{i,j} = \mathbf{x}_i W_Q W_K^T \mathbf{x}*j + \beta*{f(i-j)} $$
  - 参数说明：
    - $$ A_{i,j} $$：第$$i$$个查询和第$$j$$个键的注意力得分。
    - $$ \mathbf{x}_i, \mathbf{x}_j $$：输入向量。
    - $$ W_Q, W_K $$：查询和键的变换矩阵。
    - $$ \beta_{f(i-j)} $$：相对位置偏置，由分桶函数$$ f(\cdot) $$确定。
- **优点**：简化长距离建模，计算开销低。
- **缺点**：分桶可能丢失细粒度信息。

#### （2） **XLNet：分解注意力，学习多维度交互**

- **描述**：XLNet采用相对位置编码并结合双流注意力，通过分解注意力得分显式建模相对位置。

- 原理：将注意力得分分解为四大交互项：

  - 内容 - 内容：`x_i W_Q W_K^T x_j`（词与词的交互）
  - 内容 - 相对位置：`x_i W_Q W_K^R R_{i-j}`（词与相对位置的交互）
  - 位置 - 内容：`u W_K^T x_j`（可训练位置向量与词的交互）
  - 位置 - 相对位置：`v W_K^R R_{i-j}`（可训练位置向量与相对位置的交互）
    模型通过学习这四项，**精细捕捉 “内容 - 位置”“位置 - 内容” 等维度的依赖关系**。相对位置向量`R_{i-j}`类似三角编码生成，更灵活。

- 公式：

   $$ A_{i,j} = \mathbf{x}_i W_Q W_K^T \mathbf{x}_j + \mathbf{x}*i W_Q W_K^R \mathbf{R}*{i-j}^T + \mathbf{u} W_K^T \mathbf{x}*j + \mathbf{v} W_K^R \mathbf{R}*{i-j}^T $$

  - 参数说明：
    - $$ \mathbf{R}_{i-j} $$：相对位置向量。
    - $$ W_K^R $$：相对位置投影矩阵。
    - $$ \mathbf{u}, \mathbf{v} $$：可训练偏置向量。

- **优点**：细致建模内容与位置交互，适合长文本。

- **缺点**：计算复杂度高。

#### （3） **DeBERTa：截断 + 层次化编码**

- **描述**：DeBERTa通过解耦注意力机制分离内容和位置信息，并对相对距离进行截断。
- 原理：
  - 保留 **三项交互**（去掉 “位置 - 位置” 项），通过`δ(t,s)`将相对距离`t-s` **截断到`(-k, k]`区间**（如 k=512，超距按边界处理）。
  - 模型分 **Encoder（前 11 层用相对位置）和 Decoder（后 2 层加绝对位置）**，结合两者优势。
  - softmax 校正系数改为 `√3d`（常规为`√d`），提升稳定性。
- 公式：$$ A_{t,s} = \mathbf{x}_t^T W_Q^T W_K \mathbf{x}_s + \mathbf{x}*t^T W_Q^T W_K \mathbf{P}*{\delta(t,s)} + \mathbf{x}*s^T W_K^T W_Q \mathbf{P}*{\delta(s,t)} $$
  - 参数说明：
    - $$ \delta(t,s) = \text{clip}(t - s, -k, k) $$：截断到$$ [-k, k] $$的相对距离。
    - $$ \mathbf{P}_{\delta} $$：相对距离的编码向量。
- **优点**：精确捕捉位置关系，提升复杂任务表现。
- **缺点**：截断可能丢失长距离信息。

#### 注：T5、XLNet与DeBERTa的相对位置编码对比

以下从**实现方式**、**计算复杂度**、**位置感知能力**和**适用场景**对比T5、XLNet和DeBERTa：

| **维度**         | **T5**                                         | **XLNet**                              | **DeBERTa**                               |
| ---------------- | ---------------------------------------------- | -------------------------------------- | ----------------------------------------- |
| **实现方式**     | 注意力得分加偏置$$ \beta_{f(i-j)} $$，分桶策略 | 注意力得分分解为四项，显式建模相对位置 | 解耦注意力，截断相对距离$$ \delta(t,s) $$ |
| **计算复杂度**   | 低（少量偏置参数）                             | 高（额外矩阵和向量）                   | 中等（截断计算）                          |
| **位置感知能力** | 粗略感知相对距离                               | 细致感知内容与位置交互                 | 精确感知相对位置                          |
| **适用场景**     | 多任务、资源受限                               | 长文本、复杂依赖                       | 高精度任务                                |

- **T5**：通过分桶简化建模，适合计算资源有限的多任务场景。
- **XLNet**：分解注意力得分，细致建模位置交互，适合长文本。
- **DeBERTa**：解耦机制提升精度，适合复杂任务。

#### 2.2 旋转位置编码RoPE （以LLama为例）

B站指路：[通俗易懂-大模型的关键技术之一：旋转位置编码rope （1）_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV12x42127Pb/?share_source=copy_web&vd_source=d0e4387a6f3f6598aa698f580507ec88)

- **描述**：通过旋转变换将位置信息嵌入词向量，使模型隐式捕捉相对位置。LLaMA和GPT-J等模型采用此方法。

- **理解：**它本质就是提供了一个函数f，在计算注意力分数时可以将输入的位置i,j代入函数中，这样计算出来的注意力分数就包含了位置信息。例如，对于位置i,j，以i的视角看待j和以j的视角看待i尽管两者之间的距离相等，但是方向不一致（简单理解为从左向右和从右向左，或者前后方向），在计算时当前token会和所有位置的token计算，将计算结果中除位置信息之外的向量（也就是xi和xj）作为系数，（j-i）作为自变量，那么注意力分数y就是关于位置信息（j-i）的函数，所以说旋转位置编码可以表示相对位置信息。

- **公式**：
  
  ![image-20250707163403085](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250707163403085.png)
  
- **推导核心**：
  
  ![image-20250707163605984](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250707163605984.png)
  
- **优点**：

  - **自带相对位置编码:** ROPE 不需要额外的位置编码加法(像绝对位置编码那样)或复杂的 Attention Score 分解(像 XLNet 的一些 RPE 变体那样)。它通过对 Query 和 Key 向量进行位置相关的旋转，使得它们点积的结果天然地包含并反映了它们之间的相对距离。
  - **长序列泛化能力强(外推性):** 由于 ROPE 依赖于相对距离的三角函数形式，而三角函数是周期性的。这意味着，即使模型在训练时没有见过某个非常长的相对距离它也能通过这些函数的性质，对未见过的长距离进行合理的推断和处理,这使得模型能够有效地处理比训练时更长的序列，克服了绝对位置编码的固有缺陷。

- **缺点**：

  - 数学实现较复杂，需理解旋转矩阵的几何意义。
  - 对模型维度的要求: ROPE 需要将向量拆分为长度为 2 的子向量。这意味着词向量的维度 d必须是偶数。虽然2大多数模型都会选择偶数维度，但这仍然是一个设计上的约束。


#### 2.3 补充：Alibi位置编码（Attention with Linear Biases）

B站指路：[Alibi模型原理分析_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1dG411d7VQ/?spm_id_from=333.337.search-card.all.click&vd_source=aa6afb9d0536d09ecdcb5d2c1fcf4c79)

- **描述**：为每个注意力头引入线性偏置，随距离线性衰减。原理其实很简单，就是引入一个惩罚机制，如果相对位置越远所加入的惩罚越大。

![image-20250707165959957](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250707165959957.png)

- **公式**：
  $$
  \text{bias}_{h,i,j} = -s_h \cdot |i - j|
  $$
  - **参数说明**：
    - \($\text{bias}_{h,i,j}$\)：第 \(h\) 个注意力头的偏置。
    - \($s_h$\)：可学习的斜率参数。
    - \($|i - j|$\)：位置 \(i\) 和 \(j\) 的相对距离。

- **优点**：

  - 简单高效，支持长序列处理。
  - 传统的绝对位置编码在训练时会为每个位置分配一个固定的向量，模型可能会过度拟合这些特定长度的模式。而 ALiBi 通过在注意力分数计算中直接使用线性偏置，减少了模型对特定序列长度的依赖，从而提高了对未见过的序列长度的泛化能力。
  - ALiBi 的位置偏差随距离线性增长，这种设计让模型在处理不同长度的序列时，可以自然地根据距离调整注意力权重，无需显式学习位置编码的复杂周期性结构。
  - 由于线性偏置的引入直接与序列中元素的位置相关，没有固定大小的编码矩阵限制，理论上模型可以更容易地处理任意长度的序列，从而展现出良好的长度外推性能。
  - 通过直接在注意力分数上施加与距离相关的线性惩罚，ALiBi 鼓励模型关注更近的位置，同时不完全排除远处的依赖，从而在一定程度上平衡了局部和全局依赖的学习，这对于处理长序列尤其有利。

- **缺点**：线性衰减假设可能不适用于所有任务。

## 四、绝对位置编码与相对位置编码的对比

| **维度**       | **绝对位置编码**                                     | **相对位置编码**                         |
| -------------- | ---------------------------------------------------- | ---------------------------------------- |
| **信息类型**   | 绝对位置（如“第3个词”）                              | 相对距离（如“前2个词”）                  |
| **序列长度**   | 固定长度限制，外推性差（可学习编码）或强（三角编码） | 无固定长度限制，天然支持变长序列         |
| **计算复杂度** | 低（直接相加）                                       | 高（需修改注意力机制）                   |
| **典型场景**   | 短文本任务、固定长度输入                             | 长文本任务、动态序列（如对话、实时生成） |

### 选择建议
- **短文本任务**：优先选择绝对位置编码（如BERT的可学习编码），简单高效。
- **长文本任务**：采用相对位置编码（如RoPE或Alibi），提升长距离依赖建模能力。
- **多任务场景**：考虑T5的简化相对位置编码，统一处理不同任务。

## 五、总结

- **绝对位置编码**为每个位置提供唯一标签，适合短文本和固定长度输入任务。其实现包括可学习位置编码和三角位置编码，分别以灵活性和外推性见长。
- **相对位置编码**聚焦元素间的相对关系，擅长处理长文本和动态序列。典型方法如RoPE和Alibi在长序列任务中表现尤为突出。
- 根据任务需求、序列长度和计算资源选择合适方案至关重要。
- 随着长文本处理需求的增加，相对位置编码及其变种（如RoPE和Alibi）在现代NLP模型中愈发重要。

通过深入理解和合理应用位置编码，可以显著提升Transformer模型在各种NLP任务中的表现。本文提供的公式和分析旨在为读者提供理论与实践的全面参考。

---

**附录：核心公式速览**

### 绝对位置编码
- **可学习位置编码**：
  $$
  \text{Input}_i = \text{TokenEmbedding}_i + \text{PosEmbedding}_i + \text{SegEmbedding}_i
  $$
- **三角位置编码**：
  $$
  PE_{(\text{pos}, 2i)} = \sin\left( \dfrac{\text{pos}}{10000^{2i/d}} \right), \quad PE_{(\text{pos}, 2i+1)} = \cos\left( \dfrac{\text{pos}}{10000^{2i/d}} \right)
  $$

### 相对位置编码
- **T5相对编码**：
  $$
  A_{i,j} = \mathbf{x}_i W_Q W_K^T \mathbf{x}_j + \beta_{f(i-j)}
  $$
- **RoPE**：
  $$
  \mathbf{q}_m' = \begin{pmatrix} \cos(m\theta_i) & -\sin(m\theta_i) \\ \sin(m\theta_i) & \cos(m\theta_i) \end{pmatrix} \begin{pmatrix} q_m^{2i} \\ q_m^{2i+1} \end{pmatrix}
  $$
- **Alibi**：
  $$
  \text{bias}_{h,i,j} = -s_h \cdot |i - j|
  $$