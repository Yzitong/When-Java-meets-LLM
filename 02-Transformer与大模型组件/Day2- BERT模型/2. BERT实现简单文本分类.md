# 2.用BERT实现简单的文本分类

B站指路：[7.实战任务1中文分类_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1a44y1H7Jc/?p=7&spm_id_from=333.788.top_right_bar_window_history.content.click&vd_source=aa6afb9d0536d09ecdcb5d2c1fcf4c79)

github代码：https://github.com/lansinuote/Huggingface_Toturials

## BERT中文情感分类项目解析

这是一个基于BERT的中文情感二分类实现，使用PyTorch和Hugging Face Transformers库构建。详细的代码在后面的代码文件中，项目展示了如何利用预训练的BERT模型，通过**迁移学习**技术快速构建高效的文本分类器。

**设计原理**：

1. **迁移学习策略**：
   - BERT已在海量通用文本上预训练，具备强大的语言理解能力
   - 冻结参数可保留这些通用语言知识
   - 只需针对特定任务微调顶层
2. **全连接层的作用**：
   - 将BERT输出的768维语义向量映射到分类空间（本例是2分类）
   - **完全新建的层**，非BERT原有组件
   - 训练过程中只更新这一层的权重
3. **效率考量**：
   - BERT有1.1亿参数，全微调需巨大计算资源
   - 仅训练顶层（约1,536参数）效率高10000倍
   - 适合小规模数据集（如本例9600样本）

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250629161620090.png" alt="image-20250629161620090" style="zoom:80%;" />

## 核心代码解析

### 1. 数据集加载
```python
class Dataset(torch.utils.data.Dataset):
    def __init__(self, split):
        self.dataset = load_dataset(path='lansinuote/ChnSentiCorp', split=split)
    
    def __getitem__(self, i):
        return self.dataset[i]['text'], self.dataset[i]['label']
```
- 使用`ChnSentiCorp`中文情感分析数据集
- 包含9,600条训练样本
- 标签：0(负面) / 1(正面)

### 2. BERT预处理
```python
token = BertTokenizer.from_pretrained('bert-base-chinese')

def collate_fn(data):
    # BERT分词和编码
    encoded = token.batch_encode_plus(
        sents, 
        truncation=True,
        padding='max_length',
        max_length=500,
        return_tensors='pt'
    )
    return encoded['input_ids'], encoded['attention_mask'], ...
```
- 使用`bert-base-chinese`分词器
- 将文本转换为BERT可处理的数字序列
- 自动处理填充(Padding)和截断(Truncation)

### 3. 模型架构
```python
# 加载预训练BERT（冻结参数）
pretrained = BertModel.from_pretrained('bert-base-chinese')
for param in pretrained.parameters():
    param.requires_grad_(False)

# 自定义分类模型
class Model(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = torch.nn.Linear(768, 2)  # 添加全连接层
    
    def forward(self, input_ids, attention_mask, token_type_ids):
        with torch.no_grad():
            out = pretrained(input_ids, attention_mask, token_type_ids)
        
        # 取[CLS]标记的输出
        cls_output = out.last_hidden_state[:, 0]
        return self.fc(cls_output).softmax(dim=1)
```

## 关键概念解析

### 1. 迁移学习与微调
| **技术**     | **说明**                   | **本项目的应用**           |
| ------------ | -------------------------- | -------------------------- |
| **迁移学习** | 将预训练知识迁移到新任务   | 使用预训练的BERT中文模型   |
| **特征提取** | 冻结预训练模型，仅训练新层 | 冻结BERT参数，训练全连接层 |
| **微调**     | 解冻部分层继续训练         | 未使用（可扩展）           |

### 2. [CLS]标记的作用
BERT在输入序列开头添加的特殊分类标记：

- 在预训练阶段，[CLS]用于下一句预测(NSP)任务
- 其最终隐藏状态被设计为整个序列的语义摘要
- 分类任务中自然适合作为特征输入

### 3. 全连接层的作用
为什么添加一个简单的线性层就能实现分类？
- **维度转换**：将768维向量→2维分类空间
- **可学习参数**：模型只需学习权重矩阵W(768×2)和偏置b(2)
- **数学表达**：  
  $y = \text{softmax}(W \cdot h_{[CLS]} + b)$

## 训练过程
```python
optimizer = AdamW(model.parameters(), lr=5e-4)
criterion = torch.nn.CrossEntropyLoss()

for batch in loader:
    # 前向传播
    outputs = model(input_ids, attention_mask, token_type_ids)
    
    # 计算损失
    loss = criterion(outputs, labels)
    
    # 反向传播
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
```
- **优化器**：AdamW（BERT训练标准选择）
- **学习率**：5e-4（特征提取的常用值）
- **批大小**：16（适合大多数GPU）

## 评估结果
在验证集上的评估代码：
```python
def test():
    model.eval()
    correct = 0
    total = 0
    
    for batch in test_loader:
        with torch.no_grad():
            outputs = model(...)
        
        preds = outputs.argmax(dim=1)
        correct += (preds == labels).sum().item()
        total += len(labels)
    
    print(f'准确率: {correct/total:.2f}')
```
- 训练300个batch后达到约85%的准确率
- 注：完整训练应使用更多epoch

## 常见问题解答

### Q1：为什么要冻结BERT参数？
**A**：冻结参数可以：
1. 保留预训练获得的中文语言知识
2. 大幅减少训练所需计算资源
3. 避免小数据集上的过拟合
4. 加速训练过程（仅需训练顶层分类器）

### Q2：能否解冻部分BERT层？
**A**：可以！这是更高级的微调策略：
```python
# 解冻最后3层
for name, param in pretrained.named_parameters():
    if 'layer.11' in name or 'layer.10' in name or 'layer.9' in name:
        param.requires_grad = True
```
解冻顶层通常能提升性能（尤其当训练数据>10,000条时）

### Q3：为什么使用[CLS]标记而不是其他位置？
**A**：

1. 设计初衷：BERT专门将[CLS]用于分类任务
2. 信息聚合：通过自注意力机制汇聚全局信息
3. 效率考量：比平均池化/最大池化更高效
4. 实践验证：在各类NLP任务中表现优异

### Q4：如何扩展到多分类任务？
**A**：只需修改全连接层的输出维度：
```python
# 10分类任务示例
self.fc = torch.nn.Linear(768, 10)
```
同时确保数据集标签为0-9的整数

### Q5：为什么加一个线性层就可以做到分类？

\- BERT模型输出的`[CLS]`标记的隐藏状态（一个768维的向量）包含了整个输入序列的语义信息（因为BERT的设计就是在`[CLS]`中汇聚整个序列的信息用于分类任务）。

\- 我们添加的全连接层的作用是将这个768维的向量映射到分类任务的类别数（这里是2维）。这个全连接层学习的是从通用语义表示到具体任务类别的转换。

### Q6：关于`[CLS]`标记怎么理解？

\- 在BERT模型中，输入序列的开头确实添加了一个特殊的`[CLS]`标记。这个标记的最终隐藏状态（即BERT输出的对应位置的向量）被设计用于表示整个序列的摘要信息，特别适合用于分类任务。

\- 为什么用`[CLS]`？因为在BERT的预训练任务中（NSP任务），`[CLS]`对应的输出向量被用来预测两个句子是否连续。因此，在预训练过程中，模型就被训练成将整个序列的信息汇聚到`[CLS]`标记的表示中。

\- 注意：虽然`[CLS]`标记的表示包含了整个序列的信息，但通常我们也会使用其他策略（如所有token的平均、最大池化等）来得到整个序列的表示。但在BERT中，分类任务的标准做法是使用`[CLS]`标记的表示。

\### 代码中相关部分

在模型的forward函数中：

\```python

out = pretrained(input_ids=input_ids, ...)   # 输出是一个包含多个元素的元组，我们取last_hidden_state

cls_output = out.last_hidden_state[:, 0]     # 取每个序列的第一个token（即[CLS]）的隐藏状态

output = self.fc(cls_output)                 # 将[CLS]的向量输入全连接层

## 改进建议（感兴趣可以自己尝试）

1. **增加训练时间**：这里由于只是测试，我没有训完一个epoch
   
   ```python
   # 改为完整epoch训练
   for epoch in range(5):  # 训练5个epoch
       for batch in loader:
           # 训练代码
   ```
   
2. **添加正则化**：
   ```python
   # 在全连接层后添加Dropout
   self.dropout = nn.Dropout(0.1)
   self.fc = nn.Linear(768, 2)
   
   def forward(...):
       cls_output = ...
       return self.fc(self.dropout(cls_output))
   ```

3. **尝试不同池化策略**：
   ```python
   # 平均池化替代[CLS]
   mean_pool = out.last_hidden_state.mean(dim=1)
   return self.fc(mean_pool)
   ```

4. **使用学习率调度**：
   ```python
   from transformers import get_linear_schedule_with_warmup
   
   scheduler = get_linear_schedule_with_warmup(
       optimizer, 
       num_warmup_steps=100,
       num_training_steps=len(loader)*epochs
   )
   
   # 每个batch后调用
   scheduler.step()
   ```
