# 1. BERT模型与句子向量

B站指路:

Bert 解读→[BERT从零详细解读，看不懂来打我_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Ey4y1874y/?spm_id_from=333.1387.collection.video_card.click&vd_source=aa6afb9d0536d09ecdcb5d2c1fcf4c79)

Bert代码→[BERT代码(源码)从零解读【Pytorch-手把手教你从零实现一个BERT源码模型】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Kb4y187G6/?spm_id_from=333.1387.collection.video_card.click&vd_source=aa6afb9d0536d09ecdcb5d2c1fcf4c79)		

## 前言：

BERT（Bidirectional Encoder Representations from Transformers）是 Google 于 2018 年提出的预训练语言模型，基于 Transformer 架构，是一种端到端的训练方法。BERT 的核心能力是将文本（词、句子）转换为包含**上下文信息**的向量表征。

传统方法（如 Word2Vec）通过 “词共现” 统计生成语义向量，无法捕捉动态语义（如 “苹果” 在 “水果” 和 “公司” 中的不同含义）。而BERT模型可以解决一词多义的问题。

*BERT 与 Word2Vec 有什么关系与区别呢？*BERT 与 Word2Vec**均为语义向量生成工具**，两者均是将文本转换为向量的技术，但**Word2Vec 是静态词向量**，而**BERT 是动态上下文向量**。针对于计算速度要求很高的场景下，如语音客服场景，使用静态的词向量其相应速度更快。



## 为什么 BERT 更适合表征句子向量？

1. **从词向量到句子向量的天然优势**：
   - Word2Vec 仅能生成词向量，若要表征句子，需通过平均池化、加权求和等方法组合词向量，但这种方式忽略了词语间的语义结构（如主谓宾关系）。
   - BERT 的 Transformer 架构能直接建模句子中词语的上下文依赖，其输出的词向量已包含句子级语义信息，只需通过简单池化（如取 CLS token 向量）即可得到句子向量。
2. **语义表征能力的跃升**：
   - BERT 预训练时接触了海量语料（如 BooksCorpus、Wikipedia），学习到了更丰富的语言知识（如语法、常识），句子向量能捕捉更深层次的语义关联。
   - **实验对比**：在文本相似度任务中，BERT 句子向量对 “近义词替换”“主动句 - 被动句转换” 等语义变化的表征精度远高于 Word2Vec。
3. **任务适应性与迁移能力**：
   - BERT 通过微调可直接应用于各类 NLP 任务（如分类、问答），其句子向量能自适应任务需求，而 Word2Vec 需配合额外特征工程。

Word2Vec 解决了 “词语数字化” 的基础问题，但 BERT 通过上下文建模实现了从 “符号表征” 到 “语义理解” 的跨越。当前常用 BERT 表征句子向量，本质是因为自然语言处理任务对 “语境感知” 和 “深层语义表征” 的需求不断提升。尽管 BERT 仍有优化空间，但其动态表征能力已成为现代 NLP 任务的核心基础。



## BERT核心架构

**简介：**BERT 的全称是 Bidirectional Encoder Representations from Transformers，即基于变换器的双向编码器表征。它是 2018 年 10 月由 Google AI 研究院提出的一种预训练模型。

### 1. BERT的整体模型架构

BERT的核心是由12个ENcoder编码器组成：其中一个编码器的内部由输入部分+多头注意力机制+前馈神经网络三部分组成。

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250627113401689.png" alt="image-20250627113401689" style="zoom: 80%;" />

### 2. 输入层的3层嵌入

输入Input = 词元嵌入（token EMB）+ 段落嵌入（segment EMB）+ 位置嵌入（position EMB）

**a）Token Embeddings ：**是将文本中的每个词（或子词，subword）转换为低维稠密向量的过程。它的核心作用是**捕捉词汇的语义信息**，使语义相近的词在向量空间中距离更近。BERT 使用 **WordPiece 分词** 技术将文本拆分为子词单元（subwords）

- BERT 的输入通常由两个句子（或段落）组成，用特殊标记 `[SEP]` 分隔，例如：

```plaintext
[CLS] The dog is running [SEP] It is happy today [SEP]
```

**b）Segment Embeddings：** 用于区分输入中的不同段落（或句子），帮助模型理解文本的结构关系。在 BERT 的预训练任务（如 **Next Sentence Prediction, NSP**）中，需要判断两个句子是否连续，此时 Segment Embeddings 至关重要。

Segment Embeddings 仅使用 **两个向量**（通常记为 `E_A` 和 `E_B`）：

- 第一个句子（包括 `[CLS]`）的所有 token 使用 `E_A`；
- 第二个句子的所有 token 使用 `E_B`。

**c）Position Embeddings ：**用于表示 token 在序列中的位置信息。由于 Transformer 架构本身**不具备捕捉序列顺序的能力**（self-attention 是全连接的，不考虑位置），因此需要显式地将位置信息编码到输入中。

BERT 使用 **可学习的位置嵌入**（不同于 Transformer 原始论文中的正弦 / 余弦函数）。具体来说：

- 预定义一个最大序列长度（如 512）；
- 为每个位置（0 到 511）创建一个可训练的嵌入向量；
- 根据 token 在序列中的位置，查询对应的嵌入向量。

![](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250627114736383.png)

### 3. 如何对BERT进行预训练：MLM+NSP

BERT模型预训练的初始语料库是无标签的，也就是一个无监督学习，对于无监督的NLP任务，我们一般有两种思路：自回归模型（AR）和自编码模型（AE），关于**自回归（Autoregressive）与自编码（Autoencoder）的区别：**

- **自回归**：按照特定顺序一次生成一个 token（从左到右 or 从右到左），通常根据上文内容预测下一个可能跟随的单词，或根据下文预测前面的单词，是单向的语言模型。例如 GPT 就是典型的自回归语言模型，以从左到右的方式生成文本。优点是**与文本生成任务的流程契合度高**；缺点是不能很好地同时利用双向上下文信息，在一些需要综合理解上下文语义的任务中表现受限。

- **自编码**：通常用于降噪，也叫降噪自编码器。它首先破坏输入的 token，通过编码器编码到潜在空间，然后再解码重构到原始空间。BERT 中的 Masked Language Model（MLM）就是典型的自编码预训练任务，通过把输入进行一定的掩盖，再重建恢复原始 token 序列。优点是能**有效利用双向上下文**，提升对语义的理解能力；缺点是在输入侧引入的 Mask 标记，会**导致预训练阶段和微调阶段不一致的问题**（采用数据 80 10 10原则去解决），影响模型在某些任务上的表现。

  

**a）MLM（Masked Language Model，掩码语言模型）**
MLM 是一种**自编码（Autoencoder）** 任务，其核心思想是：随机掩盖输入文本中的一部分 token（词），然后让模型**基于上下文预测被掩盖的 token**。这种方式使模型能够同时利用被掩盖词的**上文和下文**信息，从而学习到双向的语言表示。

**具体实现：**

1. **掩码策略**：
   - 随机选择输入序列中 15% 的 token 进行掩盖；
   - 其中：
     - 80% 的概率将 token 替换为特殊标记 `[MASK]`（如 "the dog [MASK] brown"）；
     - 10% 的概率随机替换为其他 token（如 "the dog apple brown"）；
     - 10% 的概率保持不变（如 "the dog is brown"）。
   - **为什么需要8 1 1策略来解决MLM中预训练和微调不一致的问题呢？**
     - 核心原因1：想让模型“明白” 输入的数据中包含真实的单词
     - 核心原因2：85%的那部分token不参与梯度更新
     - 虽然BERT模型会将全部输入的token最终转化为词向量，并预测出来对应的token，但是只有15%进行“mask策略”的token才会计算loss并进行反向传播，能够影响模型。因此，虽然数据中存在85%的token没做任何修改，但是并不能让模型“明白”输入的数据中包含真实的单词。恰恰需要15%能够影响模型的token中，10%不进行替换的单词才能让模型“明白”输入数据中也不全是有错误的。因此能够缓解预训练阶段有[mask]，而下游任务没有[mask]这个不一致问题。
     - 原文链接：https://blog.csdn.net/muyuqing_sir/article/details/120821544
2. **训练目标**：
   模型通过 Transformer 编码器处理输入序列后，对被掩盖的位置输出预测概率分布，目标是最大化真实 token 的预测概率。

**b）NSP（Next Sentence Prediction，下一句预测）**

NSP 是一种**二分类任务**，目标是判断输入中的两个句子是否为**连续的上下文**。这一任务使模型能够学习到句子间的逻辑关系（如因果、对比、时间顺序等），对需要理解篇章结构的下游任务（如问答、自然语言推理）至关重要。

**具体实现：**

1. **数据构建**：
   - 正样本（标签为 `IsNext`）：从语料库中选择连续的两个句子；
   - 负样本（标签为 `NotNext`）：第二个句子从语料库中随机选择，与第一个句子无关。
2. **输入格式**：
   - 两个句子用 `[SEP]` 分隔，开头添加 `[CLS]` 标记（用于分类）；
   - 使用 Segment Embeddings 区分两个句子（第一个句子的 token 对应嵌入 `E_A`，第二个句子对应 `E_B`）。
3. **训练目标**：
   模型通过 Transformer 编码器处理输入后，使用 `[CLS]` 位置的输出向量进行二分类，判断两个句子是否连续。
