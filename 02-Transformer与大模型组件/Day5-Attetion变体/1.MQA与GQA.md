# MQA 与 GQA：MHA 的改进变体

在 Transformer 架构中，多头注意力（Multi-Head Attention, MHA）是核心组件之一，它通过多个注意力头并行计算，捕捉输入序列中的不同特征。然而，随着模型规模和序列长度的增加，MHA 的显存占用和计算复杂度成为瓶颈。为了优化显存使用并提高效率，研究者提出了 **多查询注意力（Multi-Query Attention, MQA）** 和 **分组查询注意力（Grouped Query Attention, GQA）** 两种变体。本笔记将详细讲解 MQA 和 GQA 的基本思想、原理，并与 MHA 进行对比。

------

## 1. MHA（Multi-Head Attention）基础

### 1.1 基本思想

MHA 通过将输入投影到多个独立的注意力头，捕捉序列中不同方面的依赖关系。每个头都有自己的查询（Query, Q）、键（Key, K）和值（Value, V）矩阵。

### 1.2 原理与公式

给定输入序列 $X \in \mathbb{R}^{L \times d}$（$L$ 为序列长度，$d$ 为模型维度），MHA 的计算过程如下：

1. **投影**：将输入 $X$ 通过线性变换分别投影到 $Q$、$K$ 和 $V$：
   $$
   Q = X W_Q, \quad K = X W_K, \quad V = X W_V
   $$
   其中，$W_Q, W_K, W_V \in \mathbb{R}^{d \times d}$ 是可学习的投影矩阵。
2. **拆分到多个头**：将 $Q$、$K$、$V$ 拆分为 $h$ 个头（$h$ 为头数）：
   $$
   Q_i = Q[:, i \cdot d_h : (i+1) \cdot d_h], \quad K_i = K[:, i \cdot d_h : (i+1) \cdot d_h], \quad V_i = V[:, i \cdot d_h : (i+1) \cdot d_h]
   $$
   其中，$d_h = d / h$ 为每个头的维度，$i = 0, 1, \dots, h-1$。
3. **注意力计算**：每个头独立计算注意力输出：
   $$
   \text{Attention}(Q_i, K_i, V_i) = \text{softmax}\left( \frac{Q_i K_i^T}{\sqrt{d_h}} \right) V_i
   $$
   其中，$\sqrt{d_h}$ 是缩放因子，用于稳定梯度。
4. **拼接与投影**：将所有头的输出拼接并通过一个投影矩阵 $W_O$ 得到最终结果：
   $$
   \text{MHA}(X) = \text{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h) W_O
   $$
   其中，$W_O \in \mathbb{R}^{d \times d}$。

### 1.3 细节

- **KV Cache**：在自回归任务（如语言生成）中，MHA 需要存储每个头的 $K_i$ 和 $V_i$，显存占用为 $O(L \times h \times d_h)$。
- **计算复杂度**：每步计算注意力需要 $O(L^2 \times h \times d_h)$。

------

## 2. MQA（Multi-Query Attention）

### 2.1 基本思想

MQA 的核心改进是 **所有注意力头共享同一个 $K$ 和 $V$ 矩阵**，而每个头仍然保留独立的 $Q$ 矩阵。这种设计显著减少了 KV Cache 的显存占用，尤其适用于长序列任务。

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250716191336477.png" alt="image-20250716191336477" style="zoom:80%;" />

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250716194002745.png" alt="image-20250716194002745" style="zoom: 80%;" />

### 2.2 原理与公式

在 MQA 中：

- **共享 $K$ 和 $V$**：不再为每个头生成独立的 $K_i$ 和 $V_i$，而是使用全局的 $K$ 和 $V$：
  $$
  K = X W_K^{\text{MQA}}, \quad V = X W_V^{\text{MQA}}
  $$
  其中，$W_K^{\text{MQA}}, W_V^{\text{MQA}} \in \mathbb{R}^{d \times d_h}$，输出维度为 $L \times d_h$。

- **独立 $Q$**：每个头仍有自己的 $Q_i$：
  $$
  Q_i = X W_{Q_i}
  $$
  其中，$W_{Q_i} \in \mathbb{R}^{d \times d_h}$。

- **注意力计算**：
  $$
  \text{Attention}(Q_i, K, V) = \text{softmax}\left( \frac{Q_i K^T}{\sqrt{d_h}} \right) V
  $$

- **输出**：
  $$
  \text{MQA}(X) = \text{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h) W_O
  $$

### 2.3 与 MHA 的区别

- **共享机制**：MHA 中每个头有独立的 $K_i$ 和 $V_i$，而 MQA 中所有头共享单一的 $K$ 和 $V$。
- **显存占用**：MQA 的 KV Cache 仅为 $O(L \times d_h)$，相比 MHA 的 $O(L \times h \times d_h)$ 减少了 $h$ 倍。
- **性能影响**：共享 $K$ 和 $V$ 限制了头之间的特征多样性，可能导致性能下降。

### 2.4 细节

- **为什么显存减少**：MHA 需要存储 $h$ 份 $K$ 和 $V$，而 MQA 只存储一份。
- **权衡**：显存优化是以牺牲部分表达能力为代价的，适合显存受限的场景。

------

## 3. GQA（Grouped Query Attention）

### 3.1 基本思想

GQA 在 MQA 和 MHA 之间寻找平衡点。它将 $h$ 个头分成 $g$ 组（$g \leq h$），每组内的头共享同一个 $K$ 和 $V$，不同组有独立的 $K$ 和 $V$。通过调整组数 $g$，可以在显存和性能之间灵活权衡。

### 3.2 原理与公式

假设 $h$ 个头分为 $g$ 组，每组有 $h / g$ 个头：

- **分组 $K$ 和 $V$**：每组 $j$（$j = 1, 2, \dots, g$）有自己的 $K_j$ 和 $V_j$：
  $$
  K_j = X W_{K_j}, \quad V_j = X W_{V_j}
  $$
  其中，$W_{K_j}, W_{V_j} \in \mathbb{R}^{d \times d_h}$。
- **独立 $Q$**：每个头 $i$ 有自己的 $Q_i$：
  $$
  Q_i = X W_{Q_i}
  $$
- **注意力计算**：头 $i$ 使用其所在组 $j$ 的 $K_j$ 和 $V_j$：
  $$
  \text{Attention}(Q_i, K_j, V_j) = \text{softmax}\left( \frac{Q_i K_j^T}{\sqrt{d_h}} \right) V_j
  $$
  其中，$j = \lceil i / (h / g) \rceil$ 表示头 $i$ 所属的组。
- **输出**：
  $$
  \text{GQA}(X) = \text{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h) W_O
  $$

### 3.3 与 MHA 和 MQA 的区别

- **共享机制**：MHA 每个头独立，MQA 所有头共享，GQA 每组共享。
- **显存占用**：GQA 的 KV Cache 为 $O  (L \times g \times d_h)$，介于 MHA（$g = h$）和 MQA（$g = 1$）之间。
- **性能**：GQA 保留了部分head间的多样性，性能优于 MQA，逊于 MHA。

### 3.4 细节

- **组数 $g$ 的作用**：$g = 1$ 时退化为 MQA，$g = h$ 时退化为 MHA。
- **灵活性**：通过调整 $g$（如 $g = 8$），可在显存和性能间找到最佳点。

------

## 4. MHA、MQA 与 GQA 的对比

![image-20250716194059443](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250716194059443.png)

| **特性**                  | **MHA**                      | **MQA**                      | **GQA**                      |
| ------------------------- | ---------------------------- | ---------------------------- | ---------------------------- |
| **$K$ 和 $V$ 的共享方式** | 每个头独立                   | 所有头共享                   | 每组头共享                   |
| **KV Cache 显存占用**     | $O(L \times h \times d_h)$   | $O(L \times d_h)$            | $O(L \times g \times d_h)$   |
| **计算复杂度**            | $O(L^2 \times h \times d_h)$ | $O(L^2 \times h \times d_h)$ | $O(L^2 \times h \times d_h)$ |
| **性能**                  | 最高（多样性强）             | 最低（多样性弱）             | 介于两者之间                 |
| **适用场景**              | 显存充足，追求高性能         | 显存受限，长序列             | 平衡显存和性能               |

------

## 5. 总结

- **MHA**：性能最优，但显存需求高。
- **MQA**：显存最优，性能稍弱，适合资源受限场景。
- **GQA**：通过分组机制，在显存和性能间取得平衡，灵活性强。

通过理解这些机制的原理和公式，可以根据任务需求选择合适的注意力变体。

