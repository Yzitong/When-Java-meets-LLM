# 1. 词向量深入解析

## 1. 词向量与语义信息

**词向量：**从单词到向量的映射，将语义信息数字化，让计算机学习理解。

**嵌入（embedding）：**将信息表征为模型能够处理的数字形式，这种表征方式得到的产物通常称为“嵌入”。

词向量一般分为稀疏词向量和稠密词向量两种

### 1.1 稀疏词向量

典型的就是独热词向量（one-hot），其直接的想法就是按照全部的词元（token）组成的词典维度，将词元表征为一个向量。此向量只有一个维度的下标位置是1，其余都是0。这样表征的词向量彼此之间都是正交的，无法刻画出近义词等语义关系。

![image-20250621161537102](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250621161537102.png)

注：余弦相似度计算：
$
\cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{|\mathbf{A}| |\mathbf{B}|} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \sqrt{\sum_{i=1}^{n} B_i^2}}
$
**分布式语义假设**：就是一种对人类语言的归纳偏置，该假设的内核是<u>一个词的含义与其上下文具有很强的相关性</u>。如果是上下文相似的两个词，那么它们的语义相似度应该很高，并且词向量的词嵌入（word embedding）相似度应该也很高。

将这样的先验知识融入模型训练中，构造一些具体的训练任务，就可以使得模型在向量空间具备较好的聚类特性。 词向量建模语义信息的思想就是基于以上假设将词的特征信息融入词向量的表示中。常见的词向量语义建模方式包括word2vec、FastText、GloVe等，其基本思想是将人类语言的先验知识融入模型训练的目标中，通过统计、<u>极大似然</u>或深度学习训练等方式得到对词向量最优的向量化结果，从而不断提升模型对人类语言的建模能力。

注：**极大似然训练词向量的含义**：（需要理解！后面也会用）
首先极大似然估计就是：**根据已发生的结果，反推 “最可能让这个结果出现的参数”**。举例来说，就像通过抛硬币的结果，猜硬币本身的 “正面概率”，让 “已发生的结果” 概率最大～

 “极大似然” 训练词向量（比如 word2vec），思路是一样的：

1. 假设语言数据（比如某词与某上下文 共现）是某种概率分布的结果；
2. 构造一个 “似然函数”，描述 “模型参数（词向量）” 和 “观测到的语言数据” 的匹配程度；
3. 找一组词向量，让这个似然函数最大（即 “让观测到的语言现象最可能发生”）。

### 1.2 稠密词向量

常见的稠密词向量的表示方法：word2vec、FastText、GloVe等。通过将先验知识融入到低维连续向量空间中，使得词向量更为稠密，相较于稀疏词向量的好处：

1. 使词语的表征具备语义聚类特性，向量间的欧式距离可以表征为词汇相似度。
2. 维度减少，计算效率变高

#### 1.2.1 Word2Vec模型（重点）

前言已经提到，**Word2Vec模型主要有两种架构**：

1. **连续词袋模型CBOW**(Continuous Bag of Words)：是根据目标词上下文中的词对应的词向量, 计算并输出目标词的向量表示；
2. **Skip-Gram模型**：与CBOW模型相反, 是利用目标词的向量表示计算上下文中的词向量.

为了更好的理解word2vec这种浅层神经网络的工作原理，我们依次介绍一下这两种模型的目标函数，并说明通过优化目标函数构建词向量的过程。

#### **1. 目标函数的设定**

**CBOW 模型：**

其核心思想是用上下文词预测中心词。例如句子 “我 喜欢 自然语言处理”，若以 “喜欢” 为中心词，CBOW 会用 “我” 和 “自然语言处理” 预测 “喜欢”。

对于词表 *C* 中词 *w* 的上下文词 Context(*w*)，CBOW 首先预测词 *w* 的生成概率 *p*(*w*∣Context(*w*))，接着通过最大化似然函数，最大化词表 *C* 中所有词的整体生成概率，其目标函数可以表示为：
$$
L = \max \sum_{w \in C} \log p(w \mid \text{Context}(w))
$$
**Skip - gram 模型：**

其核心思想是用中心词预测上下文词。还是以 “我 喜欢 自然语言处理” 为例，Skip - gram 会用 “喜欢” 预测 “我” 和 “自然语言处理”。

对于词表 *C* 中的词 *w*，skip - gram 首先预测 *w* 对应的上下文词 Context(*w*) 的生成概率 *p*(Context(*w*)∣*w*)，接着通过最大化似然函数，最大化词表 *C* 中所有词的整体生成概率，其目标函数可以表示为：
$$
L = \max \sum_{w \in C} \log p(\text{Context}(w) \mid w)
$$

#### 2. 模型结构与训练细节

![image-20250621213653688](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250621213653688.png)

##### a. 输入层编码：

输入层接收的词被编码为独热向量，维度大小为 1×*N*，其中 *N* 是词表 *C* 的大小。独热向量只有对应词的位置为 1，其余位置均为 0。

##### b. 权重矩阵与词向量维度：

- 输入层和隐藏层之间有维度大小为 *N*×*K* 的权重矩阵，其中 ***K* 既是隐藏层上隐藏单元的个数**，也是每个词向量的表示维度。

- 隐藏层和输出层之间也有维度大小为 *K*×*N* 的权重矩阵。

  注：隐藏层与隐藏单元是什么意思？K的含义又是什么？

  - **隐藏层**：：像一个 “提炼车间”，把输入的原始数据（如独热向量、像素），转化为**更抽象、更有用的特征表示**（如词向量、图像语义特征 ）。通过嵌入矩阵（权重矩阵 W1，维度为 V×K，K 为词向量维度）将 one-hot 向量映射为稠密词向量，即：
    $$
    \mathbf{h} = \mathbf{W1}^T \cdot \mathbf{x}
    $$
    其中，**x**是输入的 one-hot 向量，**h**是维度为 1×K 的隐藏层输出（中心词的词向量）。

  - **隐藏单元**：也叫神经元，每个隐藏单元可以理解为一个 “小筛子”，每个单元作为“信息筛选器”，负责捕捉一种 “局部特征”，共同把输入信息 “拆解 + 重组”，输出对任务有价值的结果。

  - **K的含义**：隐藏层中隐藏单元的个数，也是每个词向量的维度。K 越大，词向量能表达的语义信息越丰富，但会增加计算量和训练难度。实际使用中需根据数据量和任务需求调整，常见取值如 100、200 等。

##### c. 隐藏层输出计算：

- 在 skip - gram 模型中，隐藏层的输出可以由 1×*N* 维度的输入向量与 *N*×*K* 维度的权重矩阵计算得到。
- 在 CBOW 模型中，隐藏层最终的输出则是将多个上下文词计算而得的隐藏层输出进行相加得到的结果。

##### d. 输出层与 softmax 转换：

输出层的输出向量维度大小为 1×*N*，每一个维度均与词表中的一个词对应，每一个维度的值由隐藏层的输出向量点乘 *K*×*N* 维度权重矩阵的每一列所得。为了得到词表中每个单词的生成概率，可以首先应用 softmax 函数将输出向量中的每一个元素归一化到 0 和 1 之间的概率，接着计算损失并使用反向传播算法来更新模型的权重，进而优化目标函数。

softmax函数：将神经网络输出的任意实数得分，转换为 0 - 1 之间的概率分布，使结果可表示属于某一类的概率。在 Word2Vec 中，用于将输出层的原始得分转换为词表中每个词作为上下文词的概率。
$$
\text{softmax}(\mathbf{z})_i = \frac{\exp(z_i)}{\sum_{j=1}^{n} \exp(z_j)}
$$
这时候会不会有个疑惑：对于softmax得到的概率求和那不是永远是1了嘛？我们想要目标的概率和最大，这不没意义啊！

注意，这里有个概念是窗口大小。“概率和” 不是直接对 softmax 输出的 *N* 维概率求和（那总和永远是 1，没意义 ），而是对**语料库中所有中心词 - 窗口大小内的上下文词对的对数概率求和**，构造出 “似然函数”。

以skip-gram模型举个例子：
假设语料库只有一句话：`“猫 追 老鼠”`，我们用<u>窗口大小 1</u>（每个中心词看前后 1 个词 ）。

假设模型预测概率如下（仅举例，非真实训练结果 ）：

- *p*(猫∣追)=0.6，ln(0.6)≈−0.51；
- *p*(老鼠∣追)=0.3，ln(0.3)≈−1.20；
- *p*(追∣猫)=0.7，ln(0.7)≈−0.36；
- *p*(追∣老鼠)=0.8，ln(0.8)≈−0.22；

那么求和：ln*L* = [ln0.6+ln0.3]+[ln0.7]+[ln0.8]≈(−0.51−1.20)+(−0.36)+(−0.22)≈−2.29

训练的目标就是调整词向量和输出层权重，让这个 ln*L* 变得更大（比如从 -2.29 变到 -1.5 ），也就是让 “语料库中观察到的词对，发生的概率更高”。

##### e. 损失函数的计算与权重的更新

上面的过程介绍了正向计算预测概率的过程，接下来需要计算损失并通过反向传播来更新模型权重，其本质是通过<u>*梯度下降法*</u>最小化预测概率与真实标签的差异。以下是具体过程的详细拆解：

- **交叉熵损失函数：**损失函数（Loss Function）是衡量模型预测结果与真实标签之间偏差的数学函数，用于量化模型的 “错误程度”。

  常见的损失函数主要是交叉熵损失，其相对于MSE损失（梯度是线性的），在预测错误时梯度大，快速优化；在预测接近正确答案时优化缓慢，更符合分类任务的需求（梯度是反比例函数），**最小化交叉熵=最大化数据的似然概率。**

  交叉熵损失函数的定义为：（p是模型预测的概率分布，y是不同类别的真实概率分布）
  $$
  L = -\sum_{i=1}^n y_i \cdot \log(p_i)
  $$

- skip-gram的损失函数表示：
  $$
  L = -\log p(Context(w)|w) = -\sum_{c\in\text{Context}(w)} \log p(c|w)
  $$
  CBOW的损失函数表示：
  $$
  L = -\log p(w|Context(w))
  $$

- 损失函数像 “导航仪”，梯度是 “方向箭头”，权重更新是 “迈步”，每走一步都离 “预测准确” 的目标更近。

##### f. 词向量的生成：

权重更新的过程可以看作构建词向量的过程，且所生成的词向量就是模型里输入层和隐藏层以及隐藏层和输出层之间的权重矩阵，二者的维度大小分别为 *N*×*K* 和 *K*×*N*，均可以作为 *N* 个词的 *K* 维向量表示。



#### 3.提升训练速度（重要！）

在利用 softmax 函数计算词表中每个词被预测成目标词的概率时，softmax 的分母项需要对词表中每个词 *w* 的向量 *v*(*w*) 进行指数运算 e*^v*(*w*) 并进行求和。当词表很大时，softmax 的计算耗时也将增大，最终将导致模型训练速度变慢。原始流程中，softmax 计算需遍历整个词表（复杂度 ***O*(*V*)**），当 *V* 很大时效率极低。

为了提升模型的训练速度，可以采用层次化 softmax 和负采样两种算法。

- **层次化 softmax。**复杂度降至 ***O*(log*V*)** , 对于大小为 *N* 的词表，层次化 softmax 会根据词频构造哈夫曼树，越靠近root的地方，词频是越高的，越靠近叶子节点，词频越低。词表中的每个词都与哈夫曼树的一个叶节点相对应。由于哈夫曼树是一棵二叉树，因此其深度不高于 log(*N*)。于是，计算一个词的输出概率就可以转化为计算从根到该词对应的叶节点路径上的条件概率的乘积，也就是将计算 *N* 个词的 softmax 任务转化为计算 log(*N*) 次二分类的任务，其中两个类别分别表示向哈夫曼树的左 / 右解码，这样便大幅提升了模型的训练效率。

  

- **负采样*NEG* : 仅对少量负例词计算梯度，而非全部词表。**这里以 CBOW 模型为例，负采样算法的任务不再是预测一个词在给定上下文中的概率，而是预测一个词是否出现在给定的上下文中。这样，对于每个训练样本，模型只需区分正样本和负样本即可。正样本指的是输入为上下文 Context(*w*)，输出为中心词 *w* 的样本，即 (Context(*w*),*w*)。负样本指的是输入为上下文 Context(*w*)，输出不是中心词 *w* 的样本。负采样算法在采样负样本时以较高的概率采样高频词，以较低的概率采样低频词，最终采样出部分负样本。在训练过程中，模型的目标是最大化正样本的概率，同时最小化负样本的概率。在每次迭代中，模型只更新与正样本和选定的负样本相关的权重（而不是词表中所有单词的权重），进而减少计算量，提升模型的训练效率。

  负采样是最常用、最实用的加速方法!它的思路是:“每次训练，只专注少量词，其他词暂时忽略掉!”。比如，你用“fox”预测“quick"这个词，这是我们希望模型学到的“正样本”。但词表里有上万个词，不可能每次都比较一遍，所以我们随机选几个“负样本”(比如"banana”、“cup"、“sky"等)，告诉模型，这些词不是“fox"的上下文。

  当引入负采样后，损失函数会从多分类问题转化为多个二分类问题，此时损失函数的形式更直观地体现为交叉熵的和：
  $$
  L = -\left[\log \sigma(\text{正样本}) + \sum \log \sigma(\text{负样本})\right]
  $$