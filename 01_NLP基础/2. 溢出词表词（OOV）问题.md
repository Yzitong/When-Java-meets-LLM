# 2. 溢出词表词（OOV）问题

## 1.问题定义

溢出词表词问题在实际场景中很常见。例如，在实时获取的社交评论文本数据中，我们经常会看到包含最新的网络用语且伴随着大量拼写错误的词语，以及各种创造性的表达方式，包括词语的缩写、单词的多种形式（如复数形式、时态变化等）等。这些形态变化丰富的词可能从未在模型的词表中出现过。

如果模型无法处理这些溢出词表词，即模型无法为这些溢出词表词构建合适的词向量，那么它就无法表示这些词的语义信息，由此便会带来 3 个不良的影响。

- 模型在诸多下游任务上的性能变差。例如，对于文本分类任务，当输入文本中包含大量溢出词表词，并且没有有效的词向量表示时，模型对输入文本的语义理解是不完整且不连贯的，这导致模型无法在向量空间中构建正确的分类边界，从而降低了模型分类的准确性。
- 弱化了模型的泛化能力。优秀的自然语言处理模型应该具备良好的泛化能力，即能够处理训练数据之外的新实例。溢出词表词问题意味着模型对于未见过的词汇缺乏处理能力，这就弱化了模型的泛化能力。
- 限制了模型的应用范围。模型在特定领域或语言中的应用效果很大程度上取决于它能否处理该领域或语言中的特有词汇。而溢出词表词问题可能导致模型无法有效地应用于专业领域或多语言环境，这就限制了模型的应用范围。

接下来，我们介绍两种处理溢出词表词问题的思路。

### *！面试常考：你了解哪些解决OOV问题的方法？！*

#### 	**思路一：尽可能找到能还原溢出词表词语义信息的预训练词向量**

​	结合 Kaggle 平台上 Quora Insincere Questions Classification 竞赛中的开源代码，介绍对于一个英文的溢出词表词，如何从开源的预训练词向量表中找到合适的向量。在具体实现上，按照下面的流程，依次从开源的预训练词向量表中进行瀑布式查找。

1. **直接查找**溢出词表词有无对应词向量。

2. 对溢出词表词进行**全大写或全小写的改写**后，查找有无对应词向量。

3. 对溢出词表词的**首字母进行大写的改写**后，查找有无对应词向量。

4. 利用 Porter Stemmer、Snowball Stemmer 以及 Lancaster Stemmer 这 3 种方法**提取词干**后查找有无对应词向量。

   PorterStemmer：基于一系列预定义的规则，逐步去除词缀。它是最常用的词干提取器之一，规则相对温和，保留了词的基本形态。

   LancasterStemmer：使用更激进的规则集，可能会产生更短的词干，但有时会过度简化。

   SnowballStemmer：也称为 Porter2 词干提取器，是 Porter 词干提取器的改进版本。它支持多种语言，并提供了更精确的词干提取规则。

5.  Peter Norvig 的**拼写纠正**算法，在改写后的词与溢出词表词在字面形式上的差异尽可能小的前提下，查找有无对应词向量。

6. 如果以上方式都无法获取向量，则使用默认的未知向量。

```python
import re
import numpy as np
from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer
import gensim.models.keyedvectors

# 初始化词干提取器
ps = PorterStemmer()
lc = LancasterStemmer()
sb = SnowballStemmer("english")

# 加载词向量模型（这里需根据实际路径调整，确保文件存在）
spell_model = gensim.models.keyedvectors.load_word2vec_format(
    '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'
)
words = spell_model.index2word
w_rank = {}
for i, word in enumerate(words):
    w_rank[word] = i
WORDS = w_rank  # 存储单词排名的全局变量


# 辅助函数
def words(text):
    return re.findall(r'\w+', text.lower())


def P(word):
    # 获取单词在字典中的相反数排名，不在字典中返回 0
    return -WORDS.get(word, 0)


def correction(word):
    # 最有可能的单词拼写纠正
    return max(candidates(word), key=P)


def candidates(word):
    # 为单词生成可能的拼写纠正
    return (known([word]) or known(edits1(word)) or [word])


def known(words):
    # 从 WORDS 字典中获取已知的单词子集
    return set(w for w in words if w in WORDS)


def edits1(word):
    # 所有与 word 相差一个编辑距离的词
    letters = 'abcdefghijklmnopqrstuvwxyz'
    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]
    deletes = [L + R[1:] for L, R in splits if R]
    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]
    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]
    inserts = [L + c + R for L, R in splits for c in letters]
    return set(deletes + transposes + replaces + inserts)


def edits2(word):
    # 所有与 word 相差两个编辑距离的词
    return (e2 for e1 in edits1(word) for e2 in edits1(e1))


def singlify(word):
    # 将单词中的连续重复字母减少到一个
    return ''.join([letter for i, letter in enumerate(word) if i == 0 or letter != word[i - 1]])


def load_glove(word_dict, lemma_dict):
    # 加载 GloVe 词嵌入文件（路径需根据实际调整）
    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'
    embeddings_index = {}

    def get_coefs(o):
        return o.split(" ")[0], np.asarray(o.split(" ")[1:], dtype='float32')

    # 从文件中读取单词和对应的向量，存入字典
    with open(EMBEDDING_FILE) as f:
        for o in f:
            word, arr = get_coefs(o)
            embeddings_index[word] = arr

    embed_size = 300
    nb_words = len(word_dict) + 1
    embedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)
    # 初始化未知向量，所有元素为 -1
    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1
    print(unknown_vector[:5])

    for key in token(word_dict):
        word = key
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue

        # 尝试不同的形式获取词向量
        word = key.lower()
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue

        word = key.upper()
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue

        word = key.capitalize()
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue

        word = ps.stem(key)
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue

        word = lc.stem(key)
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue

        word = sb.stem(key)
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue

        word = lemma_dict[key]
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[word_dict[key]] = embedding_vector
            continue

        # 如果以上方式都无法获取向量，则使用未知向量
        if len(key) > 1:
            word = correction(key)
            embedding_vector = embeddings_index.get(word)
            if embedding_vector is not None:
                embedding_matrix[word_dict[key]] = embedding_vector
                continue

        embedding_matrix[word_dict[key]] = unknown_vector

    return embedding_matrix, nb_words
```

​	

#### 	**思路二：构建比词向量粒度更小的子词向量或字符向量。**

​	模型遇溢出词表词时，分解为子词 / 字符，聚合对应向量作为其最终表示 。如遇溢出词 “preprocessing”，拆分为 “pre ”-“process”-“ing”，聚合子词向量得该词表示。因子词更可能在词表，且英语单词由字母组成，溢出词表词的子词 / 字符若在训练数据出现，就能构建合适向量，避免溢出词表词问题 。

- 假设词表中有 `["un", "##happy", "##ness", "[UNK]"]`，但无 `unhappiness`。
- 输入 `unhappiness` → 拆分为 `["un", "##happy", "##ness"]` → 用这些子词的向量（如求和、加权平均）表示原词。

​	在具体实现上，FastText 是这种思路的代表性方法，它引入了字符级别的 n-gram 向量，这意味着每个词会被表示为它的 n-gram 集合。例如，对于一个溢出词表词 applet，如果设置 n-gram 的 n 为 3（3-gram），那么溢出词表词 applet 就会被分解为 app、ple 和 let 这 3 个 n-gram 集合。这样，即使 applet 在训练集中没有出现，FastText 仍然可以利用字符级别的 n-gram 向量来构建它的词向量。BERT 等当下流行的预训练语言模型也纷纷采用子词向量或字符向量来对词进行表征，以避免溢出词表词问题。到了大模型时代，分词（tokenize）算法对于不同颗粒度的词元适应性更好，同时考虑了溢出词表词和词组表达的编码问题。

​	这种方法可以捕捉单词内部的结构、词性、时态、语法功能等信息。子词分词的常见方法包括词片分词（WordPiece）、字节对编码（BPE）等。



#### WordPiece和BPE算法（重点）

##### 一、WordPiece算法

- **核心思想**：基于 “子词复用”，认为自然语言里很多词由更小语义单元（子词）组合而成。像 “unhappiness” 可拆 “un -”（否定前缀 ）、“happy”（核心语义 ）、“-ness”（名词后缀 ），拆分后子词能在不同词汇复用，用子词向量组合表示原词，降低 OOV 影响。

- **目标**：让拆分出的子词在语料里更 “通用”，既覆盖高频完整词，又能通过子词拼接处理生僻词、合成词，平衡词表大小和语义表达能力。显式地优化语言模型的似然值。
- **训练阶段（构建子词表）**:
  - **初始状态**：从最细粒度字符开始（也可结合基础词表 ），把每个字符视为初始子词单元，统计语料中所有字符及字符组合的出现频率。
  - **合并操作**：迭代找**最可能合并的字符对**，依据是合并后 “语言模型概率提升”（或简化为统计共现频率，类似 BPE 但策略有别 ）。比如语料里 “##er” 和 “##est” 常跟在动词后，若 “er” 和 “est” 高频共现，可能合并成新子词（不过 WordPiece 更关注语言模型概率 ）。每次合并生成新子词，加入词表，直到达到设定子词数量或收敛。
  - **词表构建**：最终形成包含基础字符、高频完整词、高频子词片段的词表，让常见词可直接匹配，生僻词能拆成子词组合。
- **应用WordPiece分解新词：**根据一个已有的 WordPiece 词表，可以以如下方式对输入文本进行分词。基本思想就是在词表中寻找是否有与这一单词具备相同最长前缀的词元，然后将单词一分为二。如果未找到，那么此词元就会将其标记为 [UNK]。

应用代码如下：

```python
class WordpieceTokenizer(object):
    # WordPiece 分词器类，用于将文本分词成词片段
    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):
        self.vocab = vocab  # 预训练好的子词表，key是子词，value可按需存信息（如索引）
        self.unk_token = unk_token  # OOV 时返回的标记，如"[UNK]"
        self.max_input_chars_per_word = max_input_chars_per_word  # 单词最大字符限制，超则直接标 OOV

    def tokenize(self, text):
        """
        将一段文本分词成词片段。
        这使用一个贪婪的最长匹配优先算法来执行，
        并使用给定词汇的分词

        例如，输入 = "helloworld!"
        将返回 ["hello", "##world", "!"]

        参数:
            text: 单个标记或空格分隔的标记。这应该已经由 BasicTokenizer 处理过

        返回:
            list: 一系列词片段标记
        """
        output_tokens = []
        # 先按空白拆分文本（依赖 whitespace_tokenize）
        for token in whitespace_tokenize(text):  
            chars = list(token) # 把字符串 "Hello" 拆成单个字符组成的列表['H', 'e', 'l', 'l', 'o']
            # 字符数超限制，直接记为 OOV
            if len(chars) > self.max_input_chars_per_word:  
                output_tokens.append(self.unk_token)
                continue

            is_bad = False
            start = 0
            sub_tokens = []
            #  外层循环：逐个拆分 token 的字符
            while start < len(chars):  
                end = len(chars)
                cur_substr = None
                # 内层循环：贪心最长匹配，从长到短找子词
                while start < end:  
                    substr = "".join(chars[start:end])
                    # 非起始位置的子词，加 ## 标记（区分是否是词首，辅助模型理解边界 ）
                    if start > 0:  
                        substr = "##" + substr
                    # 子词在词表中，就选中它
                    if substr in self.vocab:  
                        cur_substr = substr
                        break
                    # 没找到，缩短子串继续找（比如从 "abcde" 找 "abcde"→"abcd"→"abc"… ）
                    end -= 1  
                # 没找到任何子词匹配，标记为坏词，后续记为 OOV
                if cur_substr is None:  
                    is_bad = True
                    break
                sub_tokens.append(cur_substr)  # 记录找到的子词
                start = end  # 从 end 位置继续拆分剩余字符

            # 坏词（无法拆分）则记为 OOV，否则把拆分的子词加入结果
            if is_bad:  
                output_tokens.append(self.unk_token)
            else:
                output_tokens.extend(sub_tokens)
        return output_tokens
 
def whitespace_tokenize(text):# 输入 = "  Hello   world!  "
    text = text.strip() #strip 去除首尾可能的空白，然后 \n（换行）、\t（制表符）都被当空白符，拆分出三个元素
    if not text:
        return []
    tokens = text.split()
    return tokens # 输出：['Hello', 'world!']
```



##### 二、BPE算法

​	BPE 的最大化目标是选取单词内相邻单元对的频数。首先分词器会为语料库中的字符建立一个基础词表，接着会执行子词合并。子词合并策略即每次选取出现频率最高的子词单元对，形成新子词单元，然后将其加入词表中。

- **核心思想**：直接从最细碎开始（字符），通过迭代合并最频繁出现的**字符对**，逐步构建子词词汇表。

- **训练阶段（构建子词表）**:

  - **初始化**：将语料库中的每个单词拆分为字符序列，并添加特殊标记（如 "</w>"）以表示词边界。初始词汇表由所有唯一字符组成。
  - **统计字符对频率**：计算语料库中相邻字符对的出现频率。
  - **合并最频繁的字符对**：选择出现频率最高的字符对，将它们合并为一个新的子词，并添加到词汇表中。
  - **更新语料库**：在语料库中，将所有出现的该字符对替换为新合并的子词。
  - **重复步骤 2-4**：迭代地执行上述步骤，直到达到预设的词汇表大小或合并次数。
  - **分词**：使用训练好的词汇表对新文本进行分词，将单词拆分为词汇表中存在的子词序列

- ##### 应用训练好的合并规则：

  - BPE 算法的核心优势在于，它不仅能从训练语料中学习子词，还能将这些知识应用到**未在训练中出现的新词**上。这一过程主要依赖于**训练阶段记录的合并规则**，而不是直接查表匹配。
  - 通过在语料库训练过程中已经学到的**合并规则以及合并顺序**，对新来的词汇进行合并字符对，从而得到子词分解的结果。

训练+应用代码如下：

```python
import re
from collections import Counter, defaultdict
from transformers import AutoTokenizer


class BPE():
    """ 字节对编码：基于子词的分词算法 """
    # 假设输入 text = "low lower newest widest"
    def __init__(self, corpus, vocab_size):
        """ 初始化 BPE 分词器 """
        self.corpus = corpus
        self.vocab_size = vocab_size

        # 使用BERT的预分词器将语料分割成单词
        self.tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
        self.word_freqs = defaultdict(int)  # 存储单词频率
        self.splits = {}  # 存储每个单词的当前分割状态
        self.merges = {}  # 存储合并规则（字符对 -> 合并后的子词）

    def train(self):
        """ 训练 BPE 分词器 """
        # 1. 计算语料中每个单词的频率
        # 得到 word_freqs = {"low": 1,"lower": 1,"newest": 1,"widest": 1}
        for text in self.corpus:
            # 使用BERT预分词器获取单词和偏移量
            pre_tokenizer = self.tokenizer.backend_tokenizer.pre_tokenizer
            words_with_offsets = pre_tokenizer.pre_tokenize_str(text)
            new_words = [word for word, offset in words_with_offsets]
            for word in new_words:
                self.word_freqs[word] += 1

        # 2. 构建基础词汇表（所有唯一字符）
        # 得到 alphabet = ["l", "o", "w", "e", "r", "n", "s", "t", "i", "d"]
        alphabet = []
        for word in self.word_freqs.keys():
            for letter in word:
                if letter not in alphabet:
                    alphabet.append(letter)
        alphabet.sort()

        # 3. 添加词边界标记 "</w>" 到词汇表
        # vocab = ["</w>"] + alphabet = ["</w>", "l", "o", "w", "e", "r", "n", "s", "t", "i", "d"]
        vocab = ["</w>"] + alphabet.copy()

        # 4. 初始化每个单词的分割状态为单个字符
        '''
        splits = {
            "low": ["l", "o", "w"],
            "lower": ["l", "o", "w", "e", "r"],
            "newest": ["n", "e", "w", "e", "s", "t"],
            "widest": ["w", "i", "d", "e", "s", "t"]
        }
        '''
        self.splits = {word: [c for c in word] for word in self.word_freqs.keys()}

        # 5. 迭代合并最频繁的字符对，直到达到目标词汇表大小
        while len(vocab) < self.vocab_size:
            # 计算当前所有相邻字符对的频率
            pair_freqs = self.compute_pair_freqs()

            # 找到频率最高的字符对
            best_pair = ""
            max_freq = None
            for pair, freq in pair_freqs.items():
                if max_freq is None or max_freq < freq:
                    best_pair = pair
                    max_freq = freq

            # 如果没有找到可合并的字符对，停止迭代
            if not best_pair:
                break

            # 合并最频繁的字符对
            self.splits = self.merge_pair(best_pair[0], best_pair[1])
            # 记录合并规则
            self.merges[best_pair] = best_pair[0] + best_pair[1]
            # 将合并后的子词添加到词汇表
            vocab.append(best_pair[0] + best_pair[1])

        return self.merges

    def compute_pair_freqs(self):
        """ 计算每对相邻字符的频率 """
        pair_freqs = defaultdict(int)
        for word, freq in self.word_freqs.items():
            split = self.splits[word]  # 获取当前单词的分割状态
            if len(split) == 1:  # 如果只有一个字符，无法形成字符对
                continue
            # 统计相邻字符对的频率
            for i in range(len(split) - 1):
                pair = (split[i], split[i + 1])
                pair_freqs[pair] += freq
        return pair_freqs

    def merge_pair(self, a, b):
        """ 合并给定的字符对 """
        for word in self.word_freqs:
            split = self.splits[word]  # 获取当前单词的分割状态
            if len(split) == 1:  # 如果只有一个字符，无法合并
                continue
            i = 0
            while i < len(split) - 1:
                if split[i] == a and split[i + 1] == b:
                    # 合并字符对，更新分割状态
                    split = split[:i] + [a + b] + split[i + 2:]
                else:
                    i += 1
            self.splits[word] = split  # 更新单词的分割状态
        return self.splits

    def tokenize(self, text):
        """
        用训练好的 BPE 分词器对给定的文本进行分词
        """
        # 1. 使用BERT预分词器将文本分割成单词
        pre_tokenizer = self.tokenizer.backend_tokenizer.pre_tokenizer
        pre_tokenize_result = pre_tokenizer.pre_tokenize_str(text)
        pre_tokenized_text = [word for word, offset in pre_tokenize_result]
        
        # 2. 将每个单词初始分割为字符
        splits_text = [[l for l in word] for word in pre_tokenized_text]

        # 3. 应用训练好的合并规则
        for pair, merge in self.merges.items():
            for idx, split in enumerate(splits_text):
                i = 0
                while i < len(split) - 1:
                    if split[i] == pair[0] and split[i + 1] == pair[1]:
                        # 合并字符对
                        split = split[:i] + [merge] + split[i + 2:]
                    else:
                        i += 1
                splits_text[idx] = split

        # 4. 展平分词结果
        result = sum(splits_text, [])
        return result
```

最后，我们来总结一下 WordPiece 和 BPE 这两种方法。

- **合并策略**：
  - BPE：选择最频繁的字符对进行合并。
  - WordPiece：选择合并后能最大化语料库似然值的字符对。
- **训练目标**：
  - BPE：基于频率统计，不直接优化任何语言模型目标。
  - WordPiece：显式地优化语言模型的似然值，因此在某些情况下可能产生更符合语义的子词。
- **实现复杂度**：
  - BPE：实现相对简单，计算效率高。
  - WordPiece：需要**计算合并后的似然值变化**，实现相对复杂，计算开销较大。
