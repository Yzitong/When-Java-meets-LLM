# 强化学习基础概念

强化学习的全套数学原理：[【强化学习的数学原理】课程：从零开始到透彻理解（完结）_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.337.search-card.all.click&vd_source=aa6afb9d0536d09ecdcb5d2c1fcf4c79)

时间有限的话，可以看重点：MDP、贝尔曼、策略迭代、蒙特卡洛、DQN、策略梯度、PPO、DPO、RLHF、GRPO

![image-20250708113800262](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250708113800262.png)

# 强化学习内容概览

**1: Basic Concepts（基础概念）**

- **位置**：体系起点，无前置依赖。
- **内容**：定义强化学习核心元素（智能体、环境、状态、动作、奖励、策略、价值函数），引入马尔可夫决策过程（MDP）。
- **角色**：构建知识“骨架”，明确目标：智能体通过试错学习最优策略。

**2: Bellman Equation（贝尔曼方程）**

- **位置**：理论基础，承接MDP。
- **内容**：推导状态价值函数 $V_\pi(s)$ 和动作价值函数 $Q_\pi(s,a)$ 的递归关系，如 $V_\pi(s) = \mathbb{E}[R_{t+1} + \gamma V_\pi(S_{t+1}) \mid S_t=s]$。
- **角色**：核心数学工具，将长期回报分解为即时奖励与后续价值。

**3: Bellman Optimality Equation（贝尔曼最优方程）**

- **位置**：理论进阶，引入最优目标。
- **内容**：定义最优价值函数 $V^*(s) = \max_\pi V_\pi(s)$、$Q^*(s,a) = \max_\pi Q_\pi(s,a)$，推导递归关系，如 $V^*(s) = \max_a \mathbb{E}[R_{t+1} + \gamma V^*(S_{t+1}) \mid S_t=s, A_t=a]$。
- **角色**：为动态规划提供最优策略的数学目标。

**4: Value Iteration & Policy Iteration（价值迭代与策略迭代）**

- **位置**：Model-Based方法核心，环境模型已知。
- 内容：
  - 策略迭代：策略评估（用贝尔曼方程）+ 策略改进（贪心更新）。
  - 价值迭代：直接更新价值至最优。
- **角色**：首个可求解最优策略的算法族，为Model-Free方法提供对比。

**5: Monte Carlo Learning（蒙特卡洛学习）**

- **位置**：Model-Free入门，依赖完整episode。
- **内容**：用回报 $G_t = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{T-1}R_T$ 估计价值，结合ε-贪心实现控制。
- **角色**：展示无模型学习的可行性，铺垫TD增量学习。

**6: Stochastic Approximation（随机近似）**

- **位置**：为增量学习提供理论支撑。
- **内容**：分析带噪声迭代（如Robbins-Monro算法），步长收敛条件：$\alpha_k \to 0$、$\sum \alpha_k = \infty$、$\sum \alpha_k^2 < \infty$。
- **角色**：为TD和函数近似提供数学保证。

**7: Temporal-Difference Learning（时序差分学习）**

- **位置**：Model-Free核心，支持增量学习。
- 内容：
  - TD(0)：单步更新 $V(s) \approx r + \gamma V(s')$。
  - TD(λ)：结合资格迹，平衡偏差与方差。
  - 控制算法：SARSA（同策略）、Q-Learning（异策略）。
- **角色**：突破MC局限，支持持续学习，为函数近似提供基础。

**8: Value Function Approximation（价值函数近似）**

- **位置**：扩展层，处理大规模状态空间。
- **内容**：用函数近似器（线性模型、神经网络）学习 $V_\theta(s) \approx V(s)$，包括GTD、LSTD等。
- **角色**：实现Value-Based方法规模化，支撑Actor-Critic的Critic。

**9: Policy Gradient Methods（策略梯度方法）**

- **位置**：Policy-Based方法核心。
- **内容**：策略梯度定理 $\nabla_\theta J(\theta) = \mathbb{E}*\pi [Q*\pi(s,a) \nabla_\theta \log\pi_\theta(a|s)]$，算法如REINFORCE。
- **角色**：直接优化策略，适用于连续动作空间，支撑Actor-Critic的Actor。

**10: Actor-Critic Methods（演员-评论家方法）**

- **位置**：融合Value-Based与Policy-Based。
- **内容**：Actor（优化策略 $\pi_\theta$）+ Critic（估计 $Q_\pi$ 或 $V_\pi$），算法如A2C、DDPG、PPO。
- **角色**：集大成者，平衡优势，适用于复杂环境。



# 强化学习基本概念（以Grid-World为例）

## 1. 强化学习简介

强化学习（Reinforcement Learning, RL）研究智能体（Agent）通过与环境（Environment）交互，通过试错学习最优策略，以最大化长期累积奖励。马尔可夫决策过程（MDP）是其数学框架，以下通过Grid-World示例介绍核心概念。

## 2. Grid-World示例

Grid-World是一个简单的二维网格环境，智能体需从起点（如左上角）移动到目标（如右下角），避开障碍（如墙）。假设网格为 $4 \times 4$，起点在 $(0,0)$，目标在 $(3,3)$，障碍在 $(1,1)$ 和 $(2,2)$。智能体可执行动作：上、下、左、右。

![image-20250708121520350](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250708121520350.png)

## 3. 基本概念（结合MDP和Grid-World）

强化学习的核心概念基于MDP，形式化为五元组 $(S, A, P, R, \gamma)$，以下逐一说明并结合Grid-World解释。

### 3.1 状态（State, $S$）

- **定义**：环境的可能配置集合，表示智能体当前所处的状况。
- **Grid-World示例**：状态是网格中的位置坐标 $(x, y)$，如 $(0,0)$、$(1,2)$。$S$ 包含所有非障碍格子，共 $16 - 2 = 14$ 个状态。
- **MDP语义**：$S$ 是有限或无限的状态集合，满足马尔可夫性质（下一状态仅依赖当前状态和动作）。

### 3.2 动作（Action, $A$）

- **定义**：智能体可执行的操作集合。
- **Grid-World示例**：$A = { \text{上}, \text{下}, \text{左}, \text{右} }$。在边缘或障碍附近，某些动作可能无效（如撞墙保持原地）。
- **MDP语义**：$A(s)$ 表示状态 $s$ 可用的动作集合，可能因状态而异。

### 3.3 状态转移概率（Transition Probability, $P$）

- **定义**：执行动作后，环境从当前状态转移到下一状态的概率，记为 $P(s'|s, a)$，表示在状态 $s$ 执行动作 $a$ 后转移到状态 $s'$ 的概率。
- **Grid-World示例**：假设动作确定性转移（无随机性），如在 $(0,0)$ 执行“右”，$P((0,1)|(0,0), \text{右}) = 1$，其他状态的转移概率为 0。若引入随机性（如 10% 概率动作失败，保持原地），则 $P((0,1)|(0,0), \text{右}) = 0.9$，$P((0,0)|(0,0), \text{右}) = 0.1$。
- **MDP语义**：$P$ 描述环境动态，满足 $\sum_{s' \in S} P(s'|s, a) = 1$。

### 3.4 奖励函数（Reward Function, $R$）

- **定义**：智能体执行动作后从环境获得的即时反馈，记为 $R(s, a, s')$，根据你当前的状态与动作给予的，表示从状态 $s$ 执行动作 $a$ 转移到 $s'$ 的奖励。
- **Grid-World示例**：到达目标 $(3,3)$ 得奖励 $+1$，撞墙或普通移动得奖励 $-0.1$（鼓励快速到达）。如从 $(2,3)$ 右移到 $(3,3)$，$R((2,3), \text{右}, (3,3)) = 1$；从 $(0,0)$ 右移到 $(0,1)$，$R((0,0), \text{右}, (0,1)) = -0.1$。
- **MDP语义**：$R$ 驱动智能体优化行为，期望奖励为 $R(s, a) = \mathbb{E}[R(s, a, s')]$。

### 3.5 折扣因子（Discount Factor, $\gamma$）

- **定义**：衡量未来奖励对当前决策影响的权重，$\gamma \in [0, 1)$。
- **Grid-World示例**：设 $\gamma = 0.9$，表示未来奖励以 0.9 的因子逐级衰减。如果减小折扣因子，表示折扣的比较快，那就会让智能体变得更加近视；如果增大折扣因子，折扣的比较慢，智能体就会更远视，而注重长期收益。
- **MDP语义**：$\gamma$ 确保累积奖励收敛，平衡短期与长期收益。

### 3.6 策略（Policy, $\pi$）

- **定义**：智能体的行为规则，$\pi(a|s)$ 表示在状态 $s$ 选择动作 $a$ 的概率。
- **Grid-World示例**：确定性策略可能为“在 $(0,0)$ 总是选右”，即 $\pi(\text{右}|(0,0)) = 1$；随机策略可能为“在 $(0,0)$ 以 0.5 概率选右，0.5 概率选下”。
- **MDP语义**：$\pi$ 决定智能体行为，目标是找到最优策略 $\pi^*$ 最大化累积奖励。

### 3.7 价值函数（Value Function）

- 定义：
  - **状态价值函数** $V_\pi(s)$：在策略 $\pi$ 下，从状态 $s$ 开始的期望累积奖励，$V_\pi(s) = \mathbb{E}*\pi[\sum*{t=0}^\infty \gamma^t R_{t+1} | S_0 = s]$。
  - **动作价值函数** $Q_\pi(s, a)$：在状态 $s$ 执行动作 $a$，随后按 $\pi$ 行动的期望累积奖励，$Q_\pi(s, a) = \mathbb{E}*\pi[\sum*{t=0}^\infty \gamma^t R_{t+1} | S_0 = s, A_0 = a]$。
- **Grid-World示例**：在 $(2,3)$，若 $\pi$ 指定“右”到 $(3,3)$，$V_\pi((2,3)) \approx 1$（忽略后续小惩罚）；$Q_\pi((2,3), \text{右}) \approx 1$，而 $Q_\pi((2,3), \text{上})$ 可能较低（如 -0.1）。
- **MDP语义**：价值函数评估策略好坏，指导策略优化。

### 3.8 回报（Return, $G_t$）

- **定义**：从时刻 $t$ 开始的折扣累积奖励，$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots$。
- **Grid-World示例**：从 $(2,3)$ 右移到 $(3,3)$，得奖励 $+1$，若无后续动作，$G_t = 1$。若多步路径（如 $(1,2) \to (2,2) \to (2,3) \to (3,3)$），$G_t = -0.1 + \gamma(-0.1) + \gamma^2(1)$。
- **MDP语义**：$G_t$ 是价值函数的采样估计，强化学习目标是最大化 $\mathbb{E}[G_t]$。

## 4. MDP在Grid-World中的整合

- MDP五元组：
  - $S$：14 个非障碍格子。
  - $A$：${ \text{上}, \text{下}, \text{左}, \text{右} }$。
  - $P$：确定性或带噪声的状态转移。
  - $R$：目标 $+1$，其他 $-0.1$。
  - $\gamma$：如 0.9。
- **目标**：基于 $\pi$，通过交互（执行动作 $a$，观察 $s'$ 和 $r$）学习最优策略 $\pi^*$，使 $V_{\pi^*}(s)$ 或 $Q_{\pi^*}(s, a)$ 最大。
- **过程**：智能体在 $(0,0)$ 启动，尝试动作，观察奖励和状态转移，更新 $\pi$ 或价值函数（如通过动态规划或蒙特卡洛方法），最终学到最短路径到 $(3,3)$。
- **马尔可夫性质（Markov Property，又称 “无记忆性”）** 在强化学习的马尔可夫决策过程（MDP）中的数学定义，核心是 **“未来仅由当前状态和动作决定，与更久远的历史无关”**

![image-20250708132538595](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250708132538595.png)

