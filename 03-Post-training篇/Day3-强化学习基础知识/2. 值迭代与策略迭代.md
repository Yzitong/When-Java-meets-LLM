# 值迭代与策略迭代

这里开始才正式的介绍强化学习中的算法，这节介绍的三类算法都属于 **基于模型（Model-Based）的强化学习方法**，用于求解马尔可夫决策过程（MDP）的**最优策略**：

- **价值迭代（Value Iteration）**：直接迭代更新**最优状态价值**（跳过显式策略表示，通过价值收敛推导最优策略）。
- **策略迭代（Policy Iteration）**：分两步循环：① **策略评估**（计算当前策略的状态价值）；② **策略改进**（贪心更新策略）。
- **截断策略迭代（Truncated Policy Iteration）**：对策略迭代的优化，减少 “策略评估” 的迭代次数（如只做 1 次价值更新），平衡计算效率与精度。

## 一、价值迭代算法（Value Iteration）

### 核心思想

直接迭代更新**最优状态价值$v$**，跳过“显式策略表示”，最终从最优价值推导最优策略。

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250708222126539.png" alt="image-20250708222126539" style="zoom:80%;" />

### 算法步骤

1. **初始化**：设定初始状态价值$v_0$（如全0向量）。

2. **迭代更新阶段：**

   a. 对每个状态 s（遍历所有状态）：

   i. 对每个可能的动作 a（遍历该状态下的所有动作）：

   - 计算该动作 a 的 Q 值：$Q (s,a) = 即时奖励 r (s,a) + γ×Σ（转移概率 p (s'|s,a) × 上一轮状态价值 v_prev (s')$）
     ii. 找出该状态 s 下所有动作 a 中 Q 值最大的那个，作为 s 的新价值 $v_new (s)$
     b. 将所有状态的 $v_new$ 替换 $v_prev$，进入下一轮迭代
   - $$v_{k+1}(s) \leftarrow \max_a \left[ r(s, a) + \gamma \sum_{s'} p(s'|s, a) v_k(s') \right]$$
     （对每个状态$s$，直接选择能最大化“即时奖励+未来价值”的动作，更新当前状态价值。）

3. **终止条件**：若$| v_{k+1} - v_k | < \theta$（价值收敛），停止迭代。

4. **得到最优策略$\pi^*$：**

- **优点**：计算更简洁（无需显式维护策略，每次迭代仅需一次“最大化”操作），适合状态数多但动作少的场景。
- **缺点**：收敛速度可能慢于策略迭代（每次仅更新一步价值，需更多轮次收敛）。

## 二、策略迭代算法（Policy Iteration）

### 核心思想

通过**“策略评估 → 策略改进”**交替迭代，逐步逼近最优策略：

- **策略评估**：固定当前策略，精确计算其状态价值（用贝尔曼期望方程迭代到收敛）。
- **策略改进**：根据当前状态价值，贪心更新策略（选能最大化动作价值的动作）。

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250708224248511.png" alt="image-20250708224248511" style="zoom:80%;" />

### 算法步骤

1. **初始化**：随机设定初始策略$\pi_0$（如每个状态下动作等概率选择）。
2. **策略评估（Policy Evaluation）**：
   对当前策略$\pi_k$，用贝尔曼期望方程迭代计算状态价值$v_{\pi_k}$，直到收敛：
   $$v_{\pi_k}(s) \leftarrow \sum_a \pi_k(a|s) \left[ r(s, a) + \gamma \sum_{s'} p(s'|s, a) v_{\pi_k}(s') \right]$$
   （对每个状态$s$，更新其价值为“当前策略下的期望回报”。）
3. **策略改进（Policy Improvement）**：
   对每个状态$s$，贪心选择动作$a$以最大化动作价值$Q_{\pi_k}(s, a)$，得到新策略$\pi_{k+1}$：
   $$\pi_{k+1}(s) \leftarrow \arg\max_a \left[ r(s, a) + \gamma \sum_{s'} p(s'|s, a) v_{\pi_k}(s') \right]$$
4. **终止条件**：若$\pi_{k+1} = \pi_k$（策略不再变化），则$\pi_{k+1}$是最优策略；否则回到步骤2。

### 特点

- **优点**：收敛速度快（策略改进保证每次迭代严格提升策略性能），最终能得到**精确最优策略**。
- **缺点**：策略评估需迭代到收敛，当状态数多（如$10^4$以上）时，计算成本极高。

### 值迭代VS策略迭代：

- 策略迭代就是，初始化策略，由贝尔曼算值，然后更新策略。
- 值迭代就是，初始化值，最大化动作选策略，然后更新值。

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250708230849374.png" alt="image-20250708230849374" style="zoom:80%;" />

## 三、截断策略迭代算法（Truncated Policy Iteration）

### 核心思想

**折中策略迭代和价值迭代的优缺点**：在策略评估阶段，**不等到价值完全收敛**，而是设置固定迭代次数$k$（截断步数），然后立即进行策略改进。值迭代与策略迭代实际上是阶段策略迭代算法的2个极端情况。

首先思考一个问题：值迭代和策略迭代这么像，他们的区别在哪呢？

![image-20250708231843723](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250708231843723.png)

对比可以发现，主要的区别在于更新的值计算方式是不同的。策略迭代每一次更新的值，是由贝尔曼公式计算得来的值；而值迭代计算的是直接带入初始化的值所得来的。

进一步想，贝尔曼计算值时，怎么解的呢？就是通过迭代无穷多次所得的，那么两者的区别就显而易见了。那我们就想会不会有一种算法可以迭代一个中间的次数来更新值呢？毕竟实际上迭代解贝尔曼公式时我们也是不可能真的迭代无穷次的。

引出截断策略迭代算法：

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250708231807922.png" alt="image-20250708231807922" style="zoom:80%;" />

### 算法步骤

1. **初始化**：设定初始策略$\pi_0$和状态价值$v_0$。
2. **截断策略评估**：
   对当前策略$\pi_k$，仅进行**$k$次价值迭代**（用贝尔曼期望方程更新价值，但提前终止），得到近似价值$\hat{v}*{\pi_k}$：
   $$\hat{v}*{\pi_k}^{(1)}(s) \leftarrow \sum_a \pi_k(a|s) \left[ r(s, a) + \gamma \sum_{s'} p(s'|s, a) v_k(s') \right] \
   \hat{v}*{\pi_k}^{(2)}(s) \leftarrow \sum_a \pi_k(a|s) \left[ r(s, a) + \gamma \sum*{s'} p(s'|s, a) \hat{v}*{\pi_k}^{(1)}(s') \right] \
   \vdots \
   \hat{v}*{\pi_k}^{(k)}(s) \leftarrow \sum_a \pi_k(a|s) \left[ r(s, a) + \gamma \sum_{s'} p(s'|s, a) \hat{v}_{\pi_k}^{(k-1)}(s') \right]$$
3. **策略改进**：同策略迭代，用$\hat{v}*{\pi_k}^{(k)}$贪心更新策略$\pi*{k+1}$。
4. **终止条件**：若策略稳定（$\pi_{k+1} = \pi_k$），停止；否则回到步骤2。

### 特点

- 灵活性

  ：通过调整$k$平衡效率和精度：

  - $k=1$：接近**价值迭代**（仅做1次策略评估更新，计算快但精度低）。
  - $k \to \infty$：接近**策略迭代**（策略评估完全收敛，精度高但计算慢）。

- **适用场景**：对计算资源敏感的场景（如大规模状态空间），可根据实际需求调优$k$。

## 四、三者对比总结

| **维度**     | 策略迭代                   | 价值迭代                    | 截断策略迭代              |
| ------------ | -------------------------- | --------------------------- | ------------------------- |
| **核心步骤** | 策略评估（收敛）→ 策略改进 | 直接更新最优价值 → 推导策略 | 截断评估（k次）→ 策略改进 |
| **收敛性**   | 严格收敛到最优策略         | 收敛到最优价值（推导策略）  | 随$k$增加逼近最优         |
| **计算成本** | 高（策略评估需收敛）       | 中（每次仅一步更新）        | 灵活（由$k$控制）         |
| **适用场景** | 状态少、追求精确           | 动作少、追求简洁            | 状态多、需平衡效率-精度   |

## 五、直观类比

![image-20250708232521293](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250708232521293.png)

- **策略迭代**：“精算师”，先把当前策略的价值算透，再改进策略。
- **价值迭代**：“探险家”，直接朝最优价值的方向一步一步走，边走边看哪条路最好。
- **截断策略迭代**：“聪明的探险家”，走几步就看看方向对不对，及时调整，不浪费时间算到底。

通过这种设计，三者覆盖了从“精确但慢”到“高效但近似”的需求，是强化学习**基于模型方法**的核心框架。