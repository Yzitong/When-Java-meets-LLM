| **维度**     | **RLHF + PPO**                    | **DPO**                        |
| ------------ | --------------------------------- | ------------------------------ |
| **核心流程** | SFT → RM训练 → PPO优化            | SFT → DPO直接优化              |
| **反馈形式** | 依赖奖励模型的评分信号            | 直接使用人类偏好排序数据       |
| **计算成本** | 高（需训练奖励模型和多次PPO迭代） | 低（单阶段优化，无需奖励模型） |
| **适用场景** | 高精度对齐、复杂决策任务          | 轻量对齐、资源受限或垂类场景   |
| **泛化能力** | 较强（奖励模型可捕捉隐性偏好）    | 较弱（依赖显式偏好数据）       |