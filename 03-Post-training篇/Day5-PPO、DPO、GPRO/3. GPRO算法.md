# DeepSeek-GPRO算法

B站指路：[DeepSeek-GRPO_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1enQLYKEA5/?spm_id_from=333.1387.homepage.video_card.click&vd_source=aa6afb9d0536d09ecdcb5d2c1fcf4c79)

本节从**PPO的局限→GRPO的创新→实际案例**逐步拆解GRPO在强化学习对齐（RLHF）中的运作，结合数学公式和示例，深入讲解其原理与优势。

---

## 一、PPO在大模型RLHF中的核心局限

在大模型的强化学习对齐（RLHF）中，PPO面临三大关键问题，导致其在**语言生成任务**中效率低、稳定性差：

### 1. 稀疏奖励与价值网络的矛盾
- PPO依赖**价值网络（Critic）**估计每个token的**状态价值 $V(s)$**，计算优势 $A = \text{累积奖励} - V(s)$。
- 但语言生成的奖励是**序列级稀疏奖励**（如“回答是否正确”仅在结尾判断，中间token无奖励），价值网络对中间token的价值估计完全是“猜测”，误差会随序列长度累积，导致优势计算失真。
  - **示例**：生成数学证明时，只有最终结论正确才给+1分，中间步骤（如“因为”“所以”）的价值被价值网络错误估计，干扰策略更新。

### 2. 单样本绝对奖励的不合理性
- PPO的优势基于**单个样本的绝对奖励**，但大模型生成时，**同一Prompt的不同回答（如“数据库是存储数据的” vs “数据库是高效存取数据的软件”）的优劣需要相对比较**，单样本奖励无法体现这种差异。
  - **示例**：若仅看单个回答的奖励（如3.8分），无法判断它比另一个回答（5.2分）差多少，而相对比较能明确差距。

### 3. PPO-CLIP的刚性约束
- PPO通过**统一的裁剪范围（如 $\text{clip}(r, 1-\varepsilon, 1+\varepsilon)$）**限制策略更新幅度，但对**低概率探索性token（如创新表达）**和**高概率保守token（如常用词）**采用相同约束：
  - 低概率token（如概率0.01）最多只能提升到0.012（$\varepsilon=0.2$），探索能力被抑制；
  - 高概率token（如概率0.9）可提升到1.08，容易过度优化导致熵坍塌（回复多样性消失）。

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250711223647123.png" alt="image-20250711223647123" style="zoom:80%;" />

---

## 二、GRPO：Group Relative Policy Optimization

### 1. 全称与核心定义
- **全称**：Group Relative Policy Optimization（**群体相对策略优化**）。
- **核心思想**：对**同一Prompt的多个候选回答（形成“群体”）**，通过**相对奖励归一化**替代价值网络，直接优化策略的**相对优势**，解决PPO的稀疏奖励和刚性约束问题。

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250711224002247.png" alt="image-20250711224002247" style="zoom:80%;" />

### 2. GRPO的核心改进（针对PPO局限）
| **PPO局限**                   | **GRPO改进**                                                 |
| ----------------------------- | ------------------------------------------------------------ |
| 依赖价值网络估计中间token价值 | **组内相对奖励**：同一Prompt生成多个回答，用组内奖励的**均值/标准差归一化**，替代价值网络。 |
| 单样本绝对奖励不合理          | **群体比较**：通过同一Prompt的多个回答的奖励对比，明确“更优回答”的**相对优势**。 |
| PPO-CLIP约束刚性              | **解耦裁剪范围**：对**探索性token（低概率）**和**利用性token（高概率）**设置不同裁剪界，鼓励探索。 |

---

## 三、GRPO的损失函数与核心公式

GRPO的目标函数在PPO基础上，对**优势计算、CLIP约束、KL正则**进行了革新，公式如下：

$$
J_{\text{GRPO}} = \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n} \min\left( 
A_{\theta'}^{\text{GRPO}}(s_n^t, a_n^t) \cdot \frac{P_\theta(a_n^t|s_n^t)}{P_{\theta'}(a_n^t|s_n^t)}, \ 
\text{clip}\left( \frac{P_\theta(a_n^t|s_n^t)}{P_{\theta'}(a_n^t|s_n^t)}, \ 1-\varepsilon_{\text{low}}, \ 1+\varepsilon_{\text{high}} \right) \cdot A_{\theta'}^{\text{GRPO}}(s_n^t, a_n^t) 
\right) - \beta \cdot \text{KL}(P_\theta \|| P_{\theta'})
$$

### 公式拆解：
1. **$A_{\text{GRPO}}$（相对优势）**：
   对同一Prompt的$G$个候选回答，计算每个回答的奖励$r_i$，通过**组内归一化**得到相对优势：
   $$
   A_{\text{GRPO}} = \frac{r_i - \mu_r}{\sigma_r}, \quad \mu_r = \frac{1}{G}\sum_{i=1}^G r_i, \ \sigma_r = \sqrt{\frac{1}{G-1}\sum_{i=1}^G (r_i - \mu_r)^2}
   $$
   
   - **示例**：3个回答的奖励为3.8、5.2、6.1，均值$\mu_r=5.03$，标准差$\sigma_r\approx1.15$，归一化后优势为：
     - 回答1：$\frac{3.8-5.03}{1.15} \approx -1.06$（相对差）；
     - 回答2：$\frac{5.2-5.03}{1.15} \approx 0.14$（中等）；
     - 回答3：$\frac{6.1-5.03}{1.15} \approx 0.92$（相对优）。
   
2. **解耦CLIP范围（$\varepsilon_{\text{low}}$ vs $\varepsilon_{\text{high}}$）**：
   - **低概率token（探索性，如创新表述）**：设置较小的下界 $1-\varepsilon_{\text{low}}$（如$\varepsilon_{\text{low}}=0.1$），允许其概率从0.01提升到0.011，鼓励探索；
   - **高概率token（利用性，如常用词）**：设置较大的上界 $1+\varepsilon_{\text{high}}$（如$\varepsilon_{\text{high}}=0.3$），允许其概率从0.9提升到1.17，强化利用。

3. **KL约束项**：
   显式约束当前策略$P_\theta$与参考策略$P_{\theta'}$（如SFT模型）的差异，避免策略突变，$\beta$控制约束强度（比PPO更灵活）。

---

## 四、为什么GRPO不需要估计状态价值？

GRPO通过**“组内相对奖励”**天然替代了价值网络的功能，核心逻辑如下：

1. **组内归一化隐含状态价值**：
   同一Prompt的多个回答共享相同的“状态”（Prompt）。组内奖励的**均值$\mu_r$**可视为该状态下的**平均价值**，而**相对优势$A_{\text{GRPO}} = \frac{r_i - \mu_r}{\sigma_r}$**则衡量了“当前回答比平均水平好多少”，这与PPO中 $A = R - V(s)$ 的逻辑一致，但**无需显式训练价值网络**。

2. **序列级奖励的整体处理**：
   语言生成的奖励是**序列级**（如整个回答是否正确），GRPO将**整个回答视为一个“状态-动作”单元**，而非逐token分解。因此，优势计算基于**整个回答的最终奖励**，自然避免了对中间token状态价值的估计。

---

## 五、GRPO对PPO的核心改进总结

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250711224136990.png" alt="image-20250711224136990" style="zoom:80%;" />

1. **优势计算革新**：
   - PPO：依赖价值网络的**绝对优势**（$A = R - V(s)$），需逐token估计状态价值。
   - GRPO：基于**组内相对优势**（$A_{\text{GRPO}} = \frac{r_i - \mu_r}{\sigma_r}$），通过同一Prompt的多个回答的统计特性，替代价值网络。

2. **策略约束解耦**：
   - PPO-CLIP：统一裁剪范围（如$1 \pm \varepsilon$），抑制探索。
   - GRPO：解耦$\varepsilon_{\text{low}}$和$\varepsilon_{\text{high}}$，区分探索性和利用性token，平衡探索与利用。

3. **稀疏奖励适配**：
   - PPO：需将稀疏奖励拆解为逐token的累积奖励（依赖价值网络，误差大）。
   - GRPO：直接处理**序列级稀疏奖励**，通过组内比较明确优劣，无需中间token奖励。

---

## 六、示例：GRPO训练数学解题模型

下面通过一个详细的数学解题案例，展示GRPO如何训练模型解决数学问题，并体现其相对于PPO的优势。

### 1. **Prompt**：*"求解方程 $2x + 3 = 7$ 的值。"*
### 2. **候选回答（群体）**：
   - **回答1**：*"直接猜：$x = 2$，因为治安，$2 \times 2 + 3 = 7$，正确。所以 $x = 2$。"* → 奖励6.0（正确但过程不完整）。
   - **回答2**：*"两边减3，得$2x = 4$，再除以2，得$x = 2$。"* → 奖励8.0（正确且过程清晰）。
   - **回答3**：*"$2x + 3 = 7$，移项得$2x = 7 - 3$，计算得$2x = 4$，两边除以2，得$x = 2$。"* → 奖励9.0（最完整、最清晰）。

### 3. **相对优势计算**：
   - **均值** $\mu_r = \frac{6.0 + 8.0 + 9.0}{3} = 7.67$；
   - **标准差** $\sigma_r = \sqrt{\frac{(6.0-7.67)^2 + (8.0-7.67)^2 + (9.0-7.67)^2}{2}} \approx 1.25$；
   - **归一化后优势**：
     - 回答1：$\frac{6.0 - 7.67}{1.25} \approx -1.34$（相对差）；
     - 回答2：$\frac{8.0 - 7.67}{1.25} \approx 0.26$（中等）；
     - 回答3：$\frac{9.0 - 7.67}{1.25} \approx 1.06$（相对优）。

### 4. **策略更新**：
   - GRPO根据相对优势，优化模型倾向于生成**回答3**（高优势），同时微调**回答2**，抑制**回答1**。
   - **解耦CLIP范围**应用：
     - 对于探索性token（如“移项”“除以2”），$\varepsilon_{\text{low}}=0.1$，允许创新表达的概率小幅提升；
     - 对于高概率token（如“得$x = 2$”），$\varepsilon_{\text{high}}=0.3$，强化其生成概率。
   - 由于奖励是**序列级**（整个解题过程是否正确），GRPO直接基于最终奖励计算优势，无需估计中间token（如“两边”“减3”）的价值，计算更准确。

### 5. **优势对比**：
   - **PPO**：需要逐token估计状态价值（如“两边减3”的价值），在稀疏奖励下误差大，导致策略更新不稳定。例如，PPO可能错误估计“直接猜”的价值，影响$x = 2$的生成概率。
   - **GRPO**：通过组内相对优势，直接比较完整回答的优劣，跳过价值网络，优化更高效。例如，GRPO明确回答3最优，优先提升其生成概率。

### 6. **结果展示**：
   - 经过GRPO训练后，模型更倾向于生成清晰、完整的解题过程，如：
     - 输入：*"求解 $3x - 5 = 10$。"*
     - 输出：*"$3x - 5 = 10$，移项得$3x = 10 + 5$，计算得$3x = 15$，两边除以3，得$x = 5$。"*

---

## 七、总结

GRPO通过**组内相对比较**重构了优势计算和策略约束，彻底解决了PPO在大模型RLHF中的核心痛点：
- **无需价值网络**，直接利用群体奖励统计特性计算相对优势。
- **解耦CLIP范围**，平衡探索与利用，适应语言生成任务的多样性需求。
- **适配稀疏奖励**，直接处理序列级奖励，避免逐token估计的误差。

通过数学解题案例，我们看到GRPO不仅提高了回答的准确性，还优化了生成过程的清晰度和完整性，使其成为更适配语言生成任务的高效算法，为大模型的RLHF提供了更稳定、更灵活的优化方案。

