# 时序差分方法（TD）

时序差分（Temporal-Difference, TD）是**Model-Free强化学习的核心方法**，通过**单步/多步Bootstrap（用估计值更新估计值）实现高效在线学习**。**时序差分（TD）的核心**：无需完整 Episode，**单步 / 多步 Bootstrap 更新**（用估计的未来价值代替真实回报)

## 一、TD的核心突破：Bootstrap + 在线更新

### 0.review回顾

**动态规划（Dynamic Programming, DP）**

- **定义**：DP是一种基于模型（Model-Based）的优化方法，利用环境的完整模型（状态转移概率$p(s' \mid s, a)$和奖励函数$r(s, a, s')$）解决马尔可夫决策过程（MDP）问题。
- **核心思想**：通过贝尔曼方程的递归性质，将问题分解为子问题，逐步求解最优策略和价值函数。

**蒙特卡洛（Monte Carlo, MC）**

- **定义**：MC是一种Model-Free方法，通过**完整Episode采样**（从初始状态到终止状态的序列，如$S_0, A_0, R_1, \dots, S_T$）估计价值函数。
- **核心思想**：用实际交互的回报$G_t = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{T-1} R_T$的平均值近似状态价值$V(s)$或动作价值$Q(s, a)$： $$ V(s) \approx \frac{1}{N(s)} \sum_{k=1}^K G_k(s) $$

**RM算法（随机近似函数）解决均值估计问题的一个衍生例子：**

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250709200820656.png" alt="image-20250709200820656" style="zoom:80%;" />

### 1. TD与MC、DP的本质区别

| **方法** | 模型依赖       | 更新依据                                           | 学习时机               | 偏差-方差                                        |
| -------- | -------------- | -------------------------------------------------- | ---------------------- | ------------------------------------------------ |
| **MC**   | 无             | 完整Episode的真实回报$G_t$                         | Episode结束后          | 无偏，**高方差**                                 |
| **DP**   | 有（环境模型） | 贝尔曼方程（依赖相邻状态价值）                     | 同步更新所有状态       | 有偏，低方差                                     |
| **TD**   | 无             | **单步Bootstrap**（$R_{t+1} + \gamma V(S_{t+1})$） | **每步交互后**立即更新 | 有偏（因$V(S_{t+1})$是估计值），**方差远低于MC** |

### 2. 核心突破

- **Bootstrap**：利用当前估计值（如$V(S_{t+1})$）预测未来收益，避免等待完整回报。
- **在线更新**：每步交互后立即调整价值函数，适合实时学习场景。

## 二、TD算法分支详解

### 1. 状态价值的TD学习（TD learning of state values）

- **目标**：估计**策略$\pi$下的状态价值$V_\pi(s)$**。TD算法是在做 policy evaluation的事情，也就是在没有模型的时候求解贝尔曼公式。

![image-20250709202439098](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250709202439098.png)

- TD算法的核心公式（TD(0)）：

  $$ V(S_t) \leftarrow V(S_t) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right] $$

  - **TD target**：$R_{t+1} + \gamma V(S_{t+1})$（用下一个状态的估计价值预测当前状态的未来收益）。
    - 之所以叫TD target 是因为$V_t(s)$在不断的向这个值靠拢
  - **TD error**：$\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$（估计值与目标的差距）。
    - 当$V_t(s)$ = $V_\pi(s)$时，也就是估计值等于目标值的时候，$\delta_t$会等于0，所以这个TD error可以反映与目标值之间的误差。

- **步骤**：
  ① 智能体从状态$S_t$执行动作$A_t$，转移到$S_{t+1}$，获得奖励$R_{t+1}$；
  ② 立即用上述公式更新$V(S_t)$，无需等待Episode结束。

- **特点**：在线学习，计算高效，适合持续任务（非episodic）。

### 2. 动作价值的TD学习：Sarsa

之前的是传统意义上的TD算法，sarsa的一系列算法也可以认为是TD算法大类中的。其主要目的也是做policy evaluation的事情。只不过是对于action的。

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250709205344193.png" alt="image-20250709205344193" style="zoom:80%;" />

- **目标**：估计**策略$\pi$下的动作价值$Q_\pi(s, a)$**，采用**同策略（On-Policy）**：学习策略 = 行为策略。

![image-20250709205536974](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250709205536974.png)

- 核心公式：就是将TD算法中的state value 替换成action value

  $$ Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \right] $$

  - $A_{t+1}$是**当前策略$\pi$**在$S_{t+1}$处选择的动作（如ε-贪心策略的输出）。

- **名字来源**：更新依赖$S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$五个元素的首字母串联。

- **策略与探索**：通常搭配**ε-贪心策略**（以$\varepsilon$概率随机探索，$1-\varepsilon$概率利用当前最优动作），保证探索的同时逐步改进策略。

### 3. 动作价值的TD学习：期望Sarsa

- **改进动机**：Sarsa中$A_{t+1}$是**采样值**（如ε-贪心随机选的动作），会引入方差。期望Sarsa用**期望**代替采样，降低方差。

![image-20250709210642996](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250709210642996.png)

- 核心公式：

  $$ Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \sum_a \pi(a \mid S_{t+1}) Q(S_{t+1}, a) - Q(S_t, A_t) \right] $$

  - 对$S_{t+1}$的所有动作$a$，按策略$\pi$的概率$\pi(a \mid S_{t+1})$加权求和，得到**期望动作价值**。

- **特点**：比Sarsa更稳定（方差低），但计算量稍大（需遍历下一个状态的所有动作）。

### 4. 动作价值的TD学习：n步Sarsa

- **改进动机**：平衡**单步TD（偏差高，方差低）和MC（偏差低，方差高）**，用**n步回报**灵活调整偏差-方差权衡。

- n步回报定义：吧

  $$ G_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1} R_{t+n} + \gamma^n Q(S_{t+n}, A_{t+n}) $$

  - $A_{t+n}$是策略$\pi$在$S_{t+n}$处的动作（同策略）。

- **更新公式**：
  $$ Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ G_t^{(n)} - Q(S_t, A_t) \right] $$

- 特例：

  - $n=1$ → 普通Sarsa；
- $n \to \infty$ → 趋近于MC（需完整Episode，n等于Episode长度）。

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250709211340081.png" alt="image-20250709211340081" style="zoom:80%;" />

### 5. 最优动作价值的TD学习：Q-learning（重要）

- **目标**：直接学习**最优动作价值$Q^*(s, a)$**，采用**异策略（Off-Policy）**：学习策略（逼近最优）≠ 行为策略（负责探索）。
  - 之前的算法都是求解贝尔曼方程，但是Q-learning是在解决贝尔曼最优方程。
  - On-Policy（同策略）和Off-Policy（异策略）是强化学习两类核心方法，核心区别在于 **“生成数据的行为策略” 和 “学习优化的目标策略” 是否为同一策略**。
  - **On-Policy** 像 “亲身实践派”：边做边改，对当前策略的变化敏感，适合探索与利用需紧密配合的场景（如实时博弈）。
    - PPO、sarsa、MC  

  - **Off-Policy** 像 “经验借鉴派”：隔空学习，突破当前策略的局限，适合数据稀缺或需高效学习的场景（如机器人控制、游戏 AI）。
    - Q-Learning、DQN
  - off-policy相对于on-policy最大的优势就是可以将探索与最优性的矛盾相分离，并且可以更充分的使用采集到的数据进行学习。


<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250709213039093.png" alt="image-20250709213039093" style="zoom:80%;" />

- 核心公式：

  $$ Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A_t) \right] $$

  - TD目标是**下一个状态的最优动作价值**（$\max_a Q(S_{t+1}, a)$），而非当前策略的动作价值（与Sarsa的核心区别！）。

- 异策略的意义：

  - 更新 Q 值时直接瞄准 “最优动作”，无需关心当前探索的策略，学习的是超越当前行为的最优策略。
  - **行为策略（Behavior Policy）**：负责**大胆探索**（如随机策略、ε- 贪心），覆盖更多状态 - 动作对。
  - **目标策略（Target Policy）**：专注**收敛到最优**（如贪心策略），不受探索行为的直接干扰。
  - 这种解耦是异策略最核心的价值，直接突破了同策略（On-Policy）的 “探索 - 利用矛盾”。

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250709215202967.png" alt="image-20250709215202967" style="zoom:80%;" />

## 三、统一视角：TD算法的共性与差异

### 1. 更新公式的统一形式

所有TD算法均可表示为：
$$ \text{估计值} \leftarrow \text{估计值} + \alpha \times (\text{目标} - \text{估计值}) $$
差异仅在于**“目标”的定义**（取决于算法类型、策略、n步长度）：

| **算法**      | 目标（Target）                                          | 策略类型 | n步长度 |
| ------------- | ------------------------------------------------------- | -------- | ------- |
| TD(0)（状态） | $R_{t+1} + \gamma V(S_{t+1})$                           | -        | 1       |
| Sarsa         | $R_{t+1} + \gamma Q(S_{t+1}, A_{t+1})$                  | 同策略   | 1       |
| 期望Sarsa     | $R_{t+1} + \gamma \sum_a \pi(a |S_{t+1}) Q(S_{t+1}, a)$ | 同策略   | 1       |
| n步Sarsa      | $G_t^{(n)}$（n步回报）                                  | 同策略   | n       |
| Q-learning    | $R_{t+1} + \gamma \max_a Q(S_{t+1}, a)$                 | 异策略   | 1       |

### 2. 策略与探索的关系

- **同策略（Sarsa、期望Sarsa、n步Sarsa）**：学习过程中，策略逐渐改进，但始终“跟着当前策略走”，探索更**保守**（如ε-贪心的ε不能随便取消）。
- **异策略（Q-learning）**：学习策略直接瞄准最优，行为策略可自由探索（甚至固定为随机策略），探索更**激进**。

### 3. 偏差-方差权衡

- **单步TD（如Sarsa、Q-learning）**：偏差较高（因用估计值Bootstrap），但方差低（仅单步采样）。
- **n步TD（如n步Sarsa，n越大）**：偏差降低（更接近真实回报），但方差升高（接近MC）。
- **期望Sarsa vs Sarsa**：期望Sarsa用期望代替采样，**方差更低**，但计算成本更高。

## 四、TD方法的核心优势与适用场景

### 1. 优势

- **在线学习**：每步交互后立即更新，无需等待Episode结束，适合持续任务。
- **高效灵活**：通过调整n、策略类型（同/异策略）、探索方式，适应不同任务需求。
- **低方差**：比MC更稳定，尤其单步TD，适合噪声大的环境。

### 2. 适用场景

- 机器人控制（持续任务，需实时更新）；
- 游戏AI（如Atari，需平衡探索与利用）；
- 资源受限场景（TD计算量远低于MC和DP）。

## 五、补充：TD方法的扩展与实际应用

### 1. TD与函数近似

- **背景**：当状态空间巨大（如图像输入），表格存储$V(s)$或$Q(s, a)$不可行，需用函数近似（如神经网络）。
- **方法**：将TD公式中的$V(s)$或$Q(s, a)$替换为参数化形式（如$V_\theta(s)$），用梯度下降更新参数$\theta$：
  $$ \theta \leftarrow \theta + \alpha \delta_t \nabla_\theta V_\theta(S_t) $$
  或
  $$ \theta \leftarrow \theta + \alpha \delta_t \nabla_\theta Q_\theta(S_t, A_t) $$
- **应用**：深度Q网络（DQN）结合Q-learning与神经网络，成功应用于Atari游戏。

### 2. TD与多智能体环境

- **挑战**：传统TD假设环境静态，多个智能体交互可能导致非平稳性。
- **改进**：引入多智能体TD学习（如多智能体Q-learning），通过共享经验或通信协调策略。
- **应用**：如自动驾驶中的多车协作。

### 3. 收敛条件

- **假设**：TD算法收敛需满足Gliespie条件（步长$\alpha_t$满足$\sum \alpha_t = \infty$且$\sum \alpha_t^2 < \infty$），且探索策略保证状态-动作对充分访问。
- **实践**：ε-贪心或Boltzmann探索常用于满足这一条件。

## 六、总结：TD的演进逻辑

从**状态价值→动作价值**（更直接服务策略），从**单步→n步**（平衡偏差-方差），从**同策略→异策略**（释放探索自由度），TD方法通过**灵活定义更新目标和策略关系**，构建了一套覆盖“高效学习→最优收敛”的完整体系。

理解这些算法的**核心差异（目标、策略、n步）**，结合函数近似、多智能体扩展及收敛条件，就能选择最适配的TD方法应用于实际问题——这也是强化学习“Model-Free”分支的核心智慧。