# 策略梯度方法（on policy）

表格型表示与函数型表示的差异：

- 首先，如何定义最优策略？
  - 当策略以 **表格形式** 表示时，若策略 *π* 能最大化 **每个状态的价值**，则它是最优策略。
  - 当策略以 **函数形式** 表示时，若策略 *π* 能最大化某些 **整体标量指标**（目标函数$J(θ)$），则它是最优策略。
- 其次，如何获取一个动作的概率？
  - 在表格型的情况下，状态 *s* 下采取动作 *a* 的概率，可通过查询表格型策略 **直接获取**。
  - 在函数型表示的情况下，需要根据函数的结构和参数， **计算** 策略 *π*(*a*∣*s*,*θ*) 的值。
- 第三，如何更新策略？
  - 当策略以 **表格形式** 表示时，可通过 **直接修改表格中的条目** 来更新策略 *π*。
  - 当策略以 **参数化函数形式** 表示时，无法再通过这种方式更新策略 *π*。相反， 只能通过修改参数 $θ$ 来更新策略。

## 一、策略梯度的基本思想

强化学习的目标是让智能体学会**最优策略**，即在每个状态下选择动作，以最大化**长期累积奖励**。

**策略梯度（Policy Gradient, PG）**采用**直接优化策略**的思路：

- 将策略参数化，用神经网络表示策略 $\pi_\theta(a|s)$，其中 $\theta$ 为网络参数，输出动作 $a$ 在状态 $s$ 下的概率分布。
- 通过调整 $\theta$，最大化**期望累积奖励**，区别于基于价值函数推导策略的间接方法（如 DQN）。

### 适用场景的优势
- **连续动作空间**：如机器人关节的连续力矩控制，策略可直接输出连续动作的概率分布（如高斯分布）。
- **随机策略更优**：在需要探索的任务中（如游戏中随机走位避障），随机策略能覆盖更多可能性。

---

## 二、定义最优策略的指标

优化策略需定义**衡量策略好坏的指标**——**期望累积奖励 $J(\theta)$**，根据任务类型分为两种定义方式：

### 1. Episodic 任务（有终止状态，如游戏关卡）
以**期望总奖励reward**为指标：
$$
J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \sum_{t=0}^T \gamma^t r_t \mid s_0 \right]
$$
<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250710174148932.png" alt="image-20250710174148932" style="zoom:80%;" />

- $T$：任务终止的时间步；
- $\gamma \in [0,1]$：折扣因子，决定未来奖励的权重（越近的奖励越重要）；
- $r_t$：时间步 $t$ 的即时奖励。

### 2. 持续任务（无终止，如机器人持续导航）
以**所有状态的平均价值**为指标，引入**状态分布 $d_{\pi_\theta}(s)$**（策略 $\pi_\theta$ 下状态 $s$ 出现的概率）：
$$
J(\theta) = \sum_s d_{\pi_\theta}(s) \cdot V_{\pi_\theta}(s)
$$
其中，状态价值函数：
$$
V_{\pi_\theta}(s) = \mathbb{E}_{\pi_\theta} \left[ \sum_{t=0}^\infty \gamma^t r_t \mid s \right]
$$
表示状态 $s$ 下的期望累积奖励。

![image-20250710175128299](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250710175128299.png)

### 关于指标的理解：

- 所有这些指标 **都是策略π的函数**。
- 由于 **π** 由参数 **θ** 进行参数化，因此这些指标 **也是θ的函数**。
- 换句话说，**θ** 的不同取值，会生成不同的指标值。
- 因此，我们可以通过 **搜索 θ的最优值**，来最大化这些指标。

### 最优策略

**最优策略** $\pi^*$ 是使 $J(\theta)$ 最大的策略： 
$$
\pi^* = \arg\max_{\pi} J(\pi)
$$

---

## 三、指标的梯度（策略梯度推导）

优化 $J(\theta)$ 需计算其对参数 $\theta$ 的梯度 $\nabla_\theta J(\theta)$，核心是**策略梯度定理**。

### 推导核心逻辑
策略 $\pi_\theta$ 生成轨迹 $\tau = (s_0, a_0, r_1, s_1, a_1, \dots)$，累积奖励是轨迹的函数。利用**对数导数技巧**：
$$
\nabla_\theta \pi_\theta(a|s) = \pi_\theta(a|s) \cdot \nabla_\theta \log\pi_\theta(a|s)
$$
将梯度转化为**动作价值的期望**。

### 策略梯度公式（简化版）
$$
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log\pi_\theta(a_t|s_t) \cdot Q_{\pi_\theta}(s_t, a_t) \right]
$$
其中，$Q_{\pi_\theta}(s,a)$ 为**动作价值函数**，表示状态 $s$ 选择动作 $a$ 后的期望累积奖励。

### 直观理解
- $\nabla_\theta \log\pi_\theta(a|s)$：策略对参数的**敏感度**，反映参数变化如何影响动作 $a$ 的概率。
- $Q_{\pi_\theta}(s,a)$：动作 $a$ 的**价值**，表示该动作带来的长期奖励。
- 梯度方向：**增加高价值动作的概率，降低低价值动作的概率**，从而优化策略。

---

## 四、梯度上升算法：REINFORCE

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250710193441340.png" alt="image-20250710193441340" style="zoom:80%;" />

**REINFORCE** 是策略梯度的经典**蒙特卡洛**实现，通过**实际轨迹的累积奖励**估计 $Q_{\pi_\theta}(s,a)$，算法流程如下：

### 算法流程
1. **初始化**：随机设置策略参数 $\theta$。
2. **收集轨迹**：用当前策略 $\pi_\theta$ 与环境交互，生成完整轨迹 $\tau = (s_0, a_0, r_1, s_1, a_1, \dots, s_T)$（直到任务终止）。
3. **计算累积奖励**：对轨迹中每个时间步 $t$，计算**折扣累积奖励 $G_t$**（替代 $Q_{\pi_\theta}(s_t, a_t)$）：
   $$
   G_t = \sum_{k=t}^T \gamma^{k-t} r_k
   $$
4. **更新策略**：沿**梯度上升**方向调整 $\theta$（最大化 $J(\theta)$）：
   $$
   \theta \leftarrow \theta + \alpha \cdot \sum_{t=0}^T \nabla_\theta \log\pi_\theta(a_t|s_t) \cdot G_t
   $$
   （$\alpha$ 为学习率，可批量处理多条轨迹降低方差。）

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250710194242658.png" alt="image-20250710194242658" style="zoom:80%;" />

### 特点与改进
- **优点**：实现简单，纯采样学习（无需环境模型），适合复杂环境。
- **缺点**：**方差大**，不同轨迹的 $G_t$ 波动剧烈，导致参数更新不稳定。
- **改进方法**：引入**基线（Baseline）**（如减去状态价值 $V(s_t)$），大幅降低方差（因 $\mathbb{E}[\nabla_\theta \log\pi \cdot V(s)] = 0$）。

---

## 五、总结：策略梯度的核心与演进

1. **核心逻辑**：通过**策略梯度定理**将策略参数与动作价值关联，利用梯度上升优化策略。
2. **REINFORCE 的定位**：策略梯度的基础实现，验证了“采样+梯度更新”的可行性，但方差问题突出。是on policy的
3. **后续发展**：
   - **A2C/A3C**：引入**优势函数 $A(s,a) = Q(s,a) - V(s)$**代替 $G_t$，结合基线和并行训练，降低方差。
   - **TRPO/PPO**：加入**信任区域**约束（如 PPO 的 clip 机制），避免策略更新幅度过大，提升稳定性。

策略梯度的**直接优化**思路为处理**连续动作**和**随机策略场景**提供了高效方案，是强化学习从理论到工程落地的关键桥梁。后续算法（如 PPO）均基于策略梯度思想延伸发展。