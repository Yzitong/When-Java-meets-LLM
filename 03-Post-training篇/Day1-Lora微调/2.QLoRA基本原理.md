# QLoRA：高效量化大语言模型微调

B站指路：[QLoRA_02_算法原理_深入解析_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1fw4m1v7xu/?spm_id_from=333.337.search-card.all.click&vd_source=aa6afb9d0536d09ecdcb5d2c1fcf4c79)

## 论文详情

- **标题**：QLoRA: Efficient Finetuning of Quantized LLMs
- **论文编号**：2305.14314v1
- **研究团队**：华盛顿大学
- **GitHub 仓库**：https://github.com/artidoro/qlora

## 1. 问题背景

- **资源密集**：微调大型语言模型（如参数量为 $65B$ 的 LLaMA 模型）需要大量资源，通常需要超过 $780GB$ 的 GPU 内存。
- **现有量化方法的局限性**：当前的量化方法虽然可以减少内存占用，但主要适用于推理阶段，在训练阶段会失效。

## 2. QLoRA 方法

QLoRA（量化低秩适配）引入了创新技术，以较低的内存需求实现大型语言模型的高效微调。主要技术包括：

- **$4$ 位 NormalFloat (NF4)**：为正态分布数据设计的信息理论上最优的量化数据类型。
- **双重量化**：通过量化量化常数进一步节省内存。
- **分页优化器**：利用 NVIDIA 统一内存管理技术，避免在处理长序列的 mini-batch 时因梯度检查点导致的内存峰值。

![image-20250707220550945](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250707220550945.png)

## 3. QLoRA 优势

- **预训练模型量化**：将预训练模型量化为 $4$ 位精度，并添加一组可学习的低秩适配器权重（Low-rank Adapter weights）。
- **内存效率**：将微调 $65B$ 参数模型的平均内存需求从超过 $780GB$ 降低到低于 $48GB$，且不影响运行时性能或预测性能。
- **性能**：使用消费级 GPU 在约 $12$ 小时内或专业级 GPU 在约 $24$ 小时内达到与 ChatGPT 相当的 $99.3%$ 性能。

## 4. 量化基础

### 4.1 量化概述

量化是一种降低模型计算复杂度和存储需求的技术，通过将高精度表示（如 $32$ 位浮点数）转换为低精度表示（如 $8$ 位整数），同时尽量保持模型性能。量化包括：

- **权重量化**：将模型权重从高精度（如 $32$ 位浮点数）转换为低精度（如 $8$ 位整数）。
- **激活量化**：将模型每一层的激活值（即输出）从高精度转换为低精度。

**量化步骤**：

1. **选择量化方案**：确定使用对称量化或非对称量化。对称量化使用统一的缩放因子，非对称量化针对每个权重或激活值使用不同的缩放因子和零点。
2. **计算缩放因子和零点**：用于将浮点数映射到整数范围。
3. **应用量化公式**：将浮点数转换为整数表示。

### 4.2 非对称量化

非对称量化通过数据的实际最小值和最大值计算缩放因子和零点，将数据映射到整数范围（如 $8$ 位整数的 $[0, 255]$）。

**示例**：
给定一组浮点数权重：$w = [0.1, -0.2, 0.3, -0.4, 0.5]$

1. **确定范围**：

   - 最小值：$w_{\text{min}} = -0.4$
   - 最大值：$w_{\text{max}} = 0.5$

2. **计算缩放因子**：
   $\text{scale} = \frac{w_{\text{max}} - w_{\text{min}}}{2^8 - 1} = \frac{0.5 - (-0.4)}{255} = \frac{0.9}{255} \approx 0.00353$

3. **应用量化公式**：
   $w_{\text{int}} = \text{round}\left(\frac{w - w_{\text{min}}}{\text{scale}}\right)$

   - $w_{\text{int}}[0] = \text{round}\left(\frac{0.1 - (-0.4)}{0.00353}\right) = \text{round}(141.36) = 141$
   - $w_{\text{int}}[1] = \text{round}\left(\frac{-0.2 - (-0.4)}{0.00353}\right) = \text{round}(56.65) = 57$
   - $w_{\text{int}}[2] = \text{round}\left(\frac{0.3 - (-0.4)}{0.00353}\right) = \text{round}(197.73) = 198$
   - $w_{\text{int}}[3] = \text{round}\left(\frac{-0.4 - (-0.4)}{0.00353}\right) = \text{round}(0) = 0$
   - $w_{\text{int}}[4] = \text{round}\left(\frac{0.5 - (-0.4)}{0.00353}\right) = \text{round}(254.95) = 255$

   量化后的权重：$w_{\text{int}} = [141, 57, 198, 0, 255]$

4. **反量化**：

   $w_{\text{float}} = w_{\text{int}} \times \text{scale} + w_{\text{min}}$

   - $w_{\text{float}}[0] = 141 \times 0.00353 + (-0.4) \approx 0.09773$
   - $w_{\text{float}}[1] = 57 \times 0.00353 + (-0.4) \approx -0.198$
   - $w_{\text{float}}[2] = 198 \times 0.00353 + (-0.4) \approx 0.29894$
   - $w_{\text{float}}[3] = 0 \times 0.00353 + (-0.4) = -0.4$
   - $w_{\text{float}}[4] = 255 \times 0.00353 + (-0.4) \approx 0.49915$

   反量化值与原始值相比存在少量误差，但基本保留了数据的分布。

### 4.3 对称量化（QLoRA 使用的量化方法）

对称量化使用绝对值的最大值进行标准化，将数据映射到对称范围（如 $8$ 位整数的 $[-127, 127]$）。

**示例**：
给定张量：$X = [1.5, -3.0, 2.0, 4.5, -2.5, 1.0, 3.5, -4.0]$，块大小 $B = 4$。

1. **分块**：
   - $X_1 = [1.5, -3.0, 2.0, 4.5]$
   - $X_2 = [-2.5, 1.0, 3.5, -4.0]$
2. **计算量化常数**：
   - 对于 $X_1$：$\text{absmax}(X_1) = 4.5$，$c_1 = \frac{127}{4.5} \approx 28.222$
   - 对于 $X_2$：$\text{absmax}(X_2) = 4.0$，$c_2 = \frac{127}{4.0} = 31.75$
3. **应用量化**：
   $X_{\text{int8}} = \text{round}(c \cdot X_{\text{FP32}})$
   - 对于 $X_1$：$\text{round}(28.222 \cdot [1.5, -3.0, 2.0, 4.5]) = [42, -85, 56, 127]$
   - 对于 $X_2$：$\text{round}(31.75 \cdot [-2.5, 1.0, 3.5, -4.0]) = [-79, 32, 111, -127]$
4. **反量化**：
   $X_{\text{FP32}} = \frac{X_{\text{int8}}}{c}$
   - 对于 $X_1$：$\frac{[42, -85, 56, 127]}{28.222} \approx [1.488, -3.011, 1.985, 4.499]$
   - 对于 $X_2$：$\frac{[-79, 32, 111, -127]}{31.75} \approx [-2.488, 1.008, 3.496, -4.004]$

分块量化通过为每个块独立计算量化常数，提高了精度。

### 4.4 对称量化VS非对称量化

- 对称量化：
  - **过程**：使用绝对最大值标准化，映射到 $[-127, 127]$。
  - **公式**：$X_{\text{int8}} = \text{round}\left(\frac{127}{\text{absmax}(X_{\text{FP32}})} \cdot X_{\text{FP32}}\right)$
  - **特点**：假设数据分布对称，仅需一个缩放因子。
- 非对称量化：
  - **过程**：考虑实际最小值和最大值，映射到 $[0, 255]$。
  - **公式**：$w_{\text{int}} = \text{round}\left(\frac{w - w_{\text{min}}}{\text{scale}}\right)$
  - **特点**：适用于数据分布不对称的情况，使用缩放因子和零点。

### 4.5 分块量化

分块量化通过将张量划分为较小的块，分别进行量化，以提高精度。

**优势**：

- **提高精度**：较小的块具有较窄的数值范围，量化更精确。
- **减少误差**：每个块使用独立的量化常数，减少全局量化误差。
- **适应性**：适应数据分布不均匀的情况。

**使用场景**：

- **必要场景**：资源受限环境或对量化精度要求高的场景。
- **非必要场景**：小型模型或数据分布均匀时，全局量化可能足够。

## 5. QLoRA 微调

QLoRA 通过以下技术实现 $4$ 位微调：

- **$4$ 位 NormalFloat (NF4) 量化**：优化正态分布数据的量化。
- **双重量化**：量化量化常数以进一步节省内存。
- **分页优化器**：利用 NVIDIA 统一内存管理，避免梯度检查点导致的内存溢出。

### 5.1 $4$ 位 NormalFloat (NF4) 量化

- **理论基础**：基于分位数量化，确保每个量化区间包含相等数量的输入值，达到信息论上的最优。
- 步骤：
  1. 估算 $2^k + 1$ 个分位数以获得 $k$ 位量化数据类型。
  2. 将张量标准化到 $[-1, 1]$ 区间，并基于绝对最大值重缩放。
  3. 应用量化公式：$q_i = \frac{1}{2} \left( Q_X\left(\frac{i}{2^k + 1}\right) + Q_X\left(\frac{i+1}{2^k + 1}\right) \right)$，其中 $Q_X$ 是标准正态分布 $N(0,1)$ 的分位数函数。
- **优化**：确保离散零点，并使用所有 $2^k$ 位表示，移除冗余零点，称为 $k$ 位 NormalFloat (NFK)。

**示例**：参见 `QLoRA_Algorithm_Demo.ipynb`。

### 5.2 双重量化

**概念**：对第一次量化的量化常数进行二次量化，进一步节省内存。使用8-bit浮点数对初始量化的结果进行进一步压缩，这一步的目标是通过减少位宽来进一步减少存储空间，同时尽量保持数据的准确性。

**双重量化的具体流程**

双重量化通过两次量化进一步压缩存储开销，但计算时需逐步反量化：

（1）第一次量化

- 将原始 32 位浮点数权重量化为 4-bit NormalFloat（NF4），并保存量化常数（如缩放因子$c^{fp32}_2$），此时$c^{fp32}_2$以 32 位浮点数存储。

（2）第二次量化

- 将第一次量化的常数$c^{fp32}_2$再次量化为 8-bit 浮点数（$c^{fp8}_2$），并保存新的量化常数$c^{fp32}_2$（32 位浮点数）。
- 最终存储的量化常数包括：
  - 8-bit 浮点数$c^{fp8}_2$（用于反量化$c^{fp32}_2$）
  - 32 位浮点数$c^{fp32}_2$（用于反量化$c^{fp8}_2$）

（3）计算时的反量化

​	**第一步**：使用$c^{fp32}_2$将$c^{fp8}_2$反量化回 32 位浮点数$c^{fp32}_2$。

​	**第二步**：使用$c^{fp32}_2$将 4-bit NormalFloat 权重反量化为 32 位浮点数。

​	**第三步**：将 32 位浮点数转换为**BFloat16**（16 位）进行实际计算。

 **为什么反量化到 BFloat16 而非 32 位？**

- **精度与效率的平衡**：BFloat16 是专为 AI 计算设计的 16 位浮点数格式，其动态范围（-3.4e38 到 3.4e38）与 32 位浮点数接近，适合处理梯度计算和矩阵乘法，同时节省计算资源。
- **性能验证**：实验表明，QLoRA 使用 BFloat16 计算时，性能与全 16 位微调相当，甚至在 MMLU 基准测试中达到 ChatGPT 性能的 99.3%

**中心化**：从第一次量化常数中减去均值，使第二次量化值集中在零点附近。

**示例**：参见 `QLoRA_Algorithm_Demo.ipynb`。

### 5.3 分页优化器

- **功能**：利用 NVIDIA 统一内存特性，自动管理 CPU 和 GPU 之间的内存交换，特别是在 GPU 内存不足时，优化器状态会自动换出到 CPU RAM，必要时再换回。
- **优势**：避免因内存不足导致的计算错误或性能下降。

## 6. 数据类型

- **存储**：使用 $4$ 位精度压缩存储，减少内存占用。QLoRA是的NF4与双重量化都是针对存储时做的压缩，在计算时反量化为16精读计算
- **计算**：使用 BFLOAT16 进行矩阵乘法，平衡精度与计算效率。

