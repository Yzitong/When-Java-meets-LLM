# Adam 优化器原理

Adam（Adaptive Moment Estimation，自适应矩估计）是一种高效的梯度下降优化算法，结合了动量法和自适应学习率的优点，广泛应用于深度学习模型的优化。Adam 通过计算梯度的一阶动量（均值）和二阶动量（未中心化的方差），自适应地调整学习率，从而加速梯度下降的收敛。

## 核心原理

Adam 优化器的核心思想是通过以下步骤更新模型参数：

1. **计算梯度**：根据损失函数计算当前参数的梯度。
2. **更新动量**：利用梯度的一阶动量（均值）和二阶动量（方差）来平滑更新方向和幅度。
3. **偏差修正**：由于动量初始化为零，可能导致早期估计偏差，Adam 对此进行修正。
4. **参数更新**：结合学习率和动量信息更新参数。

Adam 的优点包括：

- 计算效率高，适合大规模数据和参数。
- 自适应学习率，减少手动调参需求。
- 对稀疏梯度和噪声具有较好的鲁棒性。

## Adam 优化器公式

以下是 Adam 优化器的核心公式，其中 $\theta$ 表示模型参数，$t$ 表示当前迭代步数，$\eta$ 表示学习率，$\nabla L(\theta_t)$ 表示损失函数对参数 $\theta_t$ 的梯度。

1. **初始化**：
   - 一阶动量（均值）：$m_0 = 0$
   - 二阶动量（方差）：$v_0 = 0$
   - 时间步：$t = 0$
2. **梯度计算**：
   - 计算当前梯度：$g_t = \nabla L(\theta_t)$
3. **更新一阶动量（指数移动平均）**：
   - $m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t$
   - 其中，$\beta_1$ 是控制一阶动量衰减的超参数，通常取 0.9。
4. **更新二阶动量（指数移动平均）**：
   - $v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2$
   - 其中，$\beta_2$ 是控制二阶动量衰减的超参数，通常取 0.999。
5. **偏差修正**：
   - 修正一阶动量：$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$
   - 修正二阶动量：$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$
6. **参数更新**：
   - $\theta_{t+1} = \theta_t - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$
   - 其中，$\epsilon$ 是一个小的常数（通常取 $10^{-8}$），用于防止除零。

## 参数解释

- $\eta$：学习率，控制参数更新的步长，通常取 0.001。
- $\beta_1$：一阶动量的衰减率，控制历史梯度的影响。
- $\beta_2$：二阶动量的衰减率，控制历史梯度平方的影响。
- $\epsilon$：防止除零的小常数，确保数值稳定性。

## 代码实现

```python
import numpy as np

class AdamOptimizer:
    """
    Adam 优化器的 Python 实现，用于深度学习模型的参数优化。
    结合一阶动量（均值）和二阶动量（方差）进行自适应学习率调整。
    """
    def __init__(self, params, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
        """
        初始化 Adam 优化器。
        
        参数:
            params: 模型参数（numpy 数组列表）
            learning_rate: 学习率，控制参数更新步长，默认为 0.001
            beta1: 一阶动量的衰减率，默认为 0.9
            beta2: 二阶动量的衰减率，默认为 0.999
            epsilon: 小常数，防止除零，默认为 1e-8
        """
        self.params = params
        self.learning_rate = learning_rate
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        # 初始化一阶动量（m）和二阶动量（v）为零
        self.m = [np.zeros_like(p) for p in params]
        self.v = [np.zeros_like(p) for p in params]
        self.t = 0  # 时间步初始化

    def step(self, gradients):
        """
        执行一步参数更新。
        
        参数:
            gradients: 当前梯度（与 params 形状相同的 numpy 数组列表）
        
        返回:
            更新后的参数
        """
        self.t += 1  # 增加时间步
        updated_params = []

        for i, (param, grad) in enumerate(zip(self.params, gradients)):
            # 更新一阶动量：m_t = beta1 * m_{t-1} + (1 - beta1) * g_t
            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad
            
            # 更新二阶动量：v_t = beta2 * v_{t-1} + (1 - beta2) * g_t^2
            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grad ** 2)
            
            # 偏差修正：m_hat = m_t / (1 - beta1^t)
            m_hat = self.m[i] / (1 - self.beta1 ** self.t)
            
            # 偏差修正：v_hat = v_t / (1 - beta2^t)
            v_hat = self.v[i] / (1 - self.beta2 ** self.t)
            
            # 参数更新：param = param - learning_rate * m_hat / (sqrt(v_hat) + epsilon)
            param_update = param - self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)
            updated_params.append(param_update)
        
        self.params = updated_params
        return self.params

# 示例用法
if __name__ == "__main__":
    # 模拟模型参数和梯度
    params = [np.array([1.0, 2.0]), np.array([[0.5, -0.5], [1.5, -1.5]])]
    gradients = [np.array([0.1, -0.2]), np.array([[0.3, 0.1], [-0.4, 0.2]])]
    
    # 初始化 Adam 优化器
    optimizer = AdamOptimizer(params, learning_rate=0.01)
    
    # 打印初始参数
    print("初始参数:", params)
    
    # 执行一步优化
    updated_params = optimizer.step(gradients)
    
    # 打印更新后的参数
    print("更新后的参数:", updated_params)
```

