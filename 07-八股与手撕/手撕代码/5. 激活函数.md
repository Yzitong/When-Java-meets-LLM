```python
import numpy as np

# ReLU (Rectified Linear Unit) 函数
def relu(x):
    """
    ReLU 函数: f(x) = max(0, x)
    输入:
        x: 输入值，可以是标量或 numpy 数组
    输出:
        应用 ReLU 后的结果，负值置为 0，正值保持不变
    解释:
        ReLU 是一种非线性激活函数，广泛用于深度学习。
        它将输入中小于 0 的值置为 0，大于 0 的值保持不变。
        优点: 计算简单，加速梯度下降收敛，避免梯度消失问题。
        缺点: 对于负输入，梯度为 0，可能导致神经元“死亡”。
    """
    return np.maximum(0, x)

# Sigmoid 函数
def sigmoid(x):
    """
    Sigmoid 函数: f(x) = 1 / (1 + e^(-x))
    输入:
        x: 输入值，可以是标量或 numpy 数组
    输出:
        压缩到 (0, 1) 区间的值
    解释:
        Sigmoid 将任意实数映射到 (0, 1) 区间，常用于二分类问题。
        优点: 输出可解释为概率，平滑且可导。
        缺点: 容易导致梯度消失问题，特别是在深层网络中。
    """
    return 1 / (1 + np.exp(-x))

# Softmax 函数
def softmax(x):
    """
    Softmax 函数: f(x_i) = e^(x_i) / sum(e^(x_j))
    输入:
        x: 输入向量，通常是一维 numpy 数组
    输出:
        概率分布，输出向量元素和为 1
    解释:
        Softmax 将输入向量转换为概率分布，常用于多分类问题。
        它通过指数化输入并归一化，确保输出值在 (0, 1) 且总和为 1。
        优点: 输出可解释为类别概率，适合多分类任务。
        缺点: 对输入值敏感，可能导致数值不稳定（通过减去最大值改善）。
    """
    # 减去最大值以防止数值溢出
    exp_x = np.exp(x - np.max(x))
    return exp_x / np.sum(exp_x)

# 示例用法
if __name__ == "__main__":
    # 测试输入
    x = np.array([-1.0, 0.0, 1.0, 2.0])
    
    # 测试 ReLU
    print("ReLU 输入:", x)
    print("ReLU 输出:", relu(x))
    
    # 测试 Sigmoid
    print("\nSigmoid 输入:", x)
    print("Sigmoid 输出:", sigmoid(x))
    
    # 测试 Softmax
    print("\nSoftmax 输入:", x)
    print("Softmax 输出:", softmax(x))
    print("Softmax 输出和:", np.sum(softmax(x)))
```

