# 贝尔曼公式

以下基于马尔可夫决策过程（MDP）框架，讲解强化学习中的**状态价值**、**贝尔曼方程的推导**、**矩阵-向量形式**、**求解状态价值**以及**动作价值**，并结合总结。

## 1. 状态价值（State Value Function）

- **定义**：就是回报的期望值。在策略$\pi$下，状态价值函数$V_\pi(s)$表示从状态$s$开始，遵循策略$\pi$的**期望**累积折扣奖励：
  $$V_\pi(s) = \mathbb{E}*\pi\left[\sum*{t=0}^\infty \gamma^t R_{t+1} \mid S_t = s\right],$$
  其中$\gamma \in [0, 1)$是折扣因子，$R_{t+1}$是时刻$t+1$的奖励。

![image-20250708141436151](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250708141436151.png)

- **意义**：$V_\pi(s)$衡量在状态$s$下策略$\pi$的长期收益，评估策略优劣。显然一个状态的状态价值高，说明我们才是值得向这个状态走的。
- **与return回报的关系：**当从一个状态出发的策略是唯一的，那回报就是状态价值；但是一般从一个状态出发其策略是不唯一的，应该是一个概率分布，所以我们就是这多个策略回报的期望作为状态价值，从而衡量策略的好坏。
- **Grid-World示例**：在$4 \times 4$网格中，目标$(3,3)$奖励$+1$，其他移动奖励$-0.1$，$\gamma = 0.9$。若策略$\pi$总选择“右”，$V_\pi((2,3))$接近$1$（因可一步到达目标）。

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250708142026395.png" alt="image-20250708142026395" style="zoom:80%;" />

## 2. 贝尔曼方程：推导

- **目标**：将$V_\pi(s)$分解为即时奖励和后续状态价值的递归关系。
- 推导：
  1. 根据定义，$V_\pi(s) = \mathbb{E}*\pi\left[R*{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \mid S_t = s\right]$。 
  2. 分解第一步：
     $$V_\pi(s) = \mathbb{E}*\pi\left[R*{t+1} + \gamma \sum_{t=1}^\infty \gamma^{t-1} R_{t+2} \mid S_t = s\right].$$
  3. 注意到后续部分$\sum_{t=1}^\infty \gamma^{t-1} R_{t+2} = \sum_{k=0}^\infty \gamma^k R_{k+2}$，对应$V_\pi(S_{t+1})$。
  4. 引入动作$a \sim \pi(a|s)$和状态转移$s' \sim P(s'|s, a)$，得：
     $$V_\pi(s) = \mathbb{E}*\pi\left[R*{t+1} + \gamma V_\pi(S_{t+1}) \mid S_t = s\right].$$
  5. 展开期望，考虑动作和转移概率：
     $$V_\pi(s) = \sum_{a \in A} \pi(a|s) \sum_{s' \in S} P(s'|s, a) \left[ R(s, a, s') + \gamma V_\pi(s') \right].$$
- **结果**：**贝尔曼期望方程**：
  $$V_\pi(s) = \sum_{a \in A} \pi(a|s) \sum_{s' \in S} P(s'|s, a) \left[ R(s, a, s') + \gamma V_\pi(s') \right].$$
- **Grid-World示例**：在$(2,3)$，若$\pi(\text{右}|(2,3)) = 1$，$P((3,3)|(2,3), \text{右}) = 1$，$R((2,3), \text{右}, (3,3)) = 1$，则：
  $$V_\pi((2,3)) = 1 \cdot 1 \cdot [1 + \gamma \cdot V_\pi((3,3))].$$
- 求解贝尔曼公式 = 对策略进行评估

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250708142814446.png" alt="image-20250708142814446" style="zoom:80%;" />

## 3. 贝尔曼方程：矩阵-向量形式

- **前提**：假设状态空间$S$有限（$|S| = n$），将$V_\pi(s)$组织为向量$\mathbf{v}_\pi \in \mathbb{R}^n$，每个分量对应一个状态的价值。

- 矩阵形式：

  1. 定义奖励向量$\mathbf{r}*\pi$，其第$s$个分量为：
     $$r*\pi(s) = \sum_{a \in A} \pi(a|s) \sum_{s' \in S} P(s'|s, a) R(s, a, s').$$
  2. 定义状态转移矩阵$\mathbf{P}*\pi \in \mathbb{R}^{n \times n}$，其$(s, s')$元素为：
     $$P*\pi(s, s') = \sum_{a \in A} \pi(a|s) P(s'|s, a).$$
  3. 贝尔曼方程变为：
     $$\mathbf{v}*\pi = \mathbf{r}*\pi + \gamma \mathbf{P}*\pi \mathbf{v}*\pi.$$

  <img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250708182309563.png" alt="image-20250708182309563" style="zoom:80%;" />

- 推导：

  - **状态价值向量 $v_\pi$**
    - **定义**：每个元素$v_\pi(s_i)$表示“在策略$\pi$下，处于状态$s_i$的期望累积折扣回报”。
    - **形式**：列向量，$v_\pi = [v_\pi(s_1), v_\pi(s_2), \dots, v_\pi(s_n)]^T \in \mathbb{R}^n$。
  - **状态奖励向量 $r_\pi$**
    - **定义**：每个元素$r_\pi(s_i)$表示“在策略$\pi$下，处于状态$s_i$时的即时奖励期望”（即从$s_i$出发，按$\pi$选择动作，立即获得的奖励均值）。
    - **形式**：列向量，$r_\pi = [r_\pi(s_1), r_\pi(s_2), \dots, r_\pi(s_n)]^T \in \mathbb{R}^n$。
  - **状态转移矩阵 $P_\pi$**
    - **定义**：元素$[P_\pi]*{ij} = p*\pi(s_j \mid s_i)$表示“在策略$\pi$下，从状态$s_i$转移到$s_j$的概率”。
    - **形式**：$n \times n$矩阵，$P_\pi \in \mathbb{R}^{n \times n}$。

- **Grid-World示例**：$4 \times 4$网格（14个非障碍状态），$\mathbf{v}*\pi$是14维向量，$\mathbf{P}*\pi$是$14 \times 14$矩阵，$\mathbf{r}_\pi$包含每个状态的期望奖励。

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250708182401475.png" alt="image-20250708182401475" style="zoom:80%;" />

## 4. 贝尔曼方程：求解状态价值

- 方法：矩阵形式的方程$\mathbf{v}\pi = \mathbf{r}\pi + \gamma \mathbf{P}\pi \mathbf{v}\pi$是一个线性系统，可通过以下方式求解：
  1. 直接求解：
     - 整理为：$(\mathbf{I} - \gamma \mathbf{P}*\pi) \mathbf{v}*\pi = \mathbf{r}_\pi$。
     - 解线性方程：$\mathbf{v}*\pi = (\mathbf{I} - \gamma \mathbf{P}*\pi)^{-1} \mathbf{r}_\pi$。
     - 适用小规模状态空间，矩阵求逆复杂度为$O(n^3)$。实际中用的较少，求逆矩阵较为复杂。
  2. 迭代法（如策略评估）：
     - 初始化$V_\pi(s) = 0$，迭代更新：
       $$V_\pi^{k+1}(s) = \sum_{a \in A} \pi(a|s) \sum_{s' \in S} P(s'|s, a) \left[ R(s, a, s') + \gamma V_\pi^k(s') \right].$$
     - 无限次迭代后，收敛到$V_\pi(s)$，因$\gamma < 1$保证收缩映射。
- **Grid-World示例**：迭代更新$(2,3)$的价值，初始$V_\pi^0((2,3)) = 0$，第一步：
  $$V_\pi^1((2,3)) = 1 \cdot 1 \cdot [1 + \gamma \cdot 0] = 1.$$
  多次迭代后收敛至真实价值。

## 5. 动作价值（Action Value Function）

- **定义**：动作价值函数$Q_\pi(s, a)$表示在状态$s$执行动作$a$，随后按策略$\pi$行动的期望累积奖励：
  							![image-20250708185912490](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250708185912490.png)

![image-20250709111407172](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250709111407172.png)

- 关注动作价值的原因：判断那个动作可以给我们更高的价值，从而决定动作的选择。
- **公式**：$$Q(s, a) = \sum_{s' \in S} p(s' \mid s, a) \cdot \left[ r(s, a, s') + \gamma V(s') \right].$$
  - 公式解释：
    - **即时奖励期望**：$\sum_{s' \in S} p(s' \mid s, a) \cdot r(s, a, s')$，表示执行动作$a$后获得的即时奖励均值。
    - **后续状态价值期望**：$\gamma \sum_{s' \in S} p(s' \mid s, a) \cdot V(s')$，表示下一状态的折扣价值期望。
  - Grid-World示例：
    - 在状态$s$执行动作$a$，以$0.8$概率转移到$s_1$（奖励$+1$），以$0.2$概率转移到$s_2$（奖励$-1$），已知$V(s_1) = 5$，$V(s_2) = 2$，$\gamma = 0.9$。
    - 计算：
      $$Q(s, a) = 0.8 \cdot (1 + 0.9 \cdot 5) + 0.2 \cdot (-1 + 0.9 \cdot 2) = 0.8 \cdot (1 + 4.5) + 0.2 \cdot (-1 + 1.8) = 0.8 \cdot 5.5 + 0.2 \cdot 0.8 = 4.4 + 0.16 = 4.56.$$
- **动作价值与状态价值的关系**：$$Q(s, a) = r(s, a) + \gamma V(s').$$
  - Grid-World示例：
    - 确定性环境：从$s = (2,3)$执行“右”到$s' = (3,3)$，$r((2,3), \text{右}) = 1$，$V((3,3)) = 0$（目标状态无后续奖励），$\gamma = 0.9$，则：
      $$Q((2,3), \text{右}) = 1 + 0.9 \cdot 0 = 1.$$
    - 随机性环境：同方法一示例，$Q(s, a) = 4.56$。

- **直观理解：**

  - **状态价值$V(s)$**：从状态$s$出发，遵循策略$\pi$的**长期价值期望**，即：
    $$V_\pi(s) = \sum_{a \in A} \pi(a|s) Q_\pi(s, a).$$

  - **动作价值$Q(s, a)$**：从状态$s$出发，**强制执行动作$a$**，随后按策略$\pi$行动的**长期价值期望**，即“即时奖励”加“后续状态价值”。

  - **关系**：$Q(s, a)$分解了$V(s)$，聚焦于特定动作$a$的影响。

  - **Grid-World示例**：在$(2,3)$，$V_\pi((2,3))$是策略$\pi$下所有动作的期望，而$Q_\pi((2,3), \text{右})$仅考虑“右”动作的回报。

## 6. 总结

- **核心思想**：贝尔曼方程通过递归分解，将$V_\pi(s)$和$Q_\pi(s, a)$表达为即时奖励和后续价值的期望，基于MDP的$(S, A, P, R, \gamma)$。
- **矩阵形式**：将$V_\pi(s)$组织为向量，表达为线性系统，便于理论分析和求解。
- **求解方法**：直接求逆适合小规模问题，迭代法（如策略评估）更通用。
- **动作价值**：$Q_\pi(s, a)$扩展了状态价值，直接评估动作效果，支撑如Q-Learning的算法。
- **意义**：贝尔曼方程是强化学习的数学基石，连接理论（MDP）与算法（策略迭代、价值迭代），为后续Model-Free方法（如TD学习）提供基础。

# 贝尔曼最优公式

以下基于马尔可夫决策过程（MDP）框架，讲解强化学习中的贝尔曼最优方程（Bellman Optimality Equation, BOE）。

## 1. 最优策略的定义

- **定义**：在MDP $(S, A, P, R, \gamma)$中，最优策略$\pi^*$是使所有状态$s \in S$的状态价值函数$V_{\pi^*}(s)$达到最大的策略，即：
  $$V_{\pi^*}(s) = \max_{\pi} V_\pi(s), \quad \forall s \in S,$$
  其中$V_\pi(s) = \mathbb{E}*\pi\left[\sum*{t=0}^\infty \gamma^t R_{t+1} \mid S_t = s\right]$是策略$\pi$下状态$s$的期望累积折扣奖励，$\gamma \in [0, 1)$为折扣因子。
- **动作价值视角**：最优策略$\pi^*$也满足：
  $$Q_{\pi^*}(s, a) = \max_{\pi} Q_\pi(s, a), \quad \forall s \in S, a \in A,$$
  其中$Q_\pi(s, a) = \mathbb{E}*\pi\left[\sum*{t=0}^\infty \gamma^t R_{t+1} \mid S_t = s, A_t = a\right]$。
- **意义**：最优策略$\pi^*$在每个状态选择能最大化长期回报的动作，通常是确定性策略（即$\pi^*(a|s) \in {0, 1}$）。
- **Grid-World示例**：在$4 \times 4$网格中，目标$(3,3)$奖励$+1$，其他移动奖励$-0.1$，$\pi^*$可能在$(2,3)$选择“右”，直接到达$(3,3)$以最大化回报。

## 2. 贝尔曼最优方程（BOE）：引言

- **背景**：贝尔曼期望方程描述策略$\pi$的价值函数：
  $$V_\pi(s) = \sum_{a \in A} \pi(a|s) \sum_{s' \in S} P(s'|s, a) \left[ R(s, a, s') + \gamma V_\pi(s') \right].$$
  BOE则针对**最优价值函数**$V^*(s) = \max_\pi V_\pi(s)$，定义其递归关系。

![image-20250708195338204](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250708195338204.png)

- **目的**：BOE提供最优价值函数的数学表达，为求解$\pi^*$和$V^*$提供理论基础。
- **形式**：BOE假设最优策略存在，基于MDP的动态规划性质推导。

##    3. 右侧的最大化操作

- **核心思想**：最优价值函数$V^*(s)$通过在状态$s$选择最优动作$a$实现，动作$a$带来最大的期望回报。
- 推导：
  1. 由$V^*(s) = \max_\pi V_\pi(s)$，考虑$V_\pi(s) = \sum_{a \in A} \pi(a|s) Q_\pi(s, a)$。
  2. 最优策略选择最大化$Q^*(s, a) = \sum_{s' \in S} P(s'|s, a) \left[ R(s, a, s') + \gamma V^*(s') \right]$的动作。
  3. 因此，$V^*(s)$取所有动作的最大值：
     $$V^*(s) = \max_{a \in A} \sum_{s' \in S} P(s'|s, a) \left[ R(s, a, s') + \gamma V^*(s') \right].$$
- **动作价值形式**：
  $$Q^*(s, a) = \sum_{s' \in S} P(s'|s, a) \left[ R(s, a, s') + \gamma V^*(s') \right],$$
  $$V^*(s) = \max_{a \in A} Q^*(s, a).$$
- **Grid-World示例**：在$(2,3)$，动作“右”到$(3,3)$，$Q^*((2,3), \text{右}) \approx 1$；“上”可能为$-0.1$，$V^*((2,3)) = \max{1, -0.1, \dots} \approx 1$。
- 如下图的推导过程，可以感知到，最优化的过程，其实就是寻找最大动作价值的过程。

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250708195946061.png" alt="image-20250708195946061" style="zoom:80%;" />

## 4. 贝尔曼最优方程（BOE）：重写为$v = f(v)$

- **向量形式**：假设状态空间有限（$|S| = n$），定义最优价值向量$\mathbf{v}^* = [V^*(s_1), V^*(s_2), \dots, V^*(s_n)]^T \in \mathbb{R}^n$。
- 贝尔曼最优方程$V^*(s) = \max_{a \in A} \sum_{s' \in S} P(s' \mid s, a) [R(s, a, s') + \gamma V^*(s')]$可以看作$x = f(x)$的形式
- **BOE形式**：最优价值函数满足固定点方程：$$\mathbf{v}^* = f(\mathbf{v}^*).$$
- **意义**：BOE中的$f$是满足压缩映射定理的，所以最优价值$\mathbf{v}^*$一定是其不动点, 压缩映射定理保证价值迭代收敛到唯一的最优价值函数$V^*$，因为$\gamma < 1$满足压缩条件。
- **Grid-World示例**：$f(\mathbf{v})$计算每个格子的最大期望回报，迭代应用$f$使$\mathbf{v}$趋向$\mathbf{v}^*$。例如，初始$V^0(s) = 0$，通过迭代更新收敛到$V^*(s)$，速度因$\gamma$（如0.9）而呈指数级快。

## 5. 压缩映射定理（Contraction Mapping Theorem）

- **定义**：如果$f$是一个压缩映射，即存在一个常数$L \in [0, 1)$，使得对于任意$x_1, x_2 \in X$，满足：
  $$|f(\mathbf{v}_1) - f(\mathbf{v}_2)|\leq L |\mathbf{v}_1 - \mathbf{v}_2|$$。
- 对于任何具有形式$x = f(x)$的方程，如果映射$f$是一个压缩映射（contraction mapping），则以下性质成立：
  - **存在性**：存在一个固定点$x^*$，满足$f(x^*) = x^*$。
    - **解释**：压缩映射保证方程$x = f(x)$至少有一个解，即某个$x^*$使得应用$f$后返回自身。
  - **唯一性**：固定点$x^*$是唯一的。
    - **解释**：由于$L < 1$，$f$的压缩性质确保不存在多个不同的固定点，否则会违背距离收缩的定义。
  - **算法**：考虑序列${x_k}$，其中$x_{k+1} = f(x_k)$，则$x_k \to x^*$当$k \to \infty$，且收敛速度是**指数级快**。
    - **解释**：通过迭代应用$f$，初始值$x_0$会逐渐趋近唯一固定点$x^*$，且收敛速度随着迭代次数增加呈指数级加快。
- **意义**：压缩映射定理保证迭代法（如价值迭代）收敛到最优价值函数$V^*$。

## 6. 贝尔曼最优方程（BOE）：求解

- 方法1：价值迭代：
  - 初始化$\mathbf{v}^0$（如全0），迭代：
    $$\mathbf{v}^{k+1}(s) = \max_{a \in A} \sum_{s' \in S} P(s'|s, a) \left[ R(s, a, s') + \gamma \mathbf{v}^k(s') \right].$$
  - 根据压缩映射定理，$\mathbf{v}^k \to \mathbf{v}^*$。
  - 复杂度：每轮迭代$O(|S|^2 |A|)$，适合小规模MDP。
- 方法2：策略迭代：
  1. 初始化策略$\pi_0$。
  2. 策略评估：解$V_{\pi_k}(s) = \sum_{a \in A} \pi_k(a|s) \sum_{s' \in S} P(s'|s, a) [ R(s, a, s') + \gamma V_{\pi_k}(s') ]$。
  3. 策略改进：$\pi_{k+1}(s) = \arg\max_{a \in A} \sum_{s' \in S} P(s'|s, a) [ R(s, a, s') + \gamma V_{\pi_k}(s') ]$。
  4. 重复直到$\pi_{k+1} = \pi_k$，此时$V_{\pi_k} = V^*$。
- **Grid-World示例**：价值迭代更新$(2,3)$的价值，$\mathbf{v}^k((2,3)) = \max{1 + \gamma \cdot 0, -0.1 + \gamma \cdot \mathbf{v}^{k-1}((1,3)), \dots}$，逐步收敛。

## 7. 贝尔曼最优方程（BOE）：最优性

- 最优性证明：
  1. BOE定义$V^*(s) = \max_{a \in A} \sum_{s' \in S} P(s'|s, a) [ R(s, a, s') + \gamma V^*(s') ]$，满足该方程的$V^*$是所有策略价值的上界。
  2. 存在策略$\pi^*$，其动作选择满足$\pi^*(s) = \arg\max_{a \in A} Q^*(s, a)$，使得$V_{\pi^*}(s) = V^*(s)$。
  3. 压缩映射定理保证$V^*$唯一，$\pi^*$是最优策略。
- **意义**：BOE刻画了最优价值函数的自洽性，确保存在最优策略，且可通过迭代求解。

## 8. 分析最优策略

- 性质：
  - **确定性**：最优策略$\pi^*$通常是确定性的，即$\pi^*(a|s) \in {0, 1}$，选择$Q^*(s, a)$最大的动作。
  - **唯一性**：若$Q^*(s, a)$在每个状态有唯一最大动作，$\pi^*$唯一；否则可能存在多个等价最优策略。
- **提取方法**：由$V^*$或$Q^*$导出：
  $$\pi^*(s) = \arg\max_{a \in A} \sum_{s' \in S} P(s'|s, a) \left[ R(s, a, s') + \gamma V^*(s') \right].$$
- **Grid-World示例**：在$(2,3)$，$Q^*((2,3), \text{右}) \approx 1$最大，$\pi^*((2,3)) = \text{右}$，形成到$(3,3)$的最短路径。
- **实际应用**：最优策略指导智能体在复杂环境（如游戏、机器人控制）中决策，BOE是动态规划和Q-Learning等算法的理论基础。
- 在我们设置reward时，影响最后结果的是各个不同种类reward之间的相对值，而不是绝对值。所以如果等比例的线性缩放各个种类的reward是不会影响最后的最优策略的。
- 下图说明，除了设置reward值可以让智能体不要“绕远路”外，但reward即使为0也会让智能体学会不要“绕远路”，因为折扣因子也会在绕远路的时候打折打的更狠。

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250708203851944.png" alt="image-20250708203851944" style="zoom:80%;" />

## 9. 总结 

- **核心**：贝尔曼最优方程$V^*(s) = \max_{a \in A} \sum_{s' \in S} P(s'|s, a) [ R(s, a, s') + \gamma V^*(s') ]$定义最优价值函数，通过最大化操作刻画最优策略。
- **形式**：向量形式$\mathbf{v}^* = f(\mathbf{v}^*)$，$f$为压缩映射，保证收敛性。
- **求解**：价值迭代和策略迭代基于BOE求解$V^*$和$\pi^*$，适用于Model-Based环境。
- **意义**：BOE连接理论与算法，奠定动态规划基础，支撑后续Model-Free方法（如Q-Learning），是强化学习的核心工具。