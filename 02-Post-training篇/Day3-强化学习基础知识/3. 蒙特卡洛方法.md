# 蒙特卡洛方法

之前的MBRL的方法的前提假设都是事件的概率分布已知，但实际生活中显然我们是不能保证知道准确的概率分布的，这种时候怎么办呢？本节介绍**蒙特卡洛（Monte Carlo, MC）强化学习**（基于完整轨迹的采样学习），核心思想是通过大量的实验去估计真实的情况，解决 **“探索 - 利用” 平衡** 和 **数据效率** 问题：

- **MC Basic**：最基础的蒙特卡洛算法，通过采样完整轨迹估计状态 / 动作价值，但未显式处理 “探索不足” 问题。
- **MC Exploring Starts**：通过**随机初始化每个轨迹的起始状态和动作**，强制覆盖所有状态 - 动作对，保证探索完全性。
- **MC \*ε\*-Greedy**：放弃 “探索性起始” 假设，改用 ***ε\*- 贪心策略**（以*ε*概率随机选动作，1−*ε*概率选当前最优动作），更贴近实际场景。

## 一、蒙特卡洛的核心思想

### 1. 什么是“Episode”？

一个**从初始状态到终止状态的完整序列**，如：
$$ S_0, A_0, R_1, S_1, A_1, R_2, \dots, S_T \quad (\text{终止状态为} , S_T) $$

Episode的长度可以代表距离目标的距离，所以在实际中Episode的距离设定是一个有意思的问题，可以直观的感觉到，如果任务越复杂那我们设定的Episode的长度应该长一点。

### 2. 价值估计的本质

对**状态s或状态-动作对$(s, a)$**，用其在**所有Episode中获得的回报（Return）的平均值**估计价值：
$$ V(s) \approx \frac{1}{N(s)} \sum_{k=1}^K G_k(s) \quad (G_k(s) \text{是第} k \text{个Episode中状态} s \text{的回报}) $$

## 二、三类蒙特卡洛算法详解

### 1. MC Basic（基础蒙特卡洛）

#### ▶ 核心逻辑：**朴素采样，平均回报**

就是通过将策略迭代的算法中model based 变成model free的，通过经验（大数据）利用平均值估计动作价值：
			<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250709112018563.png" alt="image-20250709112018563" style="zoom:80%;" />		 

- **步骤**：
  ① **初始化**：所有状态/动作价值设为0，访问计数$N(s)$或$N(s, a)$设为0。
  ② **生成Episode**：智能体按当前策略与环境交互，记录完整轨迹。
  ③ **计算回报**：对轨迹中每个时间步$t$，计算折扣回报$G_t = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{T-1} R_T$。
  ④ **更新价值**：对每个$(s_t, a_t)$，更新动作价值$Q(s_t, a_t)$：
     $$ Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \frac{1}{N(s_t, a_t) + 1} \left( G_t - Q(s_t, a_t) \right) $$
     （状态价值$V(s)$可通过$V(s) = \sum_a \pi(a|s) Q(s, a)$推导。）

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250709151400295.png" alt="image-20250709151400295" style="zoom:80%;" />

- **问题**：
  若初始策略“偏爱”某些动作（如一直选看起来好的动作），会导致**某些状态-动作对永远未被访问（探索不足）**，无法学到最优策略。

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250709150947414.png" alt="image-20250709150947414" style="zoom:80%;" />

### 2. MC Exploring Starts（探索性起始的蒙特卡洛）

#### ▶ 核心逻辑：**强制覆盖所有状态-动作对**

- **解决的问题**：MC Basic的“探索不足”，即有的action可能没有探索到。
- **关键改进**：
  每次生成Episode时，**随机选择起始状态$s_0$和起始动作$a_0$**（均匀遍历所有可能的$(s_0, a_0)$），确保所有状态-动作对都有机会被访问。
- 优点与局限：
  - **优点**：理论上可**收敛到最优策略**（因为所有$(s, a)$都被探索）。
  - **局限**：实际中**依赖环境支持随机起始**（如游戏无法随意设定初始状态/动作），实用性极低。

### 3. MC ε-Greedy（ε-贪心的蒙特卡洛）

#### ▶ 核心逻辑：**策略自探索，平衡“探索-利用”**

- **解决的问题**：MC Exploring Starts的“环境依赖”，用**策略本身实现探索**。

- **关键机制：ε-贪心策略**：
  在状态$s$，以**ε概率随机选动作（探索）**，以**$1-\varepsilon$概率选当前最优动作（利用）**：
  $$
  \pi(a|s) =
  \begin{cases}
  1 - \varepsilon + \varepsilon/|A(s)| & \text{若} , a = \arg\max_a Q(s, a) \
  \varepsilon/|A(s)| & \text{其他动作}
  \end{cases}
  $$

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250709153607224.png" alt="image-20250709153607224" style="zoom:80%;" />

- 使用这样的贪心选择方法，可以平衡探索性与效率。当ε越大探索性越强，越小效率越高。在实际中，我们通常会使ε越大变小，最后变到0的时候，所得到的策略也就是最优策略。

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250709155325835.png" alt="image-20250709155325835" style="zoom:80%;" />

- **算法步骤**：
  ① **初始化**：$Q(s, a)=0$，$N(s, a)=0$，初始策略为随机策略（或ε-贪心）。
  ② **生成Episode**：用当前ε-贪心策略交互，记录轨迹。
  ③ **更新价值**：同MC Basic，用Episode回报更新$Q(s, a)$。
  ④ **改进策略**：定期将策略更新为当前$Q(s, a)$的ε-贪心策略（保持探索概率ε）。
- 收敛性：
  - 若**ε随时间衰减**（如$\varepsilon_t = 1/t$），可保证收敛到最优策略；
  - 若**ε固定**，会收敛到一个**次优但稳定的策略**（平衡探索和利用）。

![image-20250709154326846](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250709154326846.png)

![image-20250709154301707](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250709154301707.png)

## 三、三类算法对比（核心维度）

| **算法**            | 探索机制               | 环境依赖度 | 收敛性保障           | 实用场景           |
| ------------------- | ---------------------- | ---------- | -------------------- | ------------------ |
| MC Basic            | 依赖初始策略“偶然探索” | 低         | 无（易陷入局部最优） | 理论演示           |
| MC Exploring Starts | 强制随机起始状态/动作  | 极高       | 理论上收敛到最优     | 仿真环境（如网格） |
| MC ε-Greedy         | ε-贪心策略（自探索）   | 低         | ε衰减时收敛到最优    | 真实场景（如游戏） |

## 四、蒙特卡洛的优势与局限

### 1. 优势

- **无偏估计**：回报是真实长期收益的采样，无系统偏差（对比TD的Bootstrap有偏）。
- **Model-Free**：仅需实际交互数据，无需环境转移概率/奖励函数。

### 2. 局限

- **必须依赖Episode**：仅适用于**episodic任务**（有终止状态），持续任务需人工分段（如设最大步数）。
- **高方差**：不同Episode的回报波动大，学习过程可能不稳定（对比TD的低方差）。

## 五、与时序差分（TD）的核心差异

| **维度**      | 蒙特卡洛（MC）             | 时序差分（TD，如TD(0)）                      |
| ------------- | -------------------------- | -------------------------------------------- |
| **更新依据**  | 完整Episode的回报$G_t$     | 单步Bootstrap：$R_{t+1} + \gamma V(S_{t+1})$ |
| **更新时机**  | Episode结束后更新          | 每步交互后立即更新（在线学习）               |
| **偏差-方差** | 无偏，高方差               | 有偏，低方差                                 |
| **适用任务**  | episodic任务（如游戏关卡） | 持续任务（如机器人控制）                     |

## 六、总结：蒙特卡洛的本质

蒙特卡洛是**“用完整经验平均估计价值”**的方法，通过三类算法逐步解决“探索不足”问题：

- MC Basic展示核心逻辑，却忽略探索；
- MC Exploring Starts强制探索，却依赖环境；
- MC ε-Greedy用策略自探索，平衡实用与理论收敛。

理解这一演进，就能掌握蒙特卡洛在**“无模型、依赖采样”**框架下的设计智慧，也为后续学习TD算法（在线更新、低方差）埋下对比伏笔。