# DPO算法

B站指路：[DPO (Direct Preference Optimization) 算法讲解_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1GF4m1L7Nt/?spm_id_from=333.1387.top_right_bar_window_custom_collection.content.click&vd_source=aa6afb9d0536d09ecdcb5d2c1fcf4c79)

以下从**Bradley-Terry模型→DPO算法逻辑→实际训练流程**逐步拆解DPO在RLHF中的运作，结合“成对比较”与“教练-学生”类比，深入讲解其原理。

---

## 一、先学“成对比较”的数学工具：Bradley-Terry模型

### 1. 问题场景
当我们需要比较**两个对象的相对优势**（如：球队A和B谁更强？商品X和Y谁更受欢迎？模型回复$Y_1$和$Y_2$哪个更好？），但无法直接测“绝对实力”时，如何量化？

### 2. Bradley-Terry模型的定义
假设每个对象有一个**隐式实力参数**$\alpha_i$，则对象$i$战胜$j$的概率为：
$$
P(i > j) = \frac{\alpha_i}{\alpha_i + \alpha_j}
$$

- **核心逻辑**：实力越强（$\alpha_i$越大），战胜对手的概率越高。
- **参数特性**：$\alpha_i$只有**相对比例**有意义（例如$\alpha_A=2, \alpha_B=1$和$\alpha_A=4, \alpha_B=2$，$P(A>B)$都是$2/3$，因此通常固定一个$\alpha$为基准，如$\alpha_A=1$）。

### 3. 在RLHF中的应用：给回复“比高低”
RLHF的**奖励模型（RM）**需要判断：**同一Prompt下，回复$Y_a$和$Y_b$哪个更优质**。这本质上是**成对比较问题**，可直接套用BT模型：
- 设RM对回复的打分为$r(x,Y)$（分数越高，回复越好）。
- 则“$Y_a$比$Y_b$更优”的概率表示为：
  $$
  P(Y_a > Y_b) = \frac{\exp(r(x,Y_a))}{\exp(r(x,Y_a)) + \exp(r(x,Y_b))}
  $$
  （用**指数函数**包裹分数，避免负分导致概率为负，同时保持“分数差→概率差”的单调性。）

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250711173426494.png" alt="image-20250711173426494" style="zoom: 50%;" />

在以BT模型为基础训练奖励模型时，想让模型知道a比b好，那就让$P(Y_a > Y_b)$越大越好，构造极大似然估计，取负号得到**负对数似然损失（NLL Loss）**，本质上损失函数就是交叉熵损失，目标是最大化 “$y_w$ 优于 $y_l$” 的概率。

**为什么是交叉熵损失？**

我们的目标是训练奖励模型$R$，让它对“人类认为更优的样本”给出更高打分，即让$P(Y_a \succ Y_b | R)$尽可能接近$1$（因为已知$Y_a$确实比$Y_b$好）。

这本质上是一个**二分类任务**：

- **输入**：成对样本$(Y_a, Y_b)$；
- **标签**：真实偏好$y = 1$（表示“$Y_a \succ Y_b$”，这是已知的人类标注）；
- **模型预测**：$\hat{y} = P(Y_a \succ Y_b | R) = \sigma(R_a - R_b)$（其中$R_a = R(Y_a)$，$R_b = R(Y_b)$）。

对比二分类交叉熵的定义：

- 当$y = 1$时，交叉熵损失为$-\log(\hat{y})$；
- 在BT模型中，$\hat{y} = \sigma(R_a - R_b)$（模型预测“$Y_a \succ Y_b$”的概率），真实标签$y = 1$（已知$Y_a$更优），因此BT的损失函数正是这个场景下的二分类交叉熵损失。

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250711173606961.png" alt="image-20250711173606961" style="zoom: 50%;" />

---

## 二、从“评实力”到“教策略”：DPO算法的诞生

### 1. 传统RLHF的痛点（为什么需要DPO？）
以PPO为代表的传统方法存在以下问题：
- **维护旧策略**：通过重要性采样（IS）复用旧数据，但IS权重易失控（如策略突变，导致旧数据失效）。
- **复杂计算**：策略梯度、KL惩罚等模块，工程实现门槛高，训练易不稳定。

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250711182147217.png" alt="image-20250711182147217" style="zoom:50%;" />

DPO的野心：**直接利用“成对回复的优劣关系”，跳过复杂的策略梯度和IS，让训练更简单、更稳定**。

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250711182227707.png" alt="image-20250711182227707" style="zoom:50%;" />

### 2. DPO如何借鉴BT的“相对优势”思想？
BT解决**“静态对象的实力比较”**（如球队、商品），而DPO解决**“动态策略的回复优化”**（让模型更倾向生成优回复）。两者**核心关联**：
- **BT的“实力”** → DPO的“策略生成概率”：将“对象$i$的实力$\alpha_i$”替换为“策略$\pi$生成回复$Y$的概率$\pi(Y|x)$”。
- **BT的“胜负概率”** → DPO的“偏好概率”：将“$i$战胜$j$的概率”替换为“策略生成优回复$Y_a$比差回复$Y_b$更可能的概率”。

---

## 三、DPO的优化目标：让“优回复更易被生成”

### 1. 目标公式与拆解
DPO的优化目标是**“最大化优回复的奖励，同时保证策略不偏离基准模型太远”**：
$$
\max_\pi \underbrace{\mathbb{E}_{x,Y\sim\pi} [r(x,Y)]}_{\text{奖励最大化}} - \beta \cdot \underbrace{\text{KL}(\pi \|| \pi_{\text{ref}})}_{\text{策略稳定约束}}
$$

- **奖励最大化（第一项）**：希望策略$\pi$生成的回复在RM上的**期望奖励越高越好**（多生成优回复$Y_a$，少生成差回复$Y_b$）。
- **KL约束（第二项）**：
  - $\pi_{\text{ref}}$：基准模型（通常是**监督微调模型（SFT）**，保证回复“基本合理”，如语法正确、符合常识）。
  - **为什么约束？**：若$\pi$完全脱离$\pi_{\text{ref}}$，可能生成“语法对但逻辑崩”的回复（如SFT教“礼貌回复”，$\pi$突然说“骂人的话”，虽RM打分高，但违背用户预期）。**KL散度**衡量$\pi$和$\pi_{\text{ref}}$的分布差异，$\beta$控制约束强度（$\beta \uparrow$，策略越保守；$\beta \downarrow$，策略越激进）。

### 2. 从目标到Loss函数：数学推导的“巧妙简化”
通过**结合BT的对数似然形式 + Sigmoid函数**，目标转化为更易优化的Loss：
$$
\text{Loss}_{\text{DPO}} = -\mathbb{E} \left[ \ln \sigma\left( \beta \cdot \ln \frac{\pi(Y_a|x)}{\pi_{\text{ref}}(Y_a|x)} - \beta \cdot \ln \frac{\pi(Y_b|x)}{\pi_{\text{ref}}(Y_b|x)} \right) \right]
$$

- **参数解析**：
  - $\sigma(x)$：**Sigmoid函数**（$\sigma(x)=\frac{1}{1+\exp(-x)}$），将输入转化为[0,1]的概率（衡量“$Y_a$比$Y_b$更优”的置信度）。
  - $\pi(Y|x)$：当前策略生成回复$Y$的概率。
  - $\pi_{\text{ref}}(Y|x)$：基准模型（SFT）生成回复$Y$的概率。
  - $\beta$：平衡“奖励”和“稳定”的超参数（需实验调优，如$\beta=0.1\sim0.5$）。
  - $Y_a/Y_b$：同一Prompt下的**优/差回复对**（由RM或人工标注）。

---

## 四、DPO训练流程：手把手教模型“学聪明”（以聊天机器人为例）

**背景**：训练客服机器人生成更贴心的回复，已有：
- **SFT模型（$\pi_{\text{ref}}$）**：能生成基础合理回复（如“您的问题已记录，会尽快处理”）。
- **成对数据**：Prompt（“我的快递没收到”）下，**优回复$Y_a$**（“别担心~我帮您查物流：… 还能申请优先补发”）和**差回复$Y_b$**（“自己查物流单号”）。

### 步骤1：初始化策略$\pi$（继承SFT的“基本功”）
- 让$\pi$的初始参数与$\pi_{\text{ref}}$完全一致，保证**初始回复分布相同**（KL散度=0，回复风格稳定，就像学生先学“基础礼貌”）。

### 步骤2：用成对数据计算DPO Loss（给策略“指方向”）
对每个Prompt $x$，取成对回复$(Y_a, Y_b)$：

1. **计算“相对概率比”**：
   比较$\pi$和$\pi_{\text{ref}}$生成优/差回复的概率：
   $$
   \text{ratio}_a = \frac{\pi(Y_a|x)}{\pi_{\text{ref}}(Y_a|x)}, \quad \text{ratio}_b = \frac{\pi(Y_b|x)}{\pi_{\text{ref}}(Y_b|x)}
   $$
   
   - 若$\pi$更“喜欢”$Y_a$（$\text{ratio}_a > 1$），且更“讨厌”$Y_b$（$\text{ratio}_b < 1$），则“相对优势”明显（就像学生更爱说贴心话，不爱说冷漠话）。
   
2. **构造“偏好分数”**：
   量化“$Y_a$比$Y_b$更值得生成”的程度：
   $$
   \text{score} = \beta \cdot \left( \ln \text{ratio}_a - \ln \text{ratio}_b \right) = \beta \cdot \ln \frac{\pi(Y_a|x) \cdot \pi_{\text{ref}}(Y_b|x)}{\pi(Y_b|x) \cdot \pi_{\text{ref}}(Y_a|x)}
   $$
   
   - 分数越大，说明$\pi$生成$Y_a$比$Y_b$的“相对优势”越显著（越符合RM的偏好）。
   
3. **计算Loss（告诉策略“对不对”）**：
   $$
   \text{Loss} = -\ln \sigma(\text{score})
   $$
   
   - 若$\text{score} \to +\infty$（$\pi$极倾向$Y_a$、极抑制$Y_b$），$\sigma(\text{score}) \to 1$，$\text{Loss} \to 0$（最优，奖励！）。
   - 若$\text{score} \to -\infty$（$\pi$反而更倾向$Y_b$），$\sigma(\text{score}) \to 0$，$\text{Loss} \to +\infty$（最差，惩罚！）。

### 步骤3：反向传播更新$\pi$（让策略“学聪明”）
- 通过梯度下降最小化Loss，调整$\pi$的参数：
  - **奖励导向**：若$\pi$生成$Y_a$的概率提升、$Y_b$的概率降低，$\text{score}$增大，$\text{Loss}$减小（被鼓励，就像学生说贴心话被表扬）。
  - **稳定约束**：若$\pi$偏离$\pi_{\text{ref}}$太远（如$\text{ratio}_a$或$\text{ratio}_b$极端），KL约束通过$\text{score}$间接惩罚（保持回复风格合理，就像老师不让学生说太离谱的话）。

### 步骤4：迭代训练，监控“成长”
- **核心观察指标**：
  - **奖励趋势**：$\pi$生成的回复在RM上的平均奖励是否持续上升（学生越来越会说贴心话）。
  - **KL散度**：$\pi$与$\pi_{\text{ref}}$的KL是否≤0.05（$\beta=0.1$时，保证回复不脱离SFT的“基本功”，就像学生礼貌的底子还在）。
- 当奖励不再提升、KL稳定时，训练终止（学生出师！）。

---

## 五、总结

### 1.**DPO与Bradley-Terry的关联：思想同源，应用延伸**  
| **模型**     | **Bradley-Terry（训练奖励模型）** | **DPO（训练策略模型）**                                      |
| ------------ | --------------------------------- | ------------------------------------------------------------ |
| **核心对象** | 回复的**绝对得分** \( r(x,y) \)   | 策略与基准模型的**相对概率比** \( \ln(\pi_\theta/\pi_{\text{ref}}) \) |
| **Loss形式** | $ -\ln(\sigma(r_w - r_l)) $       | $ -\ln(\sigma(\beta \cdot (\ln(\pi_\theta^w/\pi_{\text{ref}}^w) - \ln(\pi_\theta^l/\pi_{\text{ref}}^l)))) $ |
| **共同思想** | 用**成对比较的概率**建模人类偏好  | 继承“相对优势”，将偏好转化为可优化的Loss                     |
| **本质差异** | 优化“得分→概率”的映射（静态）     | 优化“策略→相对概率”的进化（动态）                            |

## 2.DPO与PPO对比

| **维度**     | **RLHF + PPO**                    | **DPO**                        |
| ------------ | --------------------------------- | ------------------------------ |
| **核心流程** | SFT → RM训练 → PPO优化            | SFT → DPO直接优化              |
| **反馈形式** | 依赖奖励模型的评分信号            | 直接使用人类偏好排序数据       |
| **计算成本** | 高（需训练奖励模型和多次PPO迭代） | 低（单阶段优化，无需奖励模型） |
| **适用场景** | 高精度对齐、复杂决策任务          | 轻量对齐、资源受限或垂类场景   |
| **泛化能力** | 较强（奖励模型可捕捉隐性偏好）    | 较弱（依赖显式偏好数据）       |