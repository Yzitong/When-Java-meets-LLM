# PPO算法

B站指路：[零基础学习强化学习算法：ppo_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1iz421h7gb/?spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=aa6afb9d0536d09ecdcb5d2c1fcf4c79)

---

## 一、On-policy vs Off-policy & 重要性采样（IS）的作用

### 1. 本质区别
- **On-policy**：策略更新时，**必须使用当前策略生成的数据**（如A2C，每次更新需重新与环境交互）。  
  **类比**：小明自己尝试（看书/玩游戏），根据老师的反馈调整（“自己做，自己学”）。  
- **Off-policy**：策略更新时，**可以使用旧策略或其他策略生成的数据**，通过**重要性采样（IS）加权**，让旧数据等价于当前策略的采样。 
  **类比**：小红观察小明的尝试和反馈（旧数据），通过“加权模仿”学习（小红的行为与小明不同，但能借鉴小明的有效经验）。

![image-20250711133017859](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250711133017859.png)

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250711133106605.png" alt="image-20250711133106605" style="zoom:80%;" />

### 2. 为什么IS能让方法变成Off-policy？
策略更新的核心是优化期望：
$
\nabla_\theta \mathbb{E}_{\pi_\theta} \left[ A \cdot \log \pi_\theta(a|s) \right]
$
引入IS后，**旧策略$\pi'$的数据可以“冒充”当前策略$\pi$的数据**：
$
\mathbb{E}_{\pi_\theta} \left[ f(s,a) \right] = \mathbb{E}_{\pi_{\theta'}} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta'}(a|s)} \cdot f(s,a) \right]
$
通过权重$w = \frac{\pi_\theta}{\pi_{\theta'}}$，旧策略的样本被加权后，能等价于当前策略的样本期望。因此，无需实时用当前策略采样，实现Off-policy。

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250711133427145.png" alt="image-20250711133427145" style="zoom: 67%;" />

---

## 二、GAE（广义优势估计）：更聪明的“优势判断”

### 1. 传统优势函数的问题
优势函数$A(s,a) = Q(s,a) - V(s)$，衡量“动作$a$比平均情况好多少”。  
- **单步TD（$\lambda=0$）**：$A = r + \gamma V(s') - V(s)$（只看一步，**方差小但偏差大**，如“一次考试好”不能代表能力）。  
- **蒙特卡洛（$\lambda=1$）**：$A = \sum_{t=0}^T \gamma^t r_t - V(s)$（累加多步，**偏差小但方差大**，如“多次考试好”但噪声多）。

### 2. GAE的改进
GAE通过**$\lambda$加权多步TD残差**，平衡偏差和方差：
$$
A^\lambda_t = \sum_{k=0}^\infty \lambda^k \left( r_{t+k+1} + \gamma V(s_{t+k+1}) - V(s_{t+k}) \right)
$$

- **$\lambda=0$**：退化为单步TD（保守，方差小）。  
- **$\lambda=1$**：退化为蒙特卡洛（激进，偏差小）。  
- **$0<\lambda<1$**：灵活加权多步信息（如$\lambda=0.9$，主要看最近几步，兼顾偏差和方差）。

---

## 三、PPO的两种约束：不让策略“学歪”

背景：如果直接用IS更新，当$\frac{\pi_\theta}{\pi_{\theta'}}$过大，策略可能剧变（如从“保守回复”变成“胡言乱语”，导致后续数据失效）。

![image-20250711134600556](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250711134600556.png)

PPO通过**约束策略更新幅度**解决，分两种方法：

![image-20250711133610262](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250711133610262.png)

### 1. PPO1（KL惩罚）：用“距离”限制变化
- **公式**（结合RLHF的奖励和KL约束）：
$$
\text{Loss}_{\text{ppo1}} = -\underbrace{\frac{1}{N}\sum \sum A_{\theta'}^{\text{GAE}} \cdot \frac{\pi_\theta}{\pi_{\theta'}}}_{\text{奖励项（最大化优势）}} + \beta \cdot \underbrace{\text{KL}(\pi_\theta, \pi_{\theta'})}_{\text{惩罚项（限制策略差异）}}
$$
- **KL散度**：$\text{KL}(\pi_\theta, \pi_{\theta'}) = \mathbb{E}_{\pi_\theta} \left[ \log \frac{\pi_\theta}{\pi_{\theta'}} \right]$，衡量两个策略的**概率分布差异**（差异越大，KL值越高）。  

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250711162444677.png" alt="image-20250711162444677" style="zoom: 33%;" />

- **逻辑**：若KL超过阈值（如0.01），增大$\beta$“惩罚”策略突变；反之减小$\beta$，让策略更灵活。

### 2. PPO2（Clip截断）：直接“砍断”极端权重
- **公式**：
  $$
  \text{Loss}_{\text{ppo2}} = -\frac{1}{N}\sum \sum \min\left( A \cdot \frac{\pi_\theta}{\pi_{\theta'}}, \ \text{clip}\left( \frac{\pi_\theta}{\pi_{\theta'}}, \ 1-\varepsilon, \ 1+\varepsilon \right) \cdot A \right)
  $$
  
- **clip逻辑**：将重要性权重$\frac{\pi_\theta}{\pi_{\theta'}}$限制在$[1-\varepsilon, 1+\varepsilon]$（如$\varepsilon=0.2$，即权重最多变化20%）。  
  
  - 若$\frac{\pi_\theta}{\pi_{\theta'}} > 1+\varepsilon$：截断后，奖励不再随权重增加而变大，避免策略突然“冒险”。  
  - 若$\frac{\pi_\theta}{\pi_{\theta'}} < 1-\varepsilon$：截断后，奖励不再随权重减小而变小，避免策略过度“保守”。

**类比**：老师对小红说：“可以学小明，但不能学得太不像（clip限制），或不能和小明差太远（KL限制）”，否则你学的经验就没用了。

---

## 四、RLHF中PPO的完整流程：以“聊天机器人训练”为例

将**策略=聊天机器人**、**环境=奖励模型（RM，模拟人类偏好）**、**旧策略=监督微调模型（SFT，保证回复基本合理）**结合，走一遍流程：

### 步骤1：监督微调（SFT）——学“基础回复”
- **数据**：收集人工标注的优质对话（如用户问“推荐一本书”，优质回复是“《哈利波特》很有趣，适合幻想爱好者～”）。  
- **训练**：用这些数据训练策略网络$\pi$，得到**SFT模型$\pi_{\text{sft}}$**（此时$\pi'=\pi_{\text{sft}}$，后续PPO更新时，$\pi'$固定为SFT模型，避免策略偏离太多）。

### 步骤2：训练奖励模型（RM）——学“人类偏好”
- **生成候选回复**：让$\pi_{\text{sft}}$对同一问题（如“推荐一本书”）生成多个回复：  
  - 回复1：“《哈利波特》”（普通，得分6）。  
  - 回复2：“哈喽，书的话…”（差，得分3）。  
  - 回复3：“《哈利波特》很有趣，适合幻想爱好者～”（优质，得分9）。  
- **训练RM**：用“问题+回复→分数”的数据，训练RM，让它能给回复打分数（模拟人类觉得“多好”）。

### 步骤3：PPO优化策略——在RM反馈下“进化”
现在，策略$\pi$（初始为$\pi_{\text{sft}}$）与“环境”（RM）交互，用PPO更新：

#### ① 交互阶段：生成回复，算奖励和优势
- **输入**：用户提问$s=$“推荐一本书”。  
- **当前策略$\pi$**生成回复$a=$“《哈利波特》不错”（比SFT的回复稍简单）。  
- **RM打分**：$R=0.6$（假设分数越高越好，0.6比平均水平高一点）。  
- **计算优势$A$**（用GAE，假设$\lambda=0.9$，单步近似）：  
  价值网络$V(s)$估计“这个问题的平均回复得分”是0.5，则$A = R - V(s) = 0.1$（回复比平均好0.1）。

#### ② 更新阶段：用PPO Loss调整策略
- **重要性权重**：$\text{ratio} = \frac{\pi(a|s)}{\pi'(a|s)}$（$\pi'$是SFT模型，它对“《哈利波特》不错”的概率是$p_{\text{old}}$，$\pi$更新后的概率是$p_{\text{new}}$）。  

- **Clip截断**：假设$\varepsilon=0.2$，ratio被限制在$[0.8, 1.2]$。  

- **Loss计算**：
  $$
  \text{Loss} = -\min\left( 0.1 \cdot \text{ratio}, \ \text{clip}(\text{ratio}, 0.8, 1.2) \cdot 0.1 \right)
  $$
  
  
  - 若$\text{ratio}=1.3$（超过1.2），截断为1.2，Loss项为$-0.1 \times 1.2 = -0.12$（反向传播时，策略“鼓励”此回复，但幅度被限制）。  
  - 若$\text{ratio}=0.7$（低于0.8），截断为0.8，Loss项为$-0.1 \times 0.8 = -0.08$（避免策略过度“放弃”此回复）。

#### ③ 迭代优化
重复步骤3，$\pi$逐渐学会生成RM打分更高的回复（如从“《哈利波特》不错”进化到“《哈利波特》很有趣，适合幻想爱好者～”），同时**clip保证$\pi$不会突然生成奇怪回复**（如突然说“买我的书！”，即使RM没见过，ratio过大被截断，避免策略突变）。

---

## 五、关键总结：PPO在RLHF里的核心逻辑
1. **Off-policy能力**：通过IS，复用SFT模型的旧数据，无需每次重新采样，效率更高。  
2. **GAE的作用**：更准确估计“回复质量比平均好多少”，平衡偏差和方差。  
3. **PPO的约束**：通过KL或clip，限制策略与SFT模型的差异，避免生成“违背常识”的回复（如SFT教的是礼貌回复，PPO不让它突然说脏话）。

**类比收尾**：  
- **On-policy小明**：自己试错，慢慢学。  
- **Off-policy小红+PPO**：观察小明的试错（旧数据），加权学习，还被老师限制“不能学太歪”，最终高效学会“好行为”（优质回复）。

通过从概念到公式，再到实际训练流程，PPO在RLHF中的运作逻辑应已清晰明了！