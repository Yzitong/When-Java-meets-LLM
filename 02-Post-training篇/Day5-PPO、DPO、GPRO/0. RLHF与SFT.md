# RLHF与SFT

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250710160056296.png" alt="image-20250710160056296" style="zoom: 67%;" />

------

## 一、微调：有监督微调（SFT）与无监督微调

### 1. 有监督微调（Supervised Fine-Tuning, SFT）

#### 定义

SFT 是 **利用人工标注的有标签数据（如“prompt-回答对”或分类标签）对预训练模型进行微调的方法**，旨在让模型学习特定任务的行为模式，如指令遵循、情感分析等。

#### 核心逻辑

- **输入**：预训练模型（具备通用语言能力） + 有标签数据（如“用户问‘如何做蛋炒饭’ → 回答烹饪步骤”）。
- **训练**：通过 **监督学习损失**（如交叉熵损失）优化模型，使输出尽可能接近标注数据。
- **目标**：模仿人类标注的表达模式，初步对齐人类意图。

#### 关键技术

- **损失函数**：交叉熵损失，最小化模型输出与标注答案的差异。
- **微调技巧**：分层微调（冻结部分预训练层，仅更新输出层）、学习率调度（防止过拟合）。

#### 特点

- **优点**：训练简单直接，适合快速适配特定任务。
- **局限**：输出受限于标注数据，无法生成超越标注的更优回答。
- **误解澄清**：SFT 是训练方法，而非“有标签数据”本身，数据仅为其输入。

#### 应用场景

- 对话生成（如 chatbot 的指令遵循）。
- 分类任务（如情感分析、意图识别）。

------

### 2. 无监督微调

#### 定义

无监督微调是 **利用目标领域的无标签数据对预训练模型进行“领域适配”**，使模型学习领域特定的语言特征（如医疗、法律领域的专业术语），无需人工标注。

#### 核心逻辑

- **输入**：目标领域的无标签文本（如医疗论文、法律条文）。
- **训练**：延续预训练的 **自监督学习** 任务，聚焦领域数据，提升模型对特定语义的理解。
- **目标**：为后续 SFT 或任务应用打下领域语言基础。

#### 典型任务

- **Masked Language Modeling（MLM）**：随机遮蔽文本中的词，预测缺失内容（如 BERT 的预训练）。
- **句子重构/去噪**：打乱句子或添加噪声，模型恢复原文（如 T5 的预训练）。
- **对比学习**：构造相似/不相似文本对，学习领域内的语义关联。

#### 特点

- **优点**：无需人工标注，数据获取成本低，适合领域适配。
- **局限**：仅增强领域知识，需结合 SFT 或 RLHF 实现任务对齐。
- **与预训练的区别**：数据更聚焦特定领域，而非通用语料。

#### 应用场景

- 医疗领域：用医学文献微调模型，增强对专业术语的理解。
- 法律领域：用法律条文微调，提升合同分析能力。

------

## 二、RLHF：基于人类反馈的强化学习

### 1. RLHF 的含义

#### 定义

**RLHF（Reinforcement Learning with Human Feedback）** 是通过 **强化学习** 优化模型，使其生成更符合人类长期偏好的输出（如更安全、更有用、更有逻辑的回答），超越 SFT 的单纯模仿。

#### 核心目标

- 从“模仿标注数据”进化到“主动优化”，生成未在训练数据中显式标注但更优的回答。
- 通过 **人类偏好数据**（如回答排序或评分）驱动模型对齐人类价值观。

------

### 2. RLHF 的主要模块

RLHF 包含两个核心阶段，对应以下模块：

#### 模块 1：奖励模型训练

- **作用**：训练一个 **奖励模型（Reward Model, RM）**，将人类偏好量化为标量奖励 $R_t$，用于指导策略优化。
- **输入**：人类偏好数据（如对同一 prompt 的多个回答，标注“哪个更优”）。
- 训练：
  - **损失函数**：Pairwise Loss（比较两个回答的偏好，如 $\max(0, R(a_2) - R(a_1) + \epsilon)$）或 Rank Loss（对齐人类排序）。
  - **模型结构**：基于预训练模型微调，输出标量奖励。
- 技术要点：
  - **数据处理**：通过众包平台收集偏好数据，需处理标注噪声（如不一致性）。
  - **抗噪声**：通过数据增强或多轮迭代优化奖励模型稳定性。

#### 模块 2：策略模型的强化学习

- **作用**：通过强化学习优化 **策略模型（Actor）**，最大化累计奖励 $\sum (R_t + \gamma V_t)$。
- 角色分工：
  - **策略模型（Actor）**：接收 prompt，生成回答 $a_t$（初始为 SFT 模型）。
  - **评估模型（Critic/价值模型）**：估计当前状态的长期价值 $V_t$（预测未来奖励的累计和）。
  - **参考模型**：通常为 SFT 模型，防止策略更新过快（如 PPO 中的“旧策略”）。
- **训练**：通过强化学习算法（如 PPO）优化策略模型，结合奖励模型的 $R_t$ 和 Critic 的 $V_t$。

------

### 3. RLHF 中的典型算法及其作用

#### PPO（Proximal Policy Optimization）

- **作用**：RLHF 中最常用的 **强化学习算法**，用于优化策略模型。
- 核心逻辑：
  - 使用 **信任区域**（如 clip 机制）约束策略更新幅度，防止过激更新导致训练崩溃。
  - 结合 **Actor-Critic 框架**：Actor 生成动作（回答），Critic 估计价值，参考模型（旧策略）稳定训练。
- 优势：
  - 稳定性高，适合复杂的大模型训练。
  - 通过 clip 损失平衡探索与稳定性，避免奖励模型噪声的干扰。
- 公式：
  - 策略梯度更新（简化为 clip 形式）：
    $$
    L^{CLIP}(\theta) = \mathbb{E}*t \left[ \min\left( \frac{\pi*\theta(a_t|s_t)}{\pi_{\text{old}}(a_t|s_t)} A_t, \text{clip}\left( \frac{\pi_\theta(a_t|s_t)}{\pi_{\text{old}}(a_t|s_t)}, 1-\epsilon, 1+\epsilon \right) A_t \right) \right]
    $$
    其中 $A_t$ 为优势函数，$\pi_{\text{old}}$ 为参考模型策略。

#### DPO（Direct Preference Optimization）

- **作用**：**简化 RLHF 的替代方案**，直接利用偏好数据优化策略，跳过奖励模型训练。
- 核心逻辑：
  - 将强化学习的“最大化奖励”问题转化为 **二分类问题**，让模型生成更符合人类偏好的回答。
  - 输入：人类偏好数据（如“回答 A 优于回答 B”）。
  - 优化目标：通过偏好损失直接调整策略模型（如基于交叉熵或对数似然）。
- 优势：
  - 避免奖励模型训练和 PPO 的复杂调试，训练更高效。
  - 直接使用偏好数据，降低算法实现难度。
- 公式：
  - 偏好损失（简化为对数形式）：
    $$
    L_{DPO}(\theta) = -\mathbb{E}*{(x, y_w, y_l)} \left[ \log \sigma \left( \beta \log \frac{\pi*\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right) \right]
    $$
    其中 $y_w$ 为更优回答，$y_l$ 为较差回答，$\pi_{\text{ref}}$ 为参考模型。

#### GPRO（Generalized Preference-based Reinforcement Optimization）

- **作用**：PPO 和 DPO 的 **扩展或变种**，结合两者的优点，优化偏好数据的使用效率。
- 核心逻辑：
  - 在 PPO 的信任区域框架下，融入 DPO 的直接偏好优化思想。
  - 通过更灵活的损失设计，平衡稳定性与偏好对齐效率。
- 优势：
  - 兼顾 PPO 的稳定性和 DPO 的高效性。
  - 适合复杂任务，适应多样化的偏好数据（如排序、评分混合）。
- **应用**：仍在研究阶段，具体实现依赖任务需求，可能结合 PPO 的 clip 机制和 DPO 的偏好损失。

------

## 总结：SFT、无监督微调与 RLHF 的定位与关系

- **SFT**：**模仿阶段**，通过监督学习让模型学习人类标注的表达模式，是 RLHF 的基础。
- **无监督微调**：**领域适配阶段**，利用无标签数据增强模型对特定领域的理解，为 SFT 或 RLHF 提供更好起点。
- **RLHF**：**进化阶段**，通过奖励模型量化人类偏好，结合强化学习（PPO、DPO、GPRO 等）让模型生成超越标注数据的更优输出。

### 技术对比表

| 概念           | 核心逻辑         | 数据类型       | 典型应用阶段 | 关键算法/技术                                      |
| -------------- | ---------------- | -------------- | ------------ | -------------------------------------------------- |
| **SFT**        | 模仿标注数据行为 | 有标签数据     | 基础对齐     | 交叉熵损失、分层微调                               |
| **无监督微调** | 学习领域语言特征 | 无标签领域数据 | 领域适配     | MLM、句子重构、对比学习                            |
| **RLHF**       | 优化人类偏好     | 人类偏好数据   | 高级对齐     | PPO（信任区域）、DPO（直接优化）、GPRO（混合优化） |

### 依赖关系

- **无监督微调 → SFT**：无监督微调为 SFT 提供领域适配的模型起点。
- **SFT → RLHF**：SFT 提供初始策略模型，奖励模型基于 SFT 输出训练，RLHF 进一步优化。
- **PPO/DPO/GPRO 在 RLHF 中**：PPO 提供稳定优化，DPO 简化流程，GPRO 探索两者的融合。