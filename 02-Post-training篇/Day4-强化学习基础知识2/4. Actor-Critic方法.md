# Actor-Critic方法

Actor-critic 方法仍属于策略梯度方法，它强调一种 **融合策略梯度（policy based）与基于价值方法（value based）的架构**。

**什么是 “Actor（执行者）” 和 “Critic（评价者）”？**

- 这里的 **“Actor” 对应策略更新**policy improvement。之所以称为 “执行者”，是因为策略会被用来执行动作。
- 这里的 **“Critic” 对应策略评估或价值估计**policy evaluation。之所以称为 “评价者”，是因为它通过评估策略来 “评判” 策略的优劣。

**补充解释（帮助理解）：**

- **Actor 的角色**：负责优化策略网络（如神经网络参数），直接决定智能体的动作输出，像 “执行者” 一样落地决策。
- **Critic 的角色**：通过学习价值函数（如状态价值 *V*(*s*) 或动作价值 *Q*(*s*,*a*)），为 Actor 的策略 “打分”（例如用优势函数 $A(s,a)$衡量动作的好坏），像 “评价者” 一样反馈策略的改进方向。
- **方法优势**：Actor-critic 结合了  策略梯度（直接优化策略）和基于价值（间接评估策略）的思路，让策略更新更高效（比如用 Critic 的价值估计替代 REINFORCE 的蒙特卡洛采样，降低训练方差）。

---

## 一、核心思想：Actor 与 Critic 的分工

**Actor-Critic（AC）** 方法结合了 **策略梯度（Policy Gradient, PG）** 和 **价值学习（Value-based）** 的优点，通过 **Actor** 和 **Critic** 的协作优化策略：

- **Actor**：学习策略 $\pi_\theta(a|s)$，直接输出动作，目标是最大化累计回报。
- **Critic**：学习价值函数（如 $Q(s,a)$ 或 $V(s)$），评估 Actor 策略的好坏，为 Actor 提供梯度指导。

**优势**

相比纯策略梯度（如 REINFORCE），AC 通过 Critic 的价值估计 **降低梯度方差**，避免依赖高方差的蒙特卡洛回报，使训练更稳定。

---

## 二、最简单的 Actor-Critic方法（QAC）

### 引入

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250710200534750.png" alt="image-20250710200534750" style="zoom:80%;" />

**从公式看角色**   

1. **Actor（演员）**：对应 **策略更新过程**（上述梯度上升算法）。  
   - 职责：学习策略\($\pi_\theta$)，通过调整$\theta$最大化累计回报。  
   - 依赖：需要$q_t(s,a)$作为“指导信号”（告诉Actor哪些动作更有价值）。  

2. **Critic（评论家）**：对应 **估计$\boldsymbol{q_t(s,a)}$的过程**。  
   - 职责：学习动作价值函数$\boldsymbol{q_t(s,a)}$，评估Actor的动作好坏。  
   - 方法：通过**时序差分（TD）**或**蒙特卡洛（MC）**更新。 

### 核心逻辑

![image-20250710201725817](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250710201725817.png)

- **Actor**：基于策略梯度更新，梯度为：
  $$
  \nabla_\theta J \approx \mathbb{E}\left[ \nabla_\theta \log\pi_\theta(a|s) \cdot Q(s,a) \right]
  $$
  其中 $Q(s,a)$ 是 Critic 估计的 **动作价值**，表示状态 $s$ 下选择动作 $a$ 的未来回报。
- **Critic**：通过 **Sarsa**基于值去更新 更新，目标是得到最新的 $q(s,a)$ 。

### 特点
- **同策略（On-policy）**：Actor 和收集数据的策略一致，必须实时与环境交互，数据无法复用。
- **方差仍较高**：$Q(s,a)$ 包含状态价值和动作优势的混合，未分离两者影响，导致梯度波动较大。

---

## 三、Advantage Actor-Critic（A2C）

### 核心改进：引入优势函数
**优势函数（Advantage Function）** 表示在state s 下，做出action a比其他动作能带来多少优势，定义为：
$$
A(s,a) = Q(s,a) - V(s)
$$
其中 $V(s)$ 是 **状态价值**，表示状态 $s$ 下所有动作的平均回报。

如图，如果没有引入baseline，那么可能所有动作都会往一个方向去增加，而我们想要的是动作之间造成的差异，也就是相对值；所以减去baseline不影响相对值的同时，减少好局势与坏局势下造成的方差。

<img src="https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250711124906256.png" alt="image-20250711124906256" style="zoom:80%;" />

#### 为什么用优势？
- **基线不变性（Baseline Invariance）**：$V(s)$ 是与动作无关的基线，减去 $V(s)$ 后梯度期望不变（因 $\mathbb{E}[V(s) \cdot \nabla_\theta \log\pi_\theta(a|s)] = 0$），但 **方差大幅降低**，因为只关注某动作得到的回报比平均回报更好或更差的部分。

### 算法流程

![image-20250710214146979](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250710214146979.png)

1. **收集轨迹**：用当前策略 $\pi_\theta$ 与环境交互，记录 $(s_t, a_t, r_t, s_{t+1})$。
2. **估计优势**：用 **广义优势估计（GAE）** 或 TD 误差近似 $A(s_t, a_t)$（GAE 结合多步回报和折扣，效率更高）。
3. **更新 Actor**：用梯度：
   $$
   \nabla_\theta J \approx \sum \nabla_\theta \log\pi_\theta(a_t|s_t) \cdot A(s_t, a_t)
   $$
4. **更新 Critic（V 网络）**：最小化 $V(s_t)$ 与目标 $r_t + \gamma V(s_{t+1})$ 的误差，学习状态价值。

### 特点
- 仍为 **同策略**，优势估计依赖当前策略的 $V(s)$。
- 相比基础 AC，方差更低，训练更高效。

---

## 四、Off-policy Actor-Critic（异策略 AC）

### 核心挑战：数据来自“旧策略”
**异策略**指 Actor 学习的策略 $\pi_\theta$ 与收集数据的 **行为策略 $\mu$** 不同（例如用随机策略 $\mu$ 探索，优化目标策略 $\pi_\theta$）。直接用 $\mu$ 的数据更新 $\pi_\theta$ 会因分布不匹配而失效，需引入 **重要性采样（Importance Sampling, IS）**。

#### 重要性采样（IS）
通过权重纠正分布差异：
$$
\rho_t = \prod_{k=0}^t \frac{\pi_\theta(a_k|s_k)}{\mu(a_k|s_k)}
$$
异策略梯度估计为：
$$
\nabla_\theta J \approx \mathbb{E}\left[ \rho_t \cdot \nabla_\theta \log\pi_\theta(a_t|s_t) \cdot Q(s_t, a_t) \right]
$$

#### off-policy梯度定理
理论上，off-policy梯度的期望与on-policy梯度等价（当 $\mu$ 覆盖 $\pi_\theta$ 的动作空间时），但实际中权重 $\rho_t$ 可能因多步累积而 **爆炸**，需通过 **截断权重**（如 $\rho_t = \min(\rho_t, c)$）或 **近端策略优化（PPO）** 缓解。

### 算法与例子
- **例子**：用 $\varepsilon$-贪心（行为策略 $\mu$）探索，学习目标策略 $\pi_\theta$（如 A2C 的off策略版本）。
- **算法**：结合 **replay buffer** 存储 $\mu$ 的轨迹，定期用 IS 加权更新 $\pi_\theta$ 和 Critic，实现数据复用。

### 特点
- **离策略**：数据可复用，提升样本效率。
- IS 引入额外方差，需权衡权重处理。

---

## 五、Deterministic Actor-Critic（DPG，确定性 AC）

### 核心区别：从“随机策略”到“确定性策略”
- **随机策略**：输出动作概率分布（如 $\pi_\theta(a|s) \sim \mathcal{N}(\mu_\theta(s), \sigma^2)$）。
- **确定性策略**：直接输出确定动作 $a = \mu_\theta(s)$，无需分布，仅优化动作值。

#### 确定性策略梯度定理
梯度推导为：
$$
\nabla_\theta J(\mu_\theta) = \mathbb{E}\left[ \nabla_\theta \mu_\theta(s) \cdot \nabla_a Q(s,a) \big|_{a=\mu_\theta(s)} \right]
$$
即：沿着 **动作对参数的梯度** 方向，乘以 **Q 对动作的梯度**（Critic 提供 $Q$ 的梯度，Actor 用它更新策略）。

### 算法（以 DDPG 为例）
1. **Actor**：学习确定性策略 $a = \mu_\theta(s)$，用 Critic 的 $\nabla_a Q(s,a)$ 指导更新：
   $$
   \theta \leftarrow \theta + \alpha \cdot \nabla_\theta \mu_\theta(s) \cdot \nabla_a Q
   $$
2. **Critic**：学习 $Q(s,a)$，用 TD 误差更新（类似 Q-learning，结合目标网络稳定训练）。
3. **探索**：通过 **动作噪声**（如 OU 噪声）实现探索，因确定性策略无随机性。

### 特点
- 适合 **连续动作空间**，无需处理分布，计算更高效。
- 天然 **离策略**，结合 replay buffer，数据来自带噪声的探索策略。

---

## 总结：AC 方法的演变逻辑

| 方法          | 策略类型  | 数据利用 | 核心改进                   | 代表算法             |
| ------------- | --------- | -------- | -------------------------- | -------------------- |
| 基础 AC       | 随机      | 同策略   | 结合策略梯度+价值学习      | 早期 AC              |
| A2C           | 随机      | 同策略   | 优势函数降低方差           | A2C、GAE             |
| 离策略 AC     | 随机/确定 | 离策略   | 重要性采样复用数据         | DDPG（确定型）、ACER |
| DPG（确定型） | 确定      | 离策略   | 确定性策略简化连续动作优化 | DDPG、TD3            |

**Actor-Critic 方法** 通过 Actor 与 Critic 的协作，平衡了策略优化的 **方差** 和 **样本效率**，成为强化学习中最核心的算法家族之一，尤其在 **连续动作任务**（如机器人控制、自动驾驶）中表现出色。其演变逻辑从基础 AC 到 A2C、离策略 AC 再到确定性 AC，逐步解决了方差、样本效率和计算复杂度的挑战。

![image-20250710145005227](https://raw.githubusercontent.com/Yzitong/LLM-Mastery-Journey/main/images/image-20250710145005227.png)